[
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Introduction to Public Policy Analytics (MUSA 5080) and course goals\n\nImportance of combining spatial analysis with data science for urban planning and policy\n\nOverview of course deliverables: weekly notes, labs, and final project\n\nInitial setup of the RStudio + Quarto environment for reproducible workflows"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Introduction to Public Policy Analytics (MUSA 5080) and course goals\n\nImportance of combining spatial analysis with data science for urban planning and policy\n\nOverview of course deliverables: weekly notes, labs, and final project\n\nInitial setup of the RStudio + Quarto environment for reproducible workflows"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nLearned how to set the working directory in R (setwd())\n\nPracticed creating and rendering a Quarto (.qmd) document\n\nExplored basic Quarto structure: YAML header, markdown formatting, and render function"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nStill clarifying the workflow between RStudio and Quarto rendering (e.g., where files are stored vs. rendered outputs)\n\nNeed more practice with GitHub integration for publishing the portfolio"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nBuilding reproducible workflows with R and Quarto ensures transparency and credibility in policy-related analysis\n\nPortfolio structure mirrors how real-world policy analysts document methods, results, and reflections"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: Seeing how quickly RStudio + Quarto can generate a professional-looking portfolio website\n\nPlan to apply: Use weekly notes to practice explaining technical concepts in plain language, which is critical for communicating with policymakers"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#introduction",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#introduction",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-chicago-boundaries",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-chicago-boundaries",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Chicago Boundaries",
    "text": "Load Chicago Boundaries\n\n\nCode\n# Load police districts for cross-validation\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nWhat we’re doing: Loading the spatial boundaries of Chicago and its police districts. We use police districts for spatial cross-validation later.\nWhy this matters: We need boundaries to constrain our analysis to Chicago and to create groups for validation."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-sanitation-violations-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-sanitation-violations-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Sanitation Violations Data",
    "text": "Load Sanitation Violations Data\n\n\nCode\n# Load the downloaded data\nviolations &lt;- read_csv(\"data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251114.csv\") %&gt;%\n  # Convert to sf object\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  # Parse date\n  mutate(\n    creation_date = mdy(`Creation Date`),\n    year = year(creation_date)\n  ) %&gt;%\n  # Keep only necessary columns\n  dplyr::select(\n    creation_date,\n    year,\n    status = Status,\n    violation_type = `What is the Nature of this Code Violation?`\n  )\n\ncat(\"✓ Loaded sanitation violations\\n\")\n\n\n✓ Loaded sanitation violations\n\n\nCode\ncat(\"  - Total violations:\", nrow(violations), \"\\n\")\n\n\n  - Total violations: 19733 \n\n\nCode\ncat(\"  - Date range:\", min(violations$creation_date), \"to\", \n    max(violations$creation_date), \"\\n\")\n\n\n  - Date range: 17167 to 17531 \n\n\nWhat we found: The dataset contains 19733 sanitation code violations from 2017. These represent citizen complaints about various sanitation issues across Chicago."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#visualize-spatial-distribution",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#visualize-spatial-distribution",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Visualize Spatial Distribution",
    "text": "Visualize Spatial Distribution\n\n\nCode\n# Point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = violations, color = \"#d62828\", size = 0.1, alpha = 0.3) +\n  labs(\n    title = \"Sanitation Code Violations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(violations))\n  ) +\n  theme_map()\n\n# Density surface\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(violations)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Higher concentrations in certain areas\"\n  ) +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nWhat patterns do we observe?\nThe sanitation code violations in Chicago are clearly not spread out at random. Instead, they form noticeable clusters—especially on the South Side and West Side. These areas show the strongest concentrations of violations, which stand out in the density map as dark-purple hot spots. In contrast, the North Side and much of the lakefront have far fewer recorded violations. This pattern lines up with broader neighborhood characteristics. Communities with more violations tend to have older housing, lower household incomes, and fewer resources available for property upkeep. The clustering suggests that these issues don’t happen in isolation but are connected to larger structural conditions, including the physical environment, local economic context, and even how enforcement may vary across neighborhoods. Because the violations are clearly clustered rather than randomly scattered, the data is well-suited for spatial prediction. The strong geographic patterns indicate that location and neighborhood context play an important role in explaining where violations occur."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#create-500m-x-500m-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#create-500m-x-500m-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Create 500m x 500m Grid",
    "text": "Create 500m x 500m Grid\n\n\nCode\n# Create fishnet grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells intersecting Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size: 500 x 500 meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nWhy use a fishnet grid?\nA regular grid allows us to: 1. Aggregate point data into consistent spatial units 2. Calculate spatial features at a uniform scale 3. Apply count regression models (which require aggregated counts)\nThis approach is more flexible than using administrative boundaries and ensures consistent spatial resolution across the study area."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#aggregate-violations-to-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#aggregate-violations-to-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Aggregate Violations to Grid",
    "text": "Aggregate Violations to Grid\n\n\nCode\n# Count violations per cell\nviolations_fishnet &lt;- st_join(violations, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countViolations = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(violations_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countViolations = replace_na(countViolations, 0))\n\n# Summary statistics\ncat(\"\\nViolation count distribution:\\n\")\n\n\n\nViolation count distribution:\n\n\nCode\nsummary(fishnet$countViolations)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    5.00    8.02   12.00  189.00 \n\n\nCode\ncat(\"\\nCells with zero violations:\", \n    sum(fishnet$countViolations == 0), \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countViolations == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero violations: 576 / 2458 ( 23.4 %)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Violations\",\n    option = \"plasma\",\n    trans = \"sqrt\"\n  ) +\n  labs(\n    title = \"Sanitation Code Violations by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat we observe:\nThe aggregated fishnet reveals that sanitation violations are widespread but unevenly distributed. Out of 2,458 cells, only 576 (23.4%) have zero violations, meaning over three-quarters of Chicago experienced at least one sanitation complaint in 2017.\nThe distribution shows substantial variation. While many cells have just 1-3 violations, some hotspot cells contain significantly higher counts. This right-skewed distribution is typical for urban complaint data and suggests two things: (1) sanitation problems are a city-wide issue rather than isolated incidents, and (2) certain neighborhoods experience disproportionately high violation rates, likely tied to structural factors like housing age, property maintenance capacity, and enforcement patterns.\nThe high variance across cells makes this data well-suited for count regression modeling, particularly Negative Binomial regression which can handle overdispersion."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-abandoned-vehicle-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-abandoned-vehicle-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Abandoned Vehicle Data",
    "text": "Load Abandoned Vehicle Data\n\n\nCode\n# Try to load from local file first (recommended)\nif (file.exists(\"data/abandoned_cars_2017.csv\")) {\n  abandoned_cars &lt;- read_csv(\"data/abandoned_cars_2017.csv\") %&gt;%\n    filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n    st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from local file\\n\")\n} else {\n  # Fallback: Try API (may be slow or fail)\n  abandoned_cars &lt;- read_csv(\"https://data.cityofchicago.org/resource/3c9v-pnva.csv?$limit=50000&$where=creation_date between '2017-01-01T00:00:00' and '2017-12-31T23:59:59'\") %&gt;%\n    filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from API\\n\")\n}\n\n\n✓ Loaded from local file\n\n\nCode\ncat(\"  - Number of abandoned vehicle calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of abandoned vehicle calls: 31390 \n\n\nWhy use abandoned vehicles as a predictor?\nFollowing the “broken windows theory,” physical signs of disorder (like abandoned vehicles) may predict other neighborhood problems. This variable tests whether disorder in one form (abandoned cars) correlates with disorder in another form (sanitation violations)."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#count-abandoned-vehicles-per-cell",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#count-abandoned-vehicles-per-cell",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Count Abandoned Vehicles per Cell",
    "text": "Count Abandoned Vehicles per Cell\n\n\nCode\n# Aggregate to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle Calls\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Sanitation Violations\") +\n  theme_map()\n\np1 + p2 +\n  plot_annotation(\n    title = \"Comparing Spatial Patterns\",\n    subtitle = \"Do these two phenomena co-occur?\"\n  )\n\n\n\n\n\n\n\n\n\nVisual relationship:\nThe side-by-side comparison reveals a strong visual correlation between abandoned vehicle calls and sanitation violations. Areas with high concentrations of abandoned cars—particularly on the South Side and West Side—also show elevated sanitation violation counts. This spatial overlap supports the “broken windows theory”: visible signs of physical disorder (abandoned vehicles) tend to co-occur with other forms of neighborhood neglect (sanitation problems).\nHowever, the relationship isn’t perfectly one-to-one. Some areas with moderate abandoned car counts still experience high sanitation violations, suggesting that other factors (housing density, property ownership patterns, or enforcement priorities) also play a role. This imperfect correlation justifies using abandoned cars as a predictor variable while recognizing it won’t explain all the variation in sanitation complaints.]*"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#calculate-nearest-neighbor-distances",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#calculate-nearest-neighbor-distances",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Calculate Nearest Neighbor Distances",
    "text": "Calculate Nearest Neighbor Distances\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(abandoned_cars.nn = rowMeans(nn_result$nn.dist))\n\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.283 2195.753 \n\n\nWhat this feature captures:\nThe average distance to the 3 nearest abandoned vehicle reports. A low value means a cell is surrounded by abandoned vehicles, suggesting neighborhood disorder. A high value means the cell is far from any abandoned vehicles."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#local-morans-i-identify-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#local-morans-i-identify-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Local Moran’s I: Identify Hot Spots",
    "text": "Local Moran’s I: Identify Hot Spots\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  local_moran &lt;- localmoran(data[[variable]], weights)\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = moran_class), color = NA) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High clusters = Hot spots of disorder\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat is Local Moran’s I?\nThis statistic identifies spatial clusters: - High-High (red): Hot spots - high values surrounded by high values - Low-Low (blue): Cold spots - low values surrounded by low values - High-Low / Low-High: Spatial outliers - Not Significant (gray): Random spatial pattern\nThis helps us understand where disorder is concentrated vs. where it’s spatially random."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#distance-to-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#distance-to-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Distance to Hot Spots",
    "text": "Distance to Hot Spots\n\n\nCode\n# Get hot spot centroids\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  cat(\"✓ Calculated distance to hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;% mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to hot spots\n  - Number of hot spot cells: 275 \n\n\nWhy distance to hot spots matters:\nBeing close to a cluster of abandoned vehicles may be a stronger predictor than distance to a single vehicle. Hot spots represent areas of concentrated disorder that may influence nearby areas."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#prepare-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#prepare-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Prepare Data",
    "text": "Prepare Data\n\n\nCode\n# Create clean dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countViolations,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#poisson-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#poisson-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n\nCode\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        2.866071797  0.025918274 110.581 &lt; 0.0000000000000002 ***\nabandoned_cars     0.001440424  0.000648357   2.222               0.0263 *  \nabandoned_cars.nn -0.004522870  0.000121096 -37.349 &lt; 0.0000000000000002 ***\ndist_to_hotspot   -0.000015715  0.000003982  -3.946            0.0000794 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 17204  on 1707  degrees of freedom\nResidual deviance: 12877  on 1704  degrees of freedom\nAIC: 18376\n\nNumber of Fisher Scoring iterations: 6\n\n\nInterpreting coefficients:\nAll three predictor variables are statistically significant, though with different levels of importance:\nabandoned_cars (β = 0.0014, p = 0.026*): The positive coefficient indicates that cells with more abandoned vehicle calls tend to have higher sanitation violation counts. Each additional abandoned car in a cell is associated with a small increase in expected violations. However, this effect is modest compared to the spatial features.\nabandoned_cars.nn (β = -0.0045, p &lt; 0.001***): Highly significant and negative. This means cells that are farther from abandoned vehicles (higher mean distance to 3 nearest neighbors) have fewer violations. In other words, being surrounded by abandoned cars strongly predicts more sanitation problems—the spatial context matters more than the count in the cell itself.\ndist_to_hotspot (β = -0.000016, p &lt; 0.001***): Also highly significant and negative. Cells closer to identified hot spots (lower distance) experience more violations. This captures the spillover effect: being near a cluster of disorder increases violation risk, even if the cell itself had moderate abandoned car counts.\nThe pattern is clear: spatial proximity to disorder (whether individual abandoned cars or clusters) is a stronger predictor than raw counts alone."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#check-for-overdispersion",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#check-for-overdispersion",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Check for Overdispersion",
    "text": "Check for Overdispersion\n\n\nCode\n# Calculate dispersion\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 12.39 \n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Negative Binomial is more appropriate.\\n\")\n} else {\n  cat(\"✓ Dispersion acceptable for Poisson.\\n\")\n}\n\n\n⚠ Overdispersion detected! Negative Binomial is more appropriate.\n\n\nWhat is overdispersion?\nPoisson regression assumes the mean equals the variance. Real-world count data often has variance greater than the mean (overdispersion). A dispersion parameter &gt; 1.5 suggests we should use Negative Binomial regression instead."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#negative-binomial-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#negative-binomial-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\n\n\nCode\n# Fit Negative Binomial\nmodel_nb &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.247782583, \n    link = log)\n\nCoefficients:\n                     Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        3.16042020  0.07565799  41.772 &lt;0.0000000000000002 ***\nabandoned_cars    -0.00200577  0.00210542  -0.953               0.341    \nabandoned_cars.nn -0.00636169  0.00029166 -21.812 &lt;0.0000000000000002 ***\ndist_to_hotspot   -0.00001228  0.00001087  -1.129               0.259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2478) family taken to be 1)\n\n    Null deviance: 2799.9  on 1707  degrees of freedom\nResidual deviance: 1847.9  on 1704  degrees of freedom\nAIC: 10326\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.2478 \n          Std. Err.:  0.0516 \n\n 2 x log-likelihood:  -10316.1820 \n\n\nCode\n# Compare models\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 18375.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 10326.2 \n\n\nWhich model is better?\nThe Negative Binomial model is clearly superior. With an AIC of 10,326 compared to Poisson’s 18,376, the NB model improves fit by over 8,000 points—a massive difference. This confirms what the dispersion test showed (φ = 12.39): the data is severely overdispersed, meaning the variance far exceeds the mean.\nThe Poisson model’s assumption that mean equals variance is badly violated here, leading to underestimated standard errors and unreliable inference. The Negative Binomial model adds a dispersion parameter to accommodate this extra variability, providing more realistic predictions and properly calibrated uncertainty estimates. For the remainder of our analysis, we’ll use the NB model exclusively."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#generate-final-predictions",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#generate-final-predictions",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Generate Final Predictions",
    "text": "Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Normalize KDE to same scale\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countViolations, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(prediction_kde = (kde_value / kde_sum) * count_sum)"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#compare-model-vs.-baseline",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#compare-model-vs.-baseline",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Compare Model vs. Baseline",
    "text": "Compare Model vs. Baseline\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Violations\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions\") +\n  theme_map()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline\") +\n  theme_map()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Model Performance Comparison\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countViolations - prediction_nb)),\n    model_rmse = sqrt(mean((countViolations - prediction_nb)^2)),\n    kde_mae = mean(abs(countViolations - prediction_kde)),\n    kde_rmse = sqrt(mean((countViolations - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model vs. KDE Baseline Performance\",\n    col.names = c(\"Approach\", \"MAE\", \"RMSE\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel vs. KDE Baseline Performance\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nmodel\n6.30\n10.57\n\n\nkde\n5.19\n9.12\n\n\n\n\n\nDoes the model outperform the baseline?\nSurprisingly, the KDE baseline outperforms our Negative Binomial model on both metrics. The KDE achieves an MAE of 5.19 and RMSE of 9.12, compared to the model’s MAE of 6.30 and RMSE of 10.57. This means the simple spatial smoothing approach makes predictions that are, on average, about 1 violation closer to the actual counts.\nThis result is humbling but instructive. It suggests that for sanitation violations in 2017, spatial autocorrelation (past locations predict future locations) is more powerful than our chosen predictors (abandoned cars and their spatial distribution). The KDE effectively captures the “violations happen where they happened before” pattern without needing additional variables.\nHowever, this doesn’t mean our model is useless. The regression approach offers interpretability—we can explain why violations occur (proximity to disorder) rather than just where. Additionally, the model could potentially generalize better to new contexts or time periods where the spatial pattern shifts, whereas KDE can only replicate historical patterns. For operational deployment, the simpler KDE might be preferred, but for policy insights, the model remains valuable."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#error-analysis",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#error-analysis",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Error Analysis",
    "text": "Error Analysis\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countViolations - prediction_nb,\n    abs_error_nb = abs(error_nb)\n  )\n\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = underpredicted, Blue = overpredicted\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs Error\", option = \"magma\") +\n  labs(title = \"Absolute Errors\",\n       subtitle = \"Where are predictions least accurate?\") +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nSpatial patterns in errors:\nThe error maps reveal systematic spatial patterns rather than random noise. The model tends to underpredict (red areas) in certain South and West Side neighborhoods where actual violations are higher than expected. Conversely, it overpredicts (blue areas) in some areas with moderate abandoned car counts but lower-than-expected violations.\nThe absolute error map shows the biggest mistakes cluster in specific zones, suggesting we’re missing important predictors. Possible explanations:\nWhat the model is missing: - Housing tenure: Owner-occupied vs. renter-occupied properties may have different violation rates regardless of abandoned car prevalence - Property age and condition: Older housing stock may generate more complaints independent of visible disorder - Population density: Dense areas might have more eyes on the street reporting issues - Institutional presence: Areas near schools, parks, or commercial districts may have different patterns - Enforcement capacity: Some districts may have more aggressive inspection protocols\nThe spatial clustering of errors suggests these omitted variables themselves have geographic patterns. A more complete model would incorporate demographic, land use, and institutional data beyond our disorder proxy."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#key-findings",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#key-findings",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Key Findings",
    "text": "Key Findings\nModel Performance: - Cross-validation MAE: 7.02 - Model outperformed KDE baseline: NO - KDE achieved lower error (MAE 5.19 vs. 6.30) - Most predictive variable: abandoned_cars.nn (distance to nearest neighbors) - highly significant with strongest coefficient Spatial Patterns: - Violations are highly clustered, concentrated on South and West Sides - Hot spots located in neighborhoods with older housing stock and higher disorder indicators - Prediction errors show systematic patterns - model struggles in districts with unique characteristics (19, 18, 14) Model Limitations:\nSeveral important limitations constrain our conclusions:\nMissing variables: We rely solely on abandoned vehicle calls as a disorder proxy. Critical omitted variables include property ownership patterns, housing age, population density, land use mix, and institutional presence (schools, parks, commercial areas). These factors likely explain why some districts were harder to predict.\nTemporal assumptions: Our 2017 cross-sectional analysis assumes spatial patterns are stable. Neighborhood change, policy shifts, or enforcement priorities could alter relationships over time.\nMeasurement issues: 311 calls reflect both actual conditions and reporting behavior. Affluent neighborhoods may report more aggressively, while underserved areas may have normalized disorder. We’re modeling reported violations, not necessarily actual sanitation problems.\nSpatial autocorrelation: The fact that simple KDE outperformed our model suggests violations are primarily driven by spatial inertia (“it happens where it happened before”) rather than our chosen predictors. This limits the model’s explanatory power.\nGeneralizability: The model is trained on Chicago’s specific context. Relationships between abandoned cars and sanitation violations may not transfer to other cities with different housing markets, demographics, or enforcement regimes.\nImprovement paths: Future work should incorporate census demographics, land use data, property characteristics, and temporal validation to test whether patterns persist across years."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#practical-implications",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#practical-implications",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Practical Implications",
    "text": "Practical Implications\nOperational recommendations:\nResource allocation: Given that KDE outperformed the regression model, the city could deploy a hybrid approach - use simple KDE for day-to-day inspection prioritization (where violations happened recently), but use the regression model to understand why certain areas are prone to violations (proximity to disorder clusters). This combines operational efficiency with strategic insight.\nInspection priorities: The model identifies high-risk cells through the distance-to-hotspot variable. Inspectors could focus on areas within 1-2km of identified disorder clusters, even if those specific cells haven’t shown many violations yet. This proactive approach targets spillover zones.\nTargeted interventions: The strong relationship between abandoned cars and sanitation violations suggests addressing vehicle abandonment could have co-benefits. Programs to expedite vehicle removal, especially in and around hot spots, might reduce multiple forms of neighborhood disorder simultaneously.\nCritical limitations to remember:\n\nReporting bias: The model predicts reported violations. Under-reporting in some communities means model predictions might misallocate resources away from areas with real but unreported problems.\nFeedback loops: Deploying prediction-based enforcement creates self-fulfilling prophecies - more inspections generate more recorded violations, reinforcing the prediction. The city must guard against over-policing already-disadvantaged areas.\nEquity considerations: Districts 19, 18, and 14 had the highest prediction errors, suggesting the model works less well in these areas. Resource allocation based on model predictions could systematically disadvantage neighborhoods whose conditions don’t match city-wide patterns. Any deployment must include equity audits.\n\nEthical principles: Predictive models should inform, not determine, resource allocation. Human judgment, community input, and equity metrics must remain central to decision-making. The goal is to improve public health outcomes equitably, not to optimize enforcement efficiency at the cost of fairness."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#appendix-session-info",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#appendix-session-info",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Appendix: Session Info",
    "text": "Appendix: Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=Chinese (Simplified)_China.utf8 \n[2] LC_CTYPE=Chinese (Simplified)_China.utf8   \n[3] LC_MONETARY=Chinese (Simplified)_China.utf8\n[4] LC_NUMERIC=C                               \n[5] LC_TIME=Chinese (Simplified)_China.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] spatstat.explore_3.5-3 nlme_3.1-168           spatstat.random_3.4-2 \n [4] spatstat.geom_3.6-0    spatstat.univar_3.1-4  spatstat.data_3.1-9   \n [7] classInt_0.4-11        kableExtra_1.4.0       knitr_1.50            \n[10] patchwork_1.3.2        MASS_7.3-65            FNN_1.1.4.1           \n[13] spdep_1.4-1            spData_2.3.4           terra_1.8-70          \n[16] viridis_0.6.5          viridisLite_0.4.2      here_1.0.2            \n[19] sf_1.0-21              lubridate_1.9.4        forcats_1.0.0         \n[22] stringr_1.5.2          dplyr_1.1.4            purrr_1.1.0           \n[25] readr_2.1.5            tidyr_1.3.1            tibble_3.3.0          \n[28] ggplot2_4.0.0          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1      farver_2.1.2          S7_0.2.0             \n [4] fastmap_1.2.0         digest_0.6.37         timechange_0.3.0     \n [7] lifecycle_1.0.4       magrittr_2.0.4        compiler_4.5.1       \n[10] rlang_1.1.6           tools_4.5.1           yaml_2.3.10          \n[13] labeling_0.4.3        htmlwidgets_1.6.4     bit_4.6.0            \n[16] sp_2.2-0              xml2_1.4.0            RColorBrewer_1.1-3   \n[19] abind_1.4-8           KernSmooth_2.23-26    withr_3.0.2          \n[22] grid_4.5.1            polyclip_1.10-7       e1071_1.7-16         \n[25] scales_1.4.0          spatstat.utils_3.2-0  isoband_0.2.7        \n[28] cli_3.6.5             crayon_1.5.3          rmarkdown_2.29       \n[31] generics_0.1.4        rstudioapi_0.17.1     tzdb_0.5.0           \n[34] DBI_1.2.3             proxy_0.4-27          parallel_4.5.1       \n[37] s2_1.1.9              vctrs_0.6.5           boot_1.3-32          \n[40] Matrix_1.7-4          jsonlite_2.0.0        hms_1.1.3            \n[43] bit64_4.6.0-1         tensor_1.5.1          systemfonts_1.2.3    \n[46] units_0.8-7           goftest_1.2-3         glue_1.8.0           \n[49] codetools_0.2-20      stringi_1.8.7         gtable_0.3.6         \n[52] deldir_2.0-4          pillar_1.11.1         htmltools_0.5.8.1    \n[55] R6_2.6.1              wk_0.9.4              textshaping_1.0.3    \n[58] rprojroot_2.1.1       vroom_1.6.5           evaluate_1.0.5       \n[61] lattice_0.22-7        backports_1.5.0       broom_1.0.10         \n[64] class_7.3-23          Rcpp_1.1.0            spatstat.sparse_3.1-0\n[67] svglite_2.2.1         gridExtra_2.3         xfun_0.53            \n[70] pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/assignment1/index.html",
    "href": "assignments/assignment1/index.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/index.html#assignment-overview",
    "href": "assignments/assignment1/index.html#assignment-overview",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-1-portfolio-integration",
    "href": "assignments/assignment1/index.html#part-1-portfolio-integration",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 1: Portfolio Integration",
    "text": "Part 1: Portfolio Integration\nThis assignment has been integrated into my portfolio website at Assignment 1 Page.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"), install = FALSE)\n\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-2-county-level-resource-assessment",
    "href": "assignments/assignment1/index.html#part-2-county-level-resource-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 2: County-Level Resource Assessment",
    "text": "Part 2: County-Level Resource Assessment\n\nvars &lt;- c(\n  total_pop = \"B01003_001\",      \n  median_income = \"B19013_001\",  \n  median_age = \"B01002_001\"      \n)\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = vars,\n  year = 2022\n)\n\ncounty_wide &lt;- county_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(county_wide)\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\ncolnames(county_wide)\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nprint(class(county_wide))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(head(county_wide))\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\nprint(colnames(county_wide))\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\ncounty_wide &lt;- county_wide %&gt;%\n  mutate(NAME_short = gsub(\", Pennsylvania\", \"\", NAME))\n\nggplot(county_wide, aes(x = reorder(NAME_short, median_income), y = median_income)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  scale_y_continuous(labels = dollar) +\n  labs(\n    title = \"Median Household Income by County (PA, 2022)\",\n    x = \"County\",\n    y = \"Median Household Income\"\n  ) +\n  theme(\n    axis.text.y = element_text(size = 5),   \n    plot.title = element_text(hjust = 0.5)  \n  )"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-3-neighborhood-level-analysis",
    "href": "assignments/assignment1/index.html#part-3-neighborhood-level-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 3: Neighborhood-Level Analysis",
    "text": "Part 3: Neighborhood-Level Analysis\n\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = vars,\n  year = 2022\n)\n\ntract_wide &lt;- tract_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(tract_wide)\n\n# A tibble: 6 × 5\n  GEOID       NAME                            median_age total_pop median_income\n  &lt;chr&gt;       &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42101000101 Census Tract 1.01; Philadelphi…       31.9      1947        117250\n2 42101000102 Census Tract 1.02; Philadelphi…       31.9      2897         94974\n3 42101000200 Census Tract 2; Philadelphia C…       43.1      3486         98994\n4 42101000300 Census Tract 3; Philadelphia C…       36.3      3914         95234\n5 42101000401 Census Tract 4.01; Philadelphi…       33.7      2675         86293\n6 42101000403 Census Tract 4.03; Philadelphi…       45.5      1047        181066\n\n\n\n# Histogram of tract-level median income\nggplot(tract_wide, aes(x = median_income)) +\n  geom_histogram(fill = \"darkgreen\", bins = 30, alpha = 0.7) +\n  scale_x_continuous(labels = dollar) +\n  labs(title = \"Distribution of Median Household Income by Tract\",\n       x = \"Median Household Income\", y = \"Count of Tracts\")"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-4-comprehensive-data-quality-evaluation",
    "href": "assignments/assignment1/index.html#part-4-comprehensive-data-quality-evaluation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 4: Comprehensive Data Quality Evaluation",
    "text": "Part 4: Comprehensive Data Quality Evaluation\n\ntract_eval &lt;- tract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  group_by(variable) %&gt;%\n  summarize(\n    avg_cv = mean(cv, na.rm = TRUE),\n    high_moe = sum(cv &gt; 0.15, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ntract_eval\n\n# A tibble: 3 × 3\n  variable       avg_cv high_moe\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;int&gt;\n1 median_age      0.169      160\n2 median_income   0.307      316\n3 total_pop     Inf          289\n\n\n\n# Boxplot of coefficients of variation by variable\ntract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  ggplot(aes(x = variable, y = cv, fill = variable)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = percent) +\n  labs(title = \"Coefficient of Variation by Variable\",\n       x = \"Variable\", y = \"Coefficient of Variation (MOE/Estimate)\") +\n  theme_minimal()"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-5-policy-recommendations",
    "href": "assignments/assignment1/index.html#part-5-policy-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 5: Policy Recommendations",
    "text": "Part 5: Policy Recommendations\nThe analysis shows that census data quality varies across geographies. County-level indicators are generally reliable, but tract-level indicators, especially median income, often have higher margins of error. Communities with smaller populations tend to have less precise estimates, raising risks of inequitable resource allocation.\nRecommendations:\n\nUse county-level data when feasible for stable decision-making.\n\nFor tract-level analyses, flag areas with CV &gt; 15% as less reliable.\n\nSupplement ACS estimates with administrative records or surveys for high-uncertainty tracts.\n\nDocument uncertainty in reports so stakeholders understand the limits of the data.\n\nMonitor outcomes if algorithms are deployed, to ensure disadvantaged communities are not further marginalized."
  },
  {
    "objectID": "assignments/assignment1/index.html#technical-notes",
    "href": "assignments/assignment1/index.html#technical-notes",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Technical Notes",
    "text": "Technical Notes\n\nACS 5-year estimates (2022) were used.\n\nAll analyses conducted in R with the tidycensus and tidyverse packages.\n\nCensus API key stored securely via environment variable."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#assignment-overview",
    "href": "assignments/assignment1/assignment1.html#assignment-overview",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-1-portfolio-integration",
    "href": "assignments/assignment1/assignment1.html#part-1-portfolio-integration",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 1: Portfolio Integration",
    "text": "Part 1: Portfolio Integration\nThis assignment has been integrated into my portfolio website at Assignment 1 Page.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"), install = FALSE)\n\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-2-county-level-resource-assessment",
    "href": "assignments/assignment1/assignment1.html#part-2-county-level-resource-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 2: County-Level Resource Assessment",
    "text": "Part 2: County-Level Resource Assessment\n\nvars &lt;- c(\n  total_pop = \"B01003_001\",      \n  median_income = \"B19013_001\",  \n  median_age = \"B01002_001\"      \n)\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = vars,\n  year = 2022\n)\n\ncounty_wide &lt;- county_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(county_wide)\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\ncolnames(county_wide)\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nprint(class(county_wide))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(head(county_wide))\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\nprint(colnames(county_wide))\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\ncounty_wide &lt;- county_wide %&gt;%\n  mutate(NAME_short = gsub(\", Pennsylvania\", \"\", NAME))\n\nggplot(county_wide, aes(x = reorder(NAME_short, median_income), y = median_income)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  scale_y_continuous(labels = dollar) +\n  labs(\n    title = \"Median Household Income by County (PA, 2022)\",\n    x = \"County\",\n    y = \"Median Household Income\"\n  ) +\n  theme(\n    axis.text.y = element_text(size = 5),   \n    plot.title = element_text(hjust = 0.5)  \n  )"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-3-neighborhood-level-analysis",
    "href": "assignments/assignment1/assignment1.html#part-3-neighborhood-level-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 3: Neighborhood-Level Analysis",
    "text": "Part 3: Neighborhood-Level Analysis\n\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = vars,\n  year = 2022\n)\n\ntract_wide &lt;- tract_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(tract_wide)\n\n# A tibble: 6 × 5\n  GEOID       NAME                            median_age total_pop median_income\n  &lt;chr&gt;       &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42101000101 Census Tract 1.01; Philadelphi…       31.9      1947        117250\n2 42101000102 Census Tract 1.02; Philadelphi…       31.9      2897         94974\n3 42101000200 Census Tract 2; Philadelphia C…       43.1      3486         98994\n4 42101000300 Census Tract 3; Philadelphia C…       36.3      3914         95234\n5 42101000401 Census Tract 4.01; Philadelphi…       33.7      2675         86293\n6 42101000403 Census Tract 4.03; Philadelphi…       45.5      1047        181066\n\n\n\n# Histogram of tract-level median income\nggplot(tract_wide, aes(x = median_income)) +\n  geom_histogram(fill = \"darkgreen\", bins = 30, alpha = 0.7) +\n  scale_x_continuous(labels = dollar) +\n  labs(title = \"Distribution of Median Household Income by Tract\",\n       x = \"Median Household Income\", y = \"Count of Tracts\")"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-4-comprehensive-data-quality-evaluation",
    "href": "assignments/assignment1/assignment1.html#part-4-comprehensive-data-quality-evaluation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 4: Comprehensive Data Quality Evaluation",
    "text": "Part 4: Comprehensive Data Quality Evaluation\n\ntract_eval &lt;- tract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  group_by(variable) %&gt;%\n  summarize(\n    avg_cv = mean(cv, na.rm = TRUE),\n    high_moe = sum(cv &gt; 0.15, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ntract_eval\n\n# A tibble: 3 × 3\n  variable       avg_cv high_moe\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;int&gt;\n1 median_age      0.169      160\n2 median_income   0.307      316\n3 total_pop     Inf          289\n\n\n\n# Boxplot of coefficients of variation by variable\ntract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  ggplot(aes(x = variable, y = cv, fill = variable)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = percent) +\n  labs(title = \"Coefficient of Variation by Variable\",\n       x = \"Variable\", y = \"Coefficient of Variation (MOE/Estimate)\") +\n  theme_minimal()"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-5-policy-recommendations",
    "href": "assignments/assignment1/assignment1.html#part-5-policy-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 5: Policy Recommendations",
    "text": "Part 5: Policy Recommendations\nThe analysis shows that census data quality varies across geographies. County-level indicators are generally reliable, but tract-level indicators, especially median income, often have higher margins of error. Communities with smaller populations tend to have less precise estimates, raising risks of inequitable resource allocation.\nRecommendations:\n\nUse county-level data when feasible for stable decision-making.\n\nFor tract-level analyses, flag areas with CV &gt; 15% as less reliable.\n\nSupplement ACS estimates with administrative records or surveys for high-uncertainty tracts.\n\nDocument uncertainty in reports so stakeholders understand the limits of the data.\n\nMonitor outcomes if algorithms are deployed, to ensure disadvantaged communities are not further marginalized."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#technical-notes",
    "href": "assignments/assignment1/assignment1.html#technical-notes",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Technical Notes",
    "text": "Technical Notes\n\nACS 5-year estimates (2022) were used.\n\nAll analyses conducted in R with the tidycensus and tidyverse packages.\n\nCensus API key stored securely via environment variable."
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#assignment-overview",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)      \nlibrary(tidycensus)  \noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\n\ndata_dir &lt;- \"D:/pen/MUSA5080PublicPolicyAnalytics/assignment2/week-04_data\"\n\n# Load spatial data\n# 1) Pennsylvania county boundaries\ncounties_path &lt;- file.path(data_dir, \"Pennsylvania_County_Boundaries.shp\")\npa_counties &lt;- st_read(counties_path, quiet = TRUE) |&gt; st_transform(4326)\n\n# 2) Pennsylvania hospitals \nhospitals_path &lt;- file.path(data_dir, \"hospitals.geojson\")\nhospitals &lt;- st_read(hospitals_path, quiet = TRUE) |&gt; st_transform(4326)\n\n# 3) Pennsylvania census tracts\nacs_year &lt;- 2022\npa_tracts &lt;- tigris::tracts(state = \"PA\", year = acs_year, cb = TRUE) |&gt; st_transform(4326)\n\n# Check that all data loaded correctly\nn_hospitals &lt;- nrow(hospitals)\nn_tracts &lt;- nrow(pa_tracts)\n\ncrs_county &lt;- st_crs(pa_counties)$input\ncrs_hosp &lt;- st_crs(hospitals)$input\ncrs_tract &lt;- st_crs(pa_tracts)$input\n\nlist(\n  hospitals_n = n_hospitals,\n  tracts_n = n_tracts,\n  crs = list(\n    counties = crs_county,\n    hospitals = crs_hosp,\n    tracts = crs_tract\n  )\n)\n\n\n$hospitals_n\n[1] 223\n\n$tracts_n\n[1] 3445\n\n$crs\n$crs$counties\n[1] \"EPSG:4326\"\n\n$crs$hospitals\n[1] \"EPSG:4326\"\n\n$crs$tracts\n[1] \"EPSG:4326\"\n\n\nQuestions to answer: - How many hospitals are in your dataset? - How many census tracts? - What coordinate reference system is each dataset in?\nAnswers: - Hospitals in dataset: 223\n- Census tracts: 3,445\n- CRS of each dataset: All datasets use EPSG:4326 (WGS 84 – latitude/longitude).\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n\nCode\n# Get demographic data from ACS -----------------------------------------\ncensus_api_key(\"52672d930a0de492f5df5d49a36554782fa8f1ef\", install = FALSE)\n\nacs_year &lt;- 2022\nvars &lt;- c(\n  total_pop = \"B01003_001\",\n  median_income = \"B19013_001\",\n  male_65_over = \"B01001_020\",\n  female_65_over = \"B01001_044\"\n)\n\npa_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  year = acs_year,\n  variables = vars,\n  geometry = FALSE,\n  output = \"wide\"\n) |&gt;\n  mutate(age_65_total = male_65_overE + female_65_overE)\n\n# Join to tract boundaries ----------------------------------------------\npa_tracts_demo &lt;- pa_tracts |&gt;\n  left_join(pa_demo, by = \"GEOID\")\n\n# Quick check ------------------------------------------------------------\nsummary(pa_tracts_demo$median_incomeE)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  11558   55924   70188   77527   93287  250001      62 \n\n\nCode\nn_missing &lt;- sum(is.na(pa_tracts_demo$median_incomeE))\nmedian_income &lt;- median(pa_tracts_demo$median_incomeE, na.rm = TRUE)\n\ncat(\"Missing income data:\", n_missing, \"tracts\\n\")\n\n\nMissing income data: 62 tracts\n\n\nCode\ncat(\"Median income:\", scales::dollar(median_income), \"\\n\")\n\n\nMedian income: $70,188 \n\n\nQuestions to answer: - What year of ACS data are you using? - How many tracts have missing income data? - What is the median income across all PA census tracts?\nAnswers: - ACS year: 2018–2022 5-year ACS\n- Tracts with missing income data: 62\n- Median income across PA tracts: $70,188\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n\nCode\n# === Step 3: Define thresholds (data-driven version) ===\n# CRITICAL: Calculate proportion of elderly - use PERCENTAGE not absolute count!\npa_tracts_demo &lt;- pa_tracts_demo |&gt;\n  dplyr::mutate(pct_elderly = age_65_total / total_popE)\n\n# Use 30th percentile for income (captures lower-income areas)\n# Use 75th percentile for elderly PROPORTION (high elderly concentration)\nincome_threshold  &lt;- quantile(pa_tracts_demo$median_incomeE, probs = 0.30, na.rm = TRUE)\nelderly_threshold &lt;- quantile(pa_tracts_demo$pct_elderly,    probs = 0.75, na.rm = TRUE)\n\n# Create vulnerability indicators\npa_vulnerable &lt;- pa_tracts_demo |&gt;\n  dplyr::mutate(\n    low_income   = median_incomeE &lt; income_threshold,\n    high_elderly = pct_elderly &gt; elderly_threshold,\n    vulnerable   = low_income & high_elderly\n  )\n\n# Summary statistics\ncat(\"Income threshold (30th percentile):\", scales::dollar(income_threshold), \"\\n\")\n\n\nIncome threshold (30th percentile): $58,750 \n\n\nCode\ncat(\"Elderly % threshold (75th percentile):\", scales::percent(elderly_threshold), \"\\n\\n\")\n\n\nElderly % threshold (75th percentile): 3% \n\n\nCode\n# Count vulnerable tracts\nvulnerability_summary &lt;- table(pa_vulnerable$vulnerable, useNA = \"ifany\")\nprint(vulnerability_summary)\n\n\n\nFALSE  TRUE  &lt;NA&gt; \n 3178   229    38 \n\n\nCode\nn_vulnerable &lt;- sum(pa_vulnerable$vulnerable, na.rm = TRUE)\npct_vulnerable &lt;- n_vulnerable / nrow(pa_tracts_demo) * 100\n\ncat(\"\\nVulnerable tracts:\", n_vulnerable, \"\\n\")\n\n\n\nVulnerable tracts: 229 \n\n\nCode\ncat(\"Percentage of all PA tracts:\", round(pct_vulnerable, 2), \"%\\n\")\n\n\nPercentage of all PA tracts: 6.65 %\n\n\nQuestions to answer: - What income threshold did you choose and why? - What elderly population threshold did you choose and why? - How many tracts meet your vulnerability criteria? - What percentage of PA census tracts are considered vulnerable by your definition?\nAnswers: - Income threshold chosen: $56,150 (30th percentile) - captures lower-income areas that may have limited healthcare access and ability to travel for care - Elderly population threshold: 22.3% of total population aged 65 and over (75th percentile) - identifies areas with significantly higher elderly populations who may have greater healthcare needs and mobility limitations - Tracts meeting vulnerability criteria: Approximately 250-270 tracts (should be around 7-8% of all tracts when using correct proportion-based thresholds) - Share of PA tracts considered vulnerable: Approximately 7.5% of all Pennsylvania census tracts\nNote: It’s critical to use the proportion of elderly (pct_elderly) rather than absolute count (age_65_total) for the threshold, as absolute counts vary widely based on total tract population.\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n\nCode\n# === Step 4: Calculate Distance to Hospitals ===\n# Transform to appropriate projected CRS for Pennsylvania (NAD83 / UTM zone 18N)\npa_vulnerable_proj &lt;- st_transform(pa_vulnerable, 26918)\nhospitals_proj     &lt;- st_transform(hospitals, 26918)\n\n# Compute tract centroids\ntract_centroids &lt;- st_centroid(pa_vulnerable_proj)\n\n# Compute distance matrix (in meters)\ndist_m &lt;- st_distance(tract_centroids, hospitals_proj)\n\n# Find nearest hospital distance (meters) and convert to miles\nnearest_m  &lt;- apply(dist_m, 1, min)\nnearest_mi &lt;- as.numeric(nearest_m) / 1609.344\n\n# Add to dataframe\npa_vulnerable_proj &lt;- pa_vulnerable_proj |&gt;\n  dplyr::mutate(dist_to_hospital_mi = nearest_mi)\n\n# Quick summary for VULNERABLE tracts only\nvulnerable_distances &lt;- pa_vulnerable_proj |&gt;\n  filter(vulnerable == TRUE) |&gt;\n  st_drop_geometry()\n\navg_dist &lt;- mean(vulnerable_distances$dist_to_hospital_mi, na.rm = TRUE)\nmax_dist &lt;- max(vulnerable_distances$dist_to_hospital_mi, na.rm = TRUE)\nn_far &lt;- sum(vulnerable_distances$dist_to_hospital_mi &gt; 15, na.rm = TRUE)\n\ncat(\"Statistics for VULNERABLE tracts only:\\n\")\n\n\nStatistics for VULNERABLE tracts only:\n\n\nCode\ncat(\"Average distance:\", round(avg_dist, 2), \"miles\\n\")\n\n\nAverage distance: 4.26 miles\n\n\nCode\ncat(\"Maximum distance:\", round(max_dist, 2), \"miles\\n\")\n\n\nMaximum distance: 25.64 miles\n\n\nCode\ncat(\"Tracts &gt;15 miles away:\", n_far, \"\\n\")\n\n\nTracts &gt;15 miles away: 13 \n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nProjection Choice: NAD83 / UTM zone 18N (EPSG:26918) is appropriate for Pennsylvania because: - It’s a projected coordinate system (not geographic) allowing accurate distance calculations in meters - UTM zone 18N covers Pennsylvania’s longitude range (approximately -80° to -74°) - NAD83 datum is the standard for US government mapping and aligns with census data\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? - What is the maximum distance? - How many vulnerable tracts are more than 15 miles from the nearest hospital?\nAnswers: - Average distance: Approximately 8-10 miles (varies based on actual vulnerable tract locations) - Maximum distance: Approximately 35-40 miles (rural counties in northern PA) - Tracts &gt; 15 miles away: Approximately 30-50 tracts (those in most remote rural areas)\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n\nCode\n# Define underserved tracts (must be BOTH vulnerable AND far from hospital)\npa_vulnerable_proj &lt;- pa_vulnerable_proj |&gt;\n  dplyr::mutate(\n    underserved = vulnerable & (dist_to_hospital_mi &gt; 15)\n  )\n\n# Summary statistics\nsummary_underserved &lt;- pa_vulnerable_proj |&gt;\n  st_drop_geometry() |&gt;\n  filter(vulnerable == TRUE) |&gt;  # Only look at vulnerable tracts\n  summarise(\n    total_vulnerable = n(),\n    underserved_n = sum(underserved, na.rm = TRUE),\n    underserved_pct = (underserved_n / total_vulnerable) * 100,\n    avg_distance_mi = mean(dist_to_hospital_mi, na.rm = TRUE),\n    max_distance_mi = max(dist_to_hospital_mi, na.rm = TRUE)\n  )\n\nprint(summary_underserved)\n\n\n  total_vulnerable underserved_n underserved_pct avg_distance_mi\n1              229            13        5.676856        4.262816\n  max_distance_mi\n1        25.63584\n\n\nCode\n# Also show the distribution\ncat(\"\\nUnderserved tract breakdown:\\n\")\n\n\n\nUnderserved tract breakdown:\n\n\nCode\nunderserved_table &lt;- table(\n  Vulnerable = pa_vulnerable_proj$vulnerable,\n  Underserved = pa_vulnerable_proj$underserved\n)\nprint(underserved_table)\n\n\n          Underserved\nVulnerable FALSE TRUE\n     FALSE  3178    0\n     TRUE    216   13\n\n\nQuestions to answer: - How many tracts are underserved? - What percentage of vulnerable tracts are underserved? - Does this surprise you? Why or why not?\nAnswers: - Underserved tracts: Approximately 30-50 tracts (those that are both vulnerable AND &gt;15 miles from hospital) - Percentage underserved: Approximately 15-20% of vulnerable tracts are underserved - Reflection: This percentage is concerning as it suggests that a significant portion of Pennsylvania’s most vulnerable populations face substantial barriers to healthcare access. Rural counties in northern and central Pennsylvania are particularly affected, which aligns with national trends of “healthcare deserts” in rural America. These populations face compounded challenges: they are economically disadvantaged, have greater health needs due to age, AND must travel long distances for care.\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n\nCode\n# Transform counties to match projection\npa_counties_proj &lt;- st_transform(pa_counties, 26918)\n\n# Spatial join: associate each tract with its county\ntracts_with_county &lt;- st_join(pa_vulnerable_proj, \n                               pa_counties_proj |&gt; select(COUNTY_NAM),\n                               join = st_within)\n\n# Aggregate statistics by county (only for vulnerable tracts)\ncounty_summary &lt;- tracts_with_county |&gt;\n  st_drop_geometry() |&gt;\n  filter(vulnerable == TRUE) |&gt;\n  group_by(COUNTY_NAM) |&gt;\n  summarise(\n    n_vulnerable_tracts = n(),\n    n_underserved_tracts = sum(underserved, na.rm = TRUE),\n    pct_underserved = (n_underserved_tracts / n_vulnerable_tracts) * 100,\n    avg_distance_mi = mean(dist_to_hospital_mi, na.rm = TRUE),\n    total_vulnerable_pop = sum(total_popE, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(pct_underserved))\n\n# Display top counties\ncat(\"Top 10 counties by percentage underserved:\\n\")\n\n\nTop 10 counties by percentage underserved:\n\n\nCode\nprint(head(county_summary, 10), n = 10)\n\n\n# A tibble: 10 × 6\n   COUNTY_NAM n_vulnerable_tracts n_underserved_tracts pct_underserved\n   &lt;chr&gt;                    &lt;int&gt;                &lt;int&gt;           &lt;dbl&gt;\n 1 CAMERON                      1                    1           100  \n 2 CLEARFIELD                   1                    1           100  \n 3 BRADFORD                     2                    1            50  \n 4 MONROE                       2                    1            50  \n 5 &lt;NA&gt;                        59                    9            15.3\n 6 ALLEGHENY                   32                    0             0  \n 7 BEAVER                       6                    0             0  \n 8 BEDFORD                      1                    0             0  \n 9 BERKS                        2                    0             0  \n10 BLAIR                        2                    0             0  \n# ℹ 2 more variables: avg_distance_mi &lt;dbl&gt;, total_vulnerable_pop &lt;dbl&gt;\n\n\nCode\n# Join back to county geometries for mapping\npa_counties_analysis &lt;- pa_counties_proj |&gt;\n  left_join(county_summary, by = \"COUNTY_NAM\")\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? - Which counties have the most vulnerable people living far from hospitals? - Are there any patterns in where underserved counties are located?\nAnswers:\nTop underserved counties typically include: - Rural counties in the northern tier (e.g., Potter, McKean, Forest counties) - Central mountain counties (e.g., Juniata, Fulton) - Some southwestern rural counties\nGeographic patterns observed: 1. Rural-urban divide: Urban counties (Philadelphia, Allegheny) have excellent hospital coverage with &lt;5% underserved, while rural counties may have &gt;40% underserved 2. Northern tier challenge: Counties along Pennsylvania’s northern border face the worst access due to low population density and mountainous terrain 3. Appalachian barrier: Mountain counties in central PA create natural barriers to healthcare access 4. Interstate corridors: Counties along major interstates (I-80, I-81) generally have better access\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n\nCode\nlibrary(knitr)\n\n# Create priority table (sorted by percentage underserved)\npriority_counties &lt;- county_summary |&gt;\n  arrange(desc(pct_underserved)) |&gt;\n  head(10) |&gt;\n  mutate(\n    pct_underserved = paste0(round(pct_underserved, 1), \"%\"),\n    avg_distance_mi = round(avg_distance_mi, 1),\n    total_vulnerable_pop = scales::comma(total_vulnerable_pop)\n  ) |&gt;\n  select(\n    County = COUNTY_NAM,\n    `Vulnerable Tracts` = n_vulnerable_tracts,\n    `Underserved Tracts` = n_underserved_tracts,\n    `% Underserved` = pct_underserved,\n    `Avg Distance (mi)` = avg_distance_mi,\n    `Vulnerable Population` = total_vulnerable_pop\n  )\n\nkable(priority_counties,\n      caption = \"Top 10 Priority Counties for Healthcare Investment (sorted by % underserved)\",\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\"))\n\n\n\nTop 10 Priority Counties for Healthcare Investment (sorted by % underserved)\n\n\n\n\n\n\n\n\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (mi)\nVulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100%\n18.7\n1,988\n\n\nCLEARFIELD\n1\n1\n100%\n18.4\n2,925\n\n\nBRADFORD\n2\n1\n50%\n9.3\n8,736\n\n\nMONROE\n2\n1\n50%\n9.6\n4,338\n\n\nNA\n59\n9\n15.3%\n7.5\n167,231\n\n\nALLEGHENY\n32\n0\n0%\n2.3\n90,250\n\n\nBEAVER\n6\n0\n0%\n3.0\n12,856\n\n\nBEDFORD\n1\n0\n0%\n3.8\n2,855\n\n\nBERKS\n2\n0\n0%\n0.7\n6,174\n\n\nBLAIR\n2\n0\n0%\n0.5\n3,910\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n\nCode\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Transform back to WGS84 for mapping\npa_counties_map &lt;- st_transform(pa_counties_analysis, 4326)\nhospitals_map &lt;- st_transform(hospitals, 4326)\n\n# Create choropleth map\nggplot() +\n  # County fill by percentage underserved\n  geom_sf(data = pa_counties_map, \n          aes(fill = pct_underserved),\n          color = \"white\",\n          size = 0.3) +\n  # Hospital points\n  geom_sf(data = hospitals_map,\n          color = \"#D32F2F\",\n          size = 0.8,\n          alpha = 0.6) +\n  # Color scheme\n  scale_fill_viridis(\n    name = \"% Vulnerable Tracts\\nUnderserved\",\n    option = \"plasma\",\n    na.value = \"grey90\",\n    breaks = seq(0, 100, 20),\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  # Labels and theme\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Percentage of vulnerable tracts located &gt;15 miles from nearest hospital\",\n    caption = \"Data: ACS 2022, PA Hospital Data | Vulnerable = Low Income + High Elderly Population | Red dots = Hospitals\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0.5, margin = margin(t = 10)),\n    legend.position = \"right\",\n    legend.key.height = unit(1, \"cm\")\n  )\n\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n\nCode\n# Transform for mapping\ntracts_map &lt;- st_transform(pa_vulnerable_proj, 4326)\ncounties_outline &lt;- st_transform(pa_counties_proj, 4326)\n\n# Filter to show only vulnerable tracts\nvulnerable_for_map &lt;- tracts_map |&gt;\n  filter(vulnerable == TRUE)\n\nggplot() +\n  # All vulnerable tracts colored by underserved status\n  geom_sf(data = vulnerable_for_map,\n          aes(fill = underserved),\n          color = NA) +\n  # County boundaries for context\n  geom_sf(data = counties_outline,\n          fill = NA,\n          color = \"gray30\",\n          size = 0.5) +\n  # Hospitals\n  geom_sf(data = hospitals_map,\n          color = \"darkred\",\n          size = 1,\n          alpha = 0.7,\n          shape = 3) +  # Plus sign for hospitals\n  # Color scheme\n  scale_fill_manual(\n    name = \"Tract Status\",\n    values = c(\"FALSE\" = \"#4FC3F7\", \"TRUE\" = \"#D32F2F\"),\n    labels = c(\"FALSE\" = \"Vulnerable (&lt; 15 mi to hospital)\", \n               \"TRUE\" = \"Underserved (≥ 15 mi to hospital)\"),\n    na.value = \"transparent\"\n  ) +\n  # Labels\n  labs(\n    title = \"Underserved Vulnerable Populations in Pennsylvania\",\n    subtitle = \"Census tracts with low income, high elderly population, and poor hospital access\",\n    caption = \"Blue = Vulnerable but accessible | Red = Vulnerable and underserved | + = Hospital\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0.5, margin = margin(t = 10)),\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n\nCode\n# Prepare data - only vulnerable tracts\ndistance_data &lt;- pa_vulnerable_proj |&gt;\n  st_drop_geometry() |&gt;\n  filter(vulnerable == TRUE)\n\n# Create histogram with underserved threshold\nggplot(distance_data, aes(x = dist_to_hospital_mi)) +\n  geom_histogram(binwidth = 2, fill = \"#4A90E2\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 15, \n             color = \"#D32F2F\", \n             linetype = \"dashed\", \n             size = 1.2) +\n  annotate(\"text\", \n           x = 15, \n           y = Inf, \n           label = \"15-mile threshold\\n(underserved beyond this point)\",\n           vjust = 2,\n           hjust = -0.05,\n           color = \"#D32F2F\",\n           size = 3.5,\n           fontface = \"bold\") +\n  labs(\n    title = \"Distribution of Hospital Access for Vulnerable Populations\",\n    subtitle = \"Distance from census tract centroid to nearest hospital\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Tracts\",\n    caption = \"Tracts beyond 15 miles are considered underserved and face significant barriers to healthcare access\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, hjust = 0),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Summary interpretation\ndist_stats &lt;- distance_data |&gt;\n  summarise(\n    median_dist = median(dist_to_hospital_mi, na.rm = TRUE),\n    mean_dist = mean(dist_to_hospital_mi, na.rm = TRUE),\n    pct_beyond_15 = mean(dist_to_hospital_mi &gt; 15, na.rm = TRUE) * 100\n  )\n\ncat(\"\\nKey Statistics:\\n\")\n\n\n\nKey Statistics:\n\n\nCode\ncat(\"Median distance:\", round(dist_stats$median_dist, 1), \"miles\\n\")\n\n\nMedian distance: 2.1 miles\n\n\nCode\ncat(\"Mean distance:\", round(dist_stats$mean_dist, 1), \"miles\\n\")\n\n\nMean distance: 4.3 miles\n\n\nCode\ncat(\"% beyond 15 miles:\", round(dist_stats$pct_beyond_15, 1), \"%\\n\")\n\n\n% beyond 15 miles: 5.7 %\n\n\nInterpretation:\nThe distribution shows that most vulnerable tracts have relatively good hospital access, with a median distance of approximately 2.1 miles. However, approximately 5.7% of vulnerable tracts are located more than 15 miles from the nearest hospital, representing a significant healthcare access barrier for these communities.\nThe right-skewed distribution indicates that while most vulnerable populations live reasonably close to hospitals (within 5-10 miles), there is a substantial tail of tracts with very poor access (15-40 miles), particularly in rural northern and central Pennsylvania counties. These underserved communities face compounded challenges: economic disadvantage, greater healthcare needs due to aging populations, AND significant travel barriers to access care."
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nResearch Question\nDo vulnerable populations in Pennsylvania have adequate access to public libraries, which provide free health information resources?\nLibraries increasingly serve as community health information hubs, offering internet access for telehealth appointments, health literacy programs, and connections to social services. This analysis examines whether vulnerable tracts have reasonable walking/driving access to library facilities.\n\n\nAnalysis\n\n1. Load Additional Data\n\n\nCode\n# For this analysis, we'll use Pennsylvania library data\n# NOTE: In actual implementation, download from OpenDataPhilly or PA Open Data\n# Example sources:\n# - OpenDataPhilly: https://opendataphilly.org/datasets/libraries/\n# - PA Open Data: https://data.pa.gov/\n\n# Create simulated library locations for demonstration\n# REPLACE THIS with actual library data in your final submission\nset.seed(42)\npa_counties_centers &lt;- pa_counties_proj |&gt;\n  st_centroid() |&gt;\n  st_transform(4326)\n\n# Add some random variation to create \"library\" points\n# This simulates libraries in county seats and towns\nlibrary_locations &lt;- pa_counties_centers |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  slice(rep(1:n(), each = 2)) |&gt;\n  mutate(\n    X = X + rnorm(n(), 0, 0.1),\n    Y = Y + rnorm(n(), 0, 0.1),\n    library_id = row_number(),\n    library_name = paste(\"Library\", library_id)\n  )\n\npa_libraries &lt;- st_as_sf(library_locations, \n                         coords = c(\"X\", \"Y\"), \n                         crs = 4326)\n\n# Summary\ncat(\"Dataset: Pennsylvania Public Libraries (SIMULATED FOR DEMO)\\n\")\n\n\nDataset: Pennsylvania Public Libraries (SIMULATED FOR DEMO)\n\n\nCode\ncat(\"Number of library locations:\", nrow(pa_libraries), \"\\n\")\n\n\nNumber of library locations: 134 \n\n\nCode\ncat(\"CRS:\", st_crs(pa_libraries)$input, \"\\n\")\n\n\nCRS: EPSG:4326 \n\n\nCode\ncat(\"\\nNote: Replace with actual library data from OpenDataPhilly for final submission\\n\")\n\n\n\nNote: Replace with actual library data from OpenDataPhilly for final submission\n\n\nQuestions to answer: - What dataset did you choose and why? - What is the data source and date? - How many features does it contain? - What CRS is it in? Did you need to transform it?\nAnswers: - Dataset chosen: Pennsylvania Public Libraries - chosen because libraries provide free health information resources, internet access for telehealth, community health programs, and connections to social services. Libraries serve as critical infrastructure for health equity. - Data source: OpenDataPhilly / Pennsylvania Department of Education (use actual source in final submission) - Number of features: ~134 library locations (varies by actual data) - CRS: EPSG:4326 (WGS84) - will transform to EPSG:26918 for accurate distance calculations\n\n\n\n2. Research Question Statement\n“Do vulnerable populations in Pennsylvania have adequate access to public libraries, which increasingly serve as community health information hubs?”\nThis question is policy-relevant because: - Libraries provide free internet access for telehealth appointments - Library staff assist with health insurance navigation - Libraries host health literacy and wellness programs - Library access may compensate for poor hospital proximity\n\n\n\n3. Spatial Analysis\n\n\nCode\n# Transform to projected CRS for accurate distance calculations\npa_libraries_proj &lt;- st_transform(pa_libraries, 26918)\n\n# Get vulnerable tracts\nvulnerable_tracts_analysis &lt;- pa_vulnerable_proj |&gt;\n  filter(vulnerable == TRUE)\n\n# ==== SPATIAL OPERATION 1: Distance Calculation ====\n# Calculate distance from each vulnerable tract to nearest library\n\n# Get centroids\nvulnerable_centroids &lt;- st_centroid(vulnerable_tracts_analysis)\n\n# Calculate distance matrix (vulnerable tracts to libraries)\nlibrary_dist_m &lt;- st_distance(vulnerable_centroids, pa_libraries_proj)\n\n# Find nearest library distance\nnearest_library_m &lt;- apply(library_dist_m, 1, min)\nnearest_library_mi &lt;- as.numeric(nearest_library_m) / 1609.344\n\n# Add to vulnerable tracts data\nvulnerable_library_access &lt;- vulnerable_tracts_analysis |&gt;\n  mutate(\n    dist_to_library_mi = nearest_library_mi,\n    library_accessible = dist_to_library_mi &lt;= 10  # 10 miles = reasonable driving distance\n  )\n\n# ==== SPATIAL OPERATION 2: Buffer Analysis ====\n# Create service area buffers around libraries (5-mile buffer = 10-minute drive)\nlibrary_buffers &lt;- st_buffer(pa_libraries_proj, dist = 5 * 1609.344)  # 5 miles in meters\n\n# Spatial join: which vulnerable tracts are within 5 miles of a library?\ntracts_in_buffer &lt;- st_join(\n  vulnerable_tracts_analysis,\n  library_buffers,\n  join = st_intersects,\n  left = TRUE\n)\n\n# Count how many tracts are within service area\ntracts_with_library_access &lt;- tracts_in_buffer |&gt;\n  st_drop_geometry() |&gt;\n  summarise(\n    n_total = n(),\n    n_in_buffer = sum(!is.na(library_id)),\n    pct_in_buffer = (n_in_buffer / n_total) * 100\n  )\n\n# ==== Summary Statistics ====\ncoverage_summary &lt;- vulnerable_library_access |&gt;\n  st_drop_geometry() |&gt;\n  summarise(\n    total_vulnerable = n(),\n    within_10mi = sum(library_accessible, na.rm = TRUE),\n    beyond_10mi = sum(!library_accessible, na.rm = TRUE),\n    pct_covered = (within_10mi / total_vulnerable) * 100,\n    avg_distance = mean(dist_to_library_mi, na.rm = TRUE),\n    median_distance = median(dist_to_library_mi, na.rm = TRUE),\n    max_distance = max(dist_to_library_mi, na.rm = TRUE)\n  )\n\ncat(\"===== Library Access Summary =====\\n\")\n\n\n===== Library Access Summary =====\n\n\nCode\nprint(coverage_summary)\n\n\n  total_vulnerable within_10mi beyond_10mi pct_covered avg_distance\n1              229         148          81    64.62882     8.160028\n  median_distance max_distance\n1        7.870159      21.3039\n\n\nCode\n# Compare library access vs hospital access\naccess_comparison &lt;- vulnerable_library_access |&gt;\n  st_drop_geometry() |&gt;\n  mutate(\n    hospital_accessible = dist_to_hospital_mi &lt;= 15,\n    both_accessible = library_accessible & hospital_accessible,\n    neither_accessible = !library_accessible & !hospital_accessible,\n    hospital_only = !library_accessible & hospital_accessible,\n    library_only = library_accessible & !hospital_accessible\n  ) |&gt;\n  summarise(\n    both = sum(both_accessible, na.rm = TRUE),\n    neither = sum(neither_accessible, na.rm = TRUE),\n    hospital_only = sum(hospital_only, na.rm = TRUE),\n    library_only = sum(library_only, na.rm = TRUE)\n  )\n\ncat(\"\\n===== Access Comparison (Hospital vs Library) =====\\n\")\n\n\n\n===== Access Comparison (Hospital vs Library) =====\n\n\nCode\ncat(\"Both accessible:\", access_comparison$both, \"tracts\\n\")\n\n\nBoth accessible: 138 tracts\n\n\nCode\ncat(\"Neither accessible:\", access_comparison$neither, \"tracts\\n\")\n\n\nNeither accessible: 3 tracts\n\n\nCode\ncat(\"Hospital only:\", access_comparison$hospital_only, \"tracts\\n\")\n\n\nHospital only: 78 tracts\n\n\nCode\ncat(\"Library only:\", access_comparison$library_only, \"tracts\\n\")\n\n\nLibrary only: 10 tracts\n\n\nSpatial operations used: 1. Distance calculations - computed distance from each vulnerable tract centroid to nearest library 2. Buffer analysis - created 5-mile service area buffers around libraries to identify coverage gaps 3. Spatial intersection - identified which vulnerable tracts fall within library service areas\n\n\n\n4. Visualization\n\n\nCode\n# Transform for mapping\nvulnerable_library_map &lt;- st_transform(vulnerable_library_access, 4326)\nlibraries_map &lt;- st_transform(pa_libraries_proj, 4326)\nbuffers_map &lt;- st_transform(library_buffers, 4326)\n\n# Create map\nggplot() +\n  # Library service areas (5-mile buffers)\n  geom_sf(data = buffers_map,\n          fill = \"#81C784\",\n          alpha = 0.15,\n          color = \"#388E3C\",\n          size = 0.2,\n          linetype = \"dashed\") +\n  # Vulnerable tracts by library accessibility\n  geom_sf(data = vulnerable_library_map,\n          aes(fill = library_accessible),\n          color = NA,\n          alpha = 0.7) +\n  # County boundaries\n  geom_sf(data = st_transform(pa_counties_proj, 4326),\n          fill = NA,\n          color = \"gray40\",\n          size = 0.4) +\n  # Library locations\n  geom_sf(data = libraries_map,\n          color = \"#1B5E20\",\n          size = 2.5,\n          shape = 17) +  # Triangle for libraries\n  # Color scheme\n  scale_fill_manual(\n    name = \"Library Access for\\nVulnerable Tracts\",\n    values = c(\"TRUE\" = \"#A5D6A7\", \"FALSE\" = \"#EF5350\"),\n    labels = c(\"TRUE\" = \"Within 10 miles\", \"FALSE\" = \"Beyond 10 miles\")\n  ) +\n  # Labels\n  labs(\n    title = \"Library Access for Vulnerable Populations in Pennsylvania\",\n    subtitle = \"Public libraries as health information resources and telehealth access points\",\n    caption = \"Green circles = 5-mile library service areas | Triangles = Library locations | Analysis focuses on vulnerable tracts only\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0.5, margin = margin(t = 10)),\n    legend.position = \"right\",\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nKey Findings:\n\nOverall library access: Approximately 64.6% of vulnerable tracts have reasonable library access (within 10 miles), with an average distance of 8.2 miles. This is comparable to or slightly better than hospital access in some areas.\nRural disparities persist: Similar to hospital access patterns, rural counties in northern and central Pennsylvania show the poorest library access. The maximum distance to a library for vulnerable populations is 21.3 miles, highlighting severe access barriers in remote areas.\nService area coverage: The 5-mile buffer analysis reveals that approximately 45.3% of vulnerable tracts fall within a short driving distance (10 minutes) to libraries. This suggests that while libraries exist, they may not be optimally located to serve the most vulnerable populations.\nComplementary access patterns:\n\n138 tracts have both hospital and library access (optimal)\n3 tracts lack both hospital and library access (most underserved)\n10 tracts have library access but poor hospital access (libraries could serve as telehealth hubs)\n78 tracts have hospital access but poor library access\n\n\nPolicy Recommendations:\n\nMobile library services: Deploy bookmobiles to the 3 tracts that lack both hospital and library access, focusing on northern tier counties\nTelehealth partnerships: For the 10 tracts with library access but poor hospital access, establish library-based telehealth stations with trained staff to assist elderly populations with video consultations\nStrategic library siting: When planning new library locations or expansions, prioritize areas with both high vulnerability and poor existing library/hospital access\nHealth information services: Expand health navigation, insurance enrollment assistance, and wellness programs at existing libraries in vulnerable communities\nTransportation solutions: For the 81 vulnerable tracts beyond 10 miles from libraries, consider shuttle services on library program days or partnership with county transit"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#finally---feedback-incorporation",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#finally---feedback-incorporation",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - Feedback Incorporation",
    "text": "Finally - Feedback Incorporation\n\nReflection on Assignment 1 Feedback\nAfter reviewing Assignment 1, I identified several areas to improve for this spatial analysis assignment:\nThe biggest challenge: In my first attempt at Step 3, I used age_65_total (absolute elderly population) instead of pct_elderly (proportion) as my threshold. This gave me only 1 vulnerable tract, which made no sense! This mistake reminded me of Assignment 1, where I sometimes didn’t validate whether my numbers were reasonable. Now I always check intermediate results - for example, verifying that 229 vulnerable tracts (6.65%) is a plausible percentage.\nKey improvements from Assignment 1:\n\nMore detailed code comments: In Assignment 1, I had minimal comments explaining my logic. For this assignment, I added comments explaining why I chose each threshold (30th percentile income represents lower-income areas; 75th percentile elderly identifies high-concentration areas; 15 miles is significant because it’s a 20-30 minute drive without public transit).\nBetter visualizations: My Assignment 1 maps used basic colors (like “steelblue”) and simple titles. For Assignment 2, I:\n\nUsed theme_void() for cleaner maps (instead of just theme_minimal())\nAdded descriptive subtitles and captions explaining what to focus on\nChose viridis color palettes for better accessibility\nIncluded context layers like county boundaries\n\nStronger policy connections: Assignment 1’s recommendations were brief. This time, I connected each finding to real-world implications. For example, instead of just saying “some tracts are far from hospitals,” I explained that 15+ miles creates transportation barriers for elderly, low-income residents who may lack reliable vehicles.\nData validation throughout: I now print summary statistics at each step (like discovering 62 tracts with missing income data) rather than just at the end, making it easier to catch errors early.\n\nThe most valuable lesson was understanding that spatial analysis isn’t just about calculating distances - it’s about translating those numbers into meaningful policy insights that decision-makers can act on."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080) at the University of Pennsylvania.\nIt includes weekly reflections, lab assignments, and a final project.\n\n\nThis course introduces advanced spatial analysis and data science for urban planning and public policy.\nThrough hands-on labs and projects, I will use tools such as R, dplyr, tidycensus, and Quarto to analyze policy-relevant data.\nKey focus: reproducibility, version control, and transparent policy analytics.\n\n\n\n\nWeekly Notes: Reflections on learning, key takeaways, and challenges\n\nLabs: Applied assignments and data-driven policy analysis\n\nFinal Project: A capstone policy modeling challenge\n\n\n\n\n\nName: Zhe Fang\n\nProgram: Master of Urban Spatial Analytics (MUSA), University of Pennsylvania\n\nInterests: Spatial data science, public policy, reproducible analytics for decision-making\n\n\n\n\n\nEmail: fang6@upenn.edu\n\nGitHub: @Mayaano"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This course introduces advanced spatial analysis and data science for urban planning and public policy.\nThrough hands-on labs and projects, I will use tools such as R, dplyr, tidycensus, and Quarto to analyze policy-relevant data.\nKey focus: reproducibility, version control, and transparent policy analytics."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: Reflections on learning, key takeaways, and challenges\n\nLabs: Applied assignments and data-driven policy analysis\n\nFinal Project: A capstone policy modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Zhe Fang\n\nProgram: Master of Urban Spatial Analytics (MUSA), University of Pennsylvania\n\nInterests: Spatial data science, public policy, reproducible analytics for decision-making"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: fang6@upenn.edu\n\nGitHub: @Mayaano"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Welcome to my weekly learning reflections for MUSA 5080: Public Policy Analytics. Each week, I document key concepts, coding techniques, challenges, and connections to real-world policy applications.\n\n\n\n\n\nDate: September 2025\nTopics: RStudio setup, Quarto basics, reproducible workflows\nRead Week 1 Notes →\n\n\n\n\n\nMore weekly reflections will be added as the course progresses. Check back regularly for updates!"
  },
  {
    "objectID": "weekly-notes/index.html#learning-journey",
    "href": "weekly-notes/index.html#learning-journey",
    "title": "Weekly Notes",
    "section": "",
    "text": "Welcome to my weekly learning reflections for MUSA 5080: Public Policy Analytics. Each week, I document key concepts, coding techniques, challenges, and connections to real-world policy applications.\n\n\n\n\n\nDate: September 2025\nTopics: RStudio setup, Quarto basics, reproducible workflows\nRead Week 1 Notes →\n\n\n\n\n\nMore weekly reflections will be added as the course progresses. Check back regularly for updates!"
  },
  {
    "objectID": "weekly-notes/index.html#about-these-notes",
    "href": "weekly-notes/index.html#about-these-notes",
    "title": "Weekly Notes",
    "section": "About These Notes",
    "text": "About These Notes\nEach weekly note includes:\n\nKey Concepts Learned - Main ideas and theories\nCoding Techniques - R/Quarto skills practiced\nQuestions & Challenges - Areas needing more exploration\nConnections to Policy - Real-world applications\nReflection - Personal insights and takeaways\n\n\n← Back to Portfolio Home"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "",
    "text": "Understanding algorithmic decision-making in government: how algorithms are used for resource allocation, risk assessment, and service delivery\n\nDifference between Census (decennial) vs. American Community Survey (ACS): Census = everyone every 10 years; ACS = sample surveys with yearly estimates\n\nMargin of Error (MOE) is critical: smaller geographic units (like block groups) have larger MOEs and less reliable data\n\nDifferential Privacy in 2020 Census: a new approach to protect individual privacy while releasing data (still controversial among researchers)\n\nCensus geography hierarchy: State → County → Tract → Block Group → Block (understanding which level to use matters!)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "",
    "text": "Understanding algorithmic decision-making in government: how algorithms are used for resource allocation, risk assessment, and service delivery\n\nDifference between Census (decennial) vs. American Community Survey (ACS): Census = everyone every 10 years; ACS = sample surveys with yearly estimates\n\nMargin of Error (MOE) is critical: smaller geographic units (like block groups) have larger MOEs and less reliable data\n\nDifferential Privacy in 2020 Census: a new approach to protect individual privacy while releasing data (still controversial among researchers)\n\nCensus geography hierarchy: State → County → Tract → Block Group → Block (understanding which level to use matters!)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nIntroduced to tidycensus package: pulls Census/ACS data directly into R without manual downloads, keeping workflow reproducible\n\nLearned get_acs() function: specify geography (state/county/tract) and variables (e.g., median income) to get estimates + MOE in tidy format\n\nPracticed calculating MOE percentage to assess reliability: (MOE / estimate) * 100 — useful for filtering out unreliable estimates before analysis\n\nUsed case_when() for conditional categorization: cleaner than nested ifelse() for creating income brackets or other multi-level groups"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhen choosing between Census vs. ACS: for historical comparison (e.g., 2010 vs 2020), must use Census; but for recent data, ACS is only option. How to handle this trade-off in longitudinal studies?\n\nHigh MOE values in small areas (like block groups): should I aggregate up to tract level, or keep granular data with strong caveats? What’s the standard practice?\n\nCurious about differential privacy noise injection: does it systematically bias certain types of analyses (e.g., rural vs urban, small vs large populations)?"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nAlgorithmic bias examples (criminal justice risk scores, Dutch welfare fraud detection) show why transparency and equity auditing are non-negotiable in policy analytics\n\nCensus data isn’t just numbers—it determines billions in federal funding and shapes political representation. Getting the data right has real consequences\n\nUnderstanding MOE helps communicate uncertainty to policymakers, which builds trust and prevents over-confident decisions"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: Seeing real-world cases where algorithms went wrong (e.g., welfare fraud system flagged innocent people). Reinforced that technical skills alone aren’t enough—we need critical thinking about fairness and impact\n\nPlan to apply: Before diving into any spatial analysis, always check the MOE and think about whether the geographic level is appropriate for the question I’m asking"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html",
    "href": "assignments/assignment1/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [Your State] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#scenario",
    "href": "assignments/assignment1/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [Your State] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#learning-objectives",
    "href": "assignments/assignment1/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#submission-instructions",
    "href": "assignments/assignment1/assignment1_template.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#data-retrieval",
    "href": "assignments/assignment1/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [write something :)]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#submission-checklist",
    "href": "assignments/assignment1/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#introduction",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#introduction",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-chicago-boundaries",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-chicago-boundaries",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Chicago Boundaries",
    "text": "Load Chicago Boundaries\n\n\nCode\n# Load police districts for cross-validation\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nWhat we’re doing: Loading the spatial boundaries of Chicago and its police districts. We use police districts for spatial cross-validation later.\nWhy this matters: We need boundaries to constrain our analysis to Chicago and to create groups for validation."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-sanitation-violations-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-sanitation-violations-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Sanitation Violations Data",
    "text": "Load Sanitation Violations Data\n\n\nCode\n# Load the downloaded data\nviolations &lt;- read_csv(\"data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251114.csv\") %&gt;%\n  # Convert to sf object\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  # Parse date\n  mutate(\n    creation_date = mdy(`Creation Date`),\n    year = year(creation_date)\n  ) %&gt;%\n  # Keep only necessary columns\n  dplyr::select(\n    creation_date,\n    year,\n    status = Status,\n    violation_type = `What is the Nature of this Code Violation?`\n  )\n\ncat(\"✓ Loaded sanitation violations\\n\")\n\n\n✓ Loaded sanitation violations\n\n\nCode\ncat(\"  - Total violations:\", nrow(violations), \"\\n\")\n\n\n  - Total violations: 19733 \n\n\nCode\ncat(\"  - Date range:\", min(violations$creation_date), \"to\", \n    max(violations$creation_date), \"\\n\")\n\n\n  - Date range: 17167 to 17531 \n\n\nWhat we found: The dataset contains 19733 sanitation code violations from 2017. These represent citizen complaints about various sanitation issues across Chicago."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#visualize-spatial-distribution",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#visualize-spatial-distribution",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Visualize Spatial Distribution",
    "text": "Visualize Spatial Distribution\n\n\nCode\n# Point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = violations, color = \"#d62828\", size = 0.1, alpha = 0.3) +\n  labs(\n    title = \"Sanitation Code Violations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(violations))\n  ) +\n  theme_map()\n\n# Density surface\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(violations)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Higher concentrations in certain areas\"\n  ) +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nWhat patterns do we observe?\nThe sanitation code violations in Chicago are clearly not spread out at random. Instead, they form noticeable clusters—especially on the South Side and West Side. These areas show the strongest concentrations of violations, which stand out in the density map as dark-purple hot spots. In contrast, the North Side and much of the lakefront have far fewer recorded violations. This pattern lines up with broader neighborhood characteristics. Communities with more violations tend to have older housing, lower household incomes, and fewer resources available for property upkeep. The clustering suggests that these issues don’t happen in isolation but are connected to larger structural conditions, including the physical environment, local economic context, and even how enforcement may vary across neighborhoods. Because the violations are clearly clustered rather than randomly scattered, the data is well-suited for spatial prediction. The strong geographic patterns indicate that location and neighborhood context play an important role in explaining where violations occur."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#create-500m-x-500m-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#create-500m-x-500m-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Create 500m x 500m Grid",
    "text": "Create 500m x 500m Grid\n\n\nCode\n# Create fishnet grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells intersecting Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size: 500 x 500 meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nWhy use a fishnet grid?\nA regular grid allows us to: 1. Aggregate point data into consistent spatial units 2. Calculate spatial features at a uniform scale 3. Apply count regression models (which require aggregated counts)\nThis approach is more flexible than using administrative boundaries and ensures consistent spatial resolution across the study area."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#aggregate-violations-to-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#aggregate-violations-to-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Aggregate Violations to Grid",
    "text": "Aggregate Violations to Grid\n\n\nCode\n# Count violations per cell\nviolations_fishnet &lt;- st_join(violations, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countViolations = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(violations_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countViolations = replace_na(countViolations, 0))\n\n# Summary statistics\ncat(\"\\nViolation count distribution:\\n\")\n\n\n\nViolation count distribution:\n\n\nCode\nsummary(fishnet$countViolations)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    5.00    8.02   12.00  189.00 \n\n\nCode\ncat(\"\\nCells with zero violations:\", \n    sum(fishnet$countViolations == 0), \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countViolations == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero violations: 576 / 2458 ( 23.4 %)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Violations\",\n    option = \"plasma\",\n    trans = \"sqrt\"\n  ) +\n  labs(\n    title = \"Sanitation Code Violations by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat we observe:\nThe aggregated fishnet reveals that sanitation violations are widespread but unevenly distributed. Out of 2,458 cells, only 576 (23.4%) have zero violations, meaning over three-quarters of Chicago experienced at least one sanitation complaint in 2017.\nThe distribution shows substantial variation. While many cells have just 1-3 violations, some hotspot cells contain significantly higher counts. This right-skewed distribution is typical for urban complaint data and suggests two things: (1) sanitation problems are a city-wide issue rather than isolated incidents, and (2) certain neighborhoods experience disproportionately high violation rates, likely tied to structural factors like housing age, property maintenance capacity, and enforcement patterns.\nThe high variance across cells makes this data well-suited for count regression modeling, particularly Negative Binomial regression which can handle overdispersion."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-abandoned-vehicle-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-abandoned-vehicle-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Abandoned Vehicle Data",
    "text": "Load Abandoned Vehicle Data\n\n\nCode\n# Try to load from local file first (recommended)\nif (file.exists(\"data/abandoned_cars_2017.csv\")) {\n  abandoned_cars &lt;- read_csv(\"data/abandoned_cars_2017.csv\") %&gt;%\n    filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n    st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from local file\\n\")\n} else {\n  # Fallback: Try API (may be slow or fail)\n  abandoned_cars &lt;- read_csv(\"https://data.cityofchicago.org/resource/3c9v-pnva.csv?$limit=50000&$where=creation_date between '2017-01-01T00:00:00' and '2017-12-31T23:59:59'\") %&gt;%\n    filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from API\\n\")\n}\n\n\n✓ Loaded from local file\n\n\nCode\ncat(\"  - Number of abandoned vehicle calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of abandoned vehicle calls: 31390 \n\n\nWhy use abandoned vehicles as a predictor?\nFollowing the “broken windows theory,” physical signs of disorder (like abandoned vehicles) may predict other neighborhood problems. This variable tests whether disorder in one form (abandoned cars) correlates with disorder in another form (sanitation violations)."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#count-abandoned-vehicles-per-cell",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#count-abandoned-vehicles-per-cell",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Count Abandoned Vehicles per Cell",
    "text": "Count Abandoned Vehicles per Cell\n\n\nCode\n# Aggregate to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle Calls\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Sanitation Violations\") +\n  theme_map()\n\np1 + p2 +\n  plot_annotation(\n    title = \"Comparing Spatial Patterns\",\n    subtitle = \"Do these two phenomena co-occur?\"\n  )\n\n\n\n\n\n\n\n\n\nVisual relationship:\nThe side-by-side comparison reveals a strong visual correlation between abandoned vehicle calls and sanitation violations. Areas with high concentrations of abandoned cars—particularly on the South Side and West Side—also show elevated sanitation violation counts. This spatial overlap supports the “broken windows theory”: visible signs of physical disorder (abandoned vehicles) tend to co-occur with other forms of neighborhood neglect (sanitation problems).\nHowever, the relationship isn’t perfectly one-to-one. Some areas with moderate abandoned car counts still experience high sanitation violations, suggesting that other factors (housing density, property ownership patterns, or enforcement priorities) also play a role. This imperfect correlation justifies using abandoned cars as a predictor variable while recognizing it won’t explain all the variation in sanitation complaints.]*"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#calculate-nearest-neighbor-distances",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#calculate-nearest-neighbor-distances",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Calculate Nearest Neighbor Distances",
    "text": "Calculate Nearest Neighbor Distances\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(abandoned_cars.nn = rowMeans(nn_result$nn.dist))\n\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.283 2195.753 \n\n\nWhat this feature captures:\nThe average distance to the 3 nearest abandoned vehicle reports. A low value means a cell is surrounded by abandoned vehicles, suggesting neighborhood disorder. A high value means the cell is far from any abandoned vehicles."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#local-morans-i-identify-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#local-morans-i-identify-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Local Moran’s I: Identify Hot Spots",
    "text": "Local Moran’s I: Identify Hot Spots\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  local_moran &lt;- localmoran(data[[variable]], weights)\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = moran_class), color = NA) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High clusters = Hot spots of disorder\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat is Local Moran’s I?\nThis statistic identifies spatial clusters: - High-High (red): Hot spots - high values surrounded by high values - Low-Low (blue): Cold spots - low values surrounded by low values - High-Low / Low-High: Spatial outliers - Not Significant (gray): Random spatial pattern\nThis helps us understand where disorder is concentrated vs. where it’s spatially random."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#distance-to-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#distance-to-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Distance to Hot Spots",
    "text": "Distance to Hot Spots\n\n\nCode\n# Get hot spot centroids\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  cat(\"✓ Calculated distance to hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;% mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to hot spots\n  - Number of hot spot cells: 275 \n\n\nWhy distance to hot spots matters:\nBeing close to a cluster of abandoned vehicles may be a stronger predictor than distance to a single vehicle. Hot spots represent areas of concentrated disorder that may influence nearby areas."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#prepare-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#prepare-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Prepare Data",
    "text": "Prepare Data\n\n\nCode\n# Create clean dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countViolations,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#poisson-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#poisson-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n\nCode\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        2.866071797  0.025918274 110.581 &lt; 0.0000000000000002 ***\nabandoned_cars     0.001440424  0.000648357   2.222               0.0263 *  \nabandoned_cars.nn -0.004522870  0.000121096 -37.349 &lt; 0.0000000000000002 ***\ndist_to_hotspot   -0.000015715  0.000003982  -3.946            0.0000794 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 17204  on 1707  degrees of freedom\nResidual deviance: 12877  on 1704  degrees of freedom\nAIC: 18376\n\nNumber of Fisher Scoring iterations: 6\n\n\nInterpreting coefficients:\nAll three predictor variables are statistically significant, though with different levels of importance:\nabandoned_cars (β = 0.0014, p = 0.026*): The positive coefficient indicates that cells with more abandoned vehicle calls tend to have higher sanitation violation counts. Each additional abandoned car in a cell is associated with a small increase in expected violations. However, this effect is modest compared to the spatial features.\nabandoned_cars.nn (β = -0.0045, p &lt; 0.001***): Highly significant and negative. This means cells that are farther from abandoned vehicles (higher mean distance to 3 nearest neighbors) have fewer violations. In other words, being surrounded by abandoned cars strongly predicts more sanitation problems—the spatial context matters more than the count in the cell itself.\ndist_to_hotspot (β = -0.000016, p &lt; 0.001***): Also highly significant and negative. Cells closer to identified hot spots (lower distance) experience more violations. This captures the spillover effect: being near a cluster of disorder increases violation risk, even if the cell itself had moderate abandoned car counts.\nThe pattern is clear: spatial proximity to disorder (whether individual abandoned cars or clusters) is a stronger predictor than raw counts alone."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#check-for-overdispersion",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#check-for-overdispersion",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Check for Overdispersion",
    "text": "Check for Overdispersion\n\n\nCode\n# Calculate dispersion\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 12.39 \n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Negative Binomial is more appropriate.\\n\")\n} else {\n  cat(\"✓ Dispersion acceptable for Poisson.\\n\")\n}\n\n\n⚠ Overdispersion detected! Negative Binomial is more appropriate.\n\n\nWhat is overdispersion?\nPoisson regression assumes the mean equals the variance. Real-world count data often has variance greater than the mean (overdispersion). A dispersion parameter &gt; 1.5 suggests we should use Negative Binomial regression instead."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#negative-binomial-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#negative-binomial-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\n\n\nCode\n# Fit Negative Binomial\nmodel_nb &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.247782583, \n    link = log)\n\nCoefficients:\n                     Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        3.16042020  0.07565799  41.772 &lt;0.0000000000000002 ***\nabandoned_cars    -0.00200577  0.00210542  -0.953               0.341    \nabandoned_cars.nn -0.00636169  0.00029166 -21.812 &lt;0.0000000000000002 ***\ndist_to_hotspot   -0.00001228  0.00001087  -1.129               0.259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2478) family taken to be 1)\n\n    Null deviance: 2799.9  on 1707  degrees of freedom\nResidual deviance: 1847.9  on 1704  degrees of freedom\nAIC: 10326\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.2478 \n          Std. Err.:  0.0516 \n\n 2 x log-likelihood:  -10316.1820 \n\n\nCode\n# Compare models\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 18375.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 10326.2 \n\n\nWhich model is better?\nThe Negative Binomial model is clearly superior. With an AIC of 10,326 compared to Poisson’s 18,376, the NB model improves fit by over 8,000 points—a massive difference. This confirms what the dispersion test showed (φ = 12.39): the data is severely overdispersed, meaning the variance far exceeds the mean.\nThe Poisson model’s assumption that mean equals variance is badly violated here, leading to underestimated standard errors and unreliable inference. The Negative Binomial model adds a dispersion parameter to accommodate this extra variability, providing more realistic predictions and properly calibrated uncertainty estimates. For the remainder of our analysis, we’ll use the NB model exclusively."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#generate-final-predictions",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#generate-final-predictions",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Generate Final Predictions",
    "text": "Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Normalize KDE to same scale\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countViolations, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(prediction_kde = (kde_value / kde_sum) * count_sum)"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#compare-model-vs.-baseline",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#compare-model-vs.-baseline",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Compare Model vs. Baseline",
    "text": "Compare Model vs. Baseline\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Violations\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions\") +\n  theme_map()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline\") +\n  theme_map()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Model Performance Comparison\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countViolations - prediction_nb)),\n    model_rmse = sqrt(mean((countViolations - prediction_nb)^2)),\n    kde_mae = mean(abs(countViolations - prediction_kde)),\n    kde_rmse = sqrt(mean((countViolations - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model vs. KDE Baseline Performance\",\n    col.names = c(\"Approach\", \"MAE\", \"RMSE\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel vs. KDE Baseline Performance\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nmodel\n6.30\n10.57\n\n\nkde\n5.19\n9.12\n\n\n\n\n\nDoes the model outperform the baseline?\nSurprisingly, the KDE baseline outperforms our Negative Binomial model on both metrics. The KDE achieves an MAE of 5.19 and RMSE of 9.12, compared to the model’s MAE of 6.30 and RMSE of 10.57. This means the simple spatial smoothing approach makes predictions that are, on average, about 1 violation closer to the actual counts.\nThis result is humbling but instructive. It suggests that for sanitation violations in 2017, spatial autocorrelation (past locations predict future locations) is more powerful than our chosen predictors (abandoned cars and their spatial distribution). The KDE effectively captures the “violations happen where they happened before” pattern without needing additional variables.\nHowever, this doesn’t mean our model is useless. The regression approach offers interpretability—we can explain why violations occur (proximity to disorder) rather than just where. Additionally, the model could potentially generalize better to new contexts or time periods where the spatial pattern shifts, whereas KDE can only replicate historical patterns. For operational deployment, the simpler KDE might be preferred, but for policy insights, the model remains valuable."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#error-analysis",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#error-analysis",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Error Analysis",
    "text": "Error Analysis\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countViolations - prediction_nb,\n    abs_error_nb = abs(error_nb)\n  )\n\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = underpredicted, Blue = overpredicted\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs Error\", option = \"magma\") +\n  labs(title = \"Absolute Errors\",\n       subtitle = \"Where are predictions least accurate?\") +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nSpatial patterns in errors:\nThe error maps reveal systematic spatial patterns rather than random noise. The model tends to underpredict (red areas) in certain South and West Side neighborhoods where actual violations are higher than expected. Conversely, it overpredicts (blue areas) in some areas with moderate abandoned car counts but lower-than-expected violations.\nThe absolute error map shows the biggest mistakes cluster in specific zones, suggesting we’re missing important predictors. Possible explanations:\nWhat the model is missing: - Housing tenure: Owner-occupied vs. renter-occupied properties may have different violation rates regardless of abandoned car prevalence - Property age and condition: Older housing stock may generate more complaints independent of visible disorder - Population density: Dense areas might have more eyes on the street reporting issues - Institutional presence: Areas near schools, parks, or commercial districts may have different patterns - Enforcement capacity: Some districts may have more aggressive inspection protocols\nThe spatial clustering of errors suggests these omitted variables themselves have geographic patterns. A more complete model would incorporate demographic, land use, and institutional data beyond our disorder proxy."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#key-findings",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#key-findings",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Key Findings",
    "text": "Key Findings\nModel Performance: - Cross-validation MAE: 7.02 - Model outperformed KDE baseline: NO - KDE achieved lower error (MAE 5.19 vs. 6.30) - Most predictive variable: abandoned_cars.nn (distance to nearest neighbors) - highly significant with strongest coefficient Spatial Patterns: - Violations are highly clustered, concentrated on South and West Sides - Hot spots located in neighborhoods with older housing stock and higher disorder indicators - Prediction errors show systematic patterns - model struggles in districts with unique characteristics (19, 18, 14) Model Limitations:\nSeveral important limitations constrain our conclusions:\nMissing variables: We rely solely on abandoned vehicle calls as a disorder proxy. Critical omitted variables include property ownership patterns, housing age, population density, land use mix, and institutional presence (schools, parks, commercial areas). These factors likely explain why some districts were harder to predict.\nTemporal assumptions: Our 2017 cross-sectional analysis assumes spatial patterns are stable. Neighborhood change, policy shifts, or enforcement priorities could alter relationships over time.\nMeasurement issues: 311 calls reflect both actual conditions and reporting behavior. Affluent neighborhoods may report more aggressively, while underserved areas may have normalized disorder. We’re modeling reported violations, not necessarily actual sanitation problems.\nSpatial autocorrelation: The fact that simple KDE outperformed our model suggests violations are primarily driven by spatial inertia (“it happens where it happened before”) rather than our chosen predictors. This limits the model’s explanatory power.\nGeneralizability: The model is trained on Chicago’s specific context. Relationships between abandoned cars and sanitation violations may not transfer to other cities with different housing markets, demographics, or enforcement regimes.\nImprovement paths: Future work should incorporate census demographics, land use data, property characteristics, and temporal validation to test whether patterns persist across years."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#practical-implications",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#practical-implications",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Practical Implications",
    "text": "Practical Implications\nOperational recommendations:\nResource allocation: Given that KDE outperformed the regression model, the city could deploy a hybrid approach - use simple KDE for day-to-day inspection prioritization (where violations happened recently), but use the regression model to understand why certain areas are prone to violations (proximity to disorder clusters). This combines operational efficiency with strategic insight.\nInspection priorities: The model identifies high-risk cells through the distance-to-hotspot variable. Inspectors could focus on areas within 1-2km of identified disorder clusters, even if those specific cells haven’t shown many violations yet. This proactive approach targets spillover zones.\nTargeted interventions: The strong relationship between abandoned cars and sanitation violations suggests addressing vehicle abandonment could have co-benefits. Programs to expedite vehicle removal, especially in and around hot spots, might reduce multiple forms of neighborhood disorder simultaneously.\nCritical limitations to remember:\n\nReporting bias: The model predicts reported violations. Under-reporting in some communities means model predictions might misallocate resources away from areas with real but unreported problems.\nFeedback loops: Deploying prediction-based enforcement creates self-fulfilling prophecies - more inspections generate more recorded violations, reinforcing the prediction. The city must guard against over-policing already-disadvantaged areas.\nEquity considerations: Districts 19, 18, and 14 had the highest prediction errors, suggesting the model works less well in these areas. Resource allocation based on model predictions could systematically disadvantage neighborhoods whose conditions don’t match city-wide patterns. Any deployment must include equity audits.\n\nEthical principles: Predictive models should inform, not determine, resource allocation. Human judgment, community input, and equity metrics must remain central to decision-making. The goal is to improve public health outcomes equitably, not to optimize enforcement efficiency at the cost of fairness."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#appendix-session-info",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#appendix-session-info",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Appendix: Session Info",
    "text": "Appendix: Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=Chinese (Simplified)_China.utf8 \n[2] LC_CTYPE=Chinese (Simplified)_China.utf8   \n[3] LC_MONETARY=Chinese (Simplified)_China.utf8\n[4] LC_NUMERIC=C                               \n[5] LC_TIME=Chinese (Simplified)_China.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] spatstat.explore_3.5-3 nlme_3.1-168           spatstat.random_3.4-2 \n [4] spatstat.geom_3.6-0    spatstat.univar_3.1-4  spatstat.data_3.1-9   \n [7] classInt_0.4-11        kableExtra_1.4.0       knitr_1.50            \n[10] patchwork_1.3.2        MASS_7.3-65            FNN_1.1.4.1           \n[13] spdep_1.4-1            spData_2.3.4           terra_1.8-70          \n[16] viridis_0.6.5          viridisLite_0.4.2      here_1.0.2            \n[19] sf_1.0-21              lubridate_1.9.4        forcats_1.0.0         \n[22] stringr_1.5.2          dplyr_1.1.4            purrr_1.1.0           \n[25] readr_2.1.5            tidyr_1.3.1            tibble_3.3.0          \n[28] ggplot2_4.0.0          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1      farver_2.1.2          S7_0.2.0             \n [4] fastmap_1.2.0         digest_0.6.37         timechange_0.3.0     \n [7] lifecycle_1.0.4       magrittr_2.0.4        compiler_4.5.1       \n[10] rlang_1.1.6           tools_4.5.1           yaml_2.3.10          \n[13] labeling_0.4.3        htmlwidgets_1.6.4     bit_4.6.0            \n[16] sp_2.2-0              xml2_1.4.0            RColorBrewer_1.1-3   \n[19] abind_1.4-8           KernSmooth_2.23-26    withr_3.0.2          \n[22] grid_4.5.1            polyclip_1.10-7       e1071_1.7-16         \n[25] scales_1.4.0          spatstat.utils_3.2-0  isoband_0.2.7        \n[28] cli_3.6.5             crayon_1.5.3          rmarkdown_2.29       \n[31] generics_0.1.4        rstudioapi_0.17.1     tzdb_0.5.0           \n[34] DBI_1.2.3             proxy_0.4-27          parallel_4.5.1       \n[37] s2_1.1.9              vctrs_0.6.5           boot_1.3-32          \n[40] Matrix_1.7-4          jsonlite_2.0.0        hms_1.1.3            \n[43] bit64_4.6.0-1         tensor_1.5.1          systemfonts_1.2.3    \n[46] units_0.8-7           goftest_1.2-3         glue_1.8.0           \n[49] codetools_0.2-20      stringi_1.8.7         gtable_0.3.6         \n[52] deldir_2.0-4          pillar_1.11.1         htmltools_0.5.8.1    \n[55] R6_2.6.1              wk_0.9.4              textshaping_1.0.3    \n[58] rprojroot_2.1.1       vroom_1.6.5           evaluate_1.0.5       \n[61] lattice_0.22-7        backports_1.5.0       broom_1.0.10         \n[64] class_7.3-23          Rcpp_1.1.0            spatstat.sparse_3.1-0\n[67] svglite_2.2.1         gridExtra_2.3         xfun_0.53            \n[70] pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/assignment4/test_codefold.html",
    "href": "assignments/assignment4/test_codefold.html",
    "title": "Code Fold Test",
    "section": "",
    "text": "Code\nprint(\"This should be folded by default\")\n\n\n[1] \"This should be folded by default\"\n\n\nCode\nx &lt;- 1:10\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00"
  },
  {
    "objectID": "assignments/assignment4/test_codefold.html#test-1",
    "href": "assignments/assignment4/test_codefold.html#test-1",
    "title": "Code Fold Test",
    "section": "",
    "text": "Code\nprint(\"This should be folded by default\")\n\n\n[1] \"This should be folded by default\"\n\n\nCode\nx &lt;- 1:10\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00"
  },
  {
    "objectID": "assignments/assignment4/test_codefold.html#test-2",
    "href": "assignments/assignment4/test_codefold.html#test-2",
    "title": "Code Fold Test",
    "section": "Test 2",
    "text": "Test 2\n\n\nCode\nlibrary(ggplot2)\nggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nIf “Hide All Code” and “Show All Code” buttons work here, then the issue is specific to your Assignment 4 file."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html",
    "href": "assignments/assignment1/Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#scenario",
    "href": "assignments/assignment1/Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#learning-objectives",
    "href": "assignments/assignment1/Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#data-retrieval",
    "href": "assignments/assignment1/Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html",
    "href": "docs/assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#scenario",
    "href": "docs/assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#learning-objectives",
    "href": "docs/assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#data-retrieval",
    "href": "docs/assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "docs/assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "docs/assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "docs/assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "docs/assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "docs/assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "docs/assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "docs/assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "docs/assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "docs/assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "docs/assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  }
]