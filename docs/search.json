[
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Introduction to Public Policy Analytics (MUSA 5080) and course goals\n\nImportance of combining spatial analysis with data science for urban planning and policy\n\nOverview of course deliverables: weekly notes, labs, and final project\n\nInitial setup of the RStudio + Quarto environment for reproducible workflows"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Introduction to Public Policy Analytics (MUSA 5080) and course goals\n\nImportance of combining spatial analysis with data science for urban planning and policy\n\nOverview of course deliverables: weekly notes, labs, and final project\n\nInitial setup of the RStudio + Quarto environment for reproducible workflows"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nLearned how to set the working directory in R (setwd())\n\nPracticed creating and rendering a Quarto (.qmd) document\n\nExplored basic Quarto structure: YAML header, markdown formatting, and render function"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nStill clarifying the workflow between RStudio and Quarto rendering (e.g., where files are stored vs. rendered outputs)\n\nNeed more practice with GitHub integration for publishing the portfolio"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nBuilding reproducible workflows with R and Quarto ensures transparency and credibility in policy-related analysis\n\nPortfolio structure mirrors how real-world policy analysts document methods, results, and reflections"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: Seeing how quickly RStudio + Quarto can generate a professional-looking portfolio website\n\nPlan to apply: Use weekly notes to practice explaining technical concepts in plain language, which is critical for communicating with policymakers"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#introduction",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#introduction",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-chicago-boundaries",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-chicago-boundaries",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Chicago Boundaries",
    "text": "Load Chicago Boundaries\n\n\nCode\n# Load police districts for cross-validation\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nWhat we’re doing: Loading the spatial boundaries of Chicago and its police districts. We use police districts for spatial cross-validation later.\nWhy this matters: We need boundaries to constrain our analysis to Chicago and to create groups for validation."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-sanitation-violations-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-sanitation-violations-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Sanitation Violations Data",
    "text": "Load Sanitation Violations Data\n\n\nCode\n# Load the downloaded data\nviolations &lt;- read_csv(\"data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251114.csv\") %&gt;%\n  # Convert to sf object\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  # Parse date\n  mutate(\n    creation_date = mdy(`Creation Date`),\n    year = year(creation_date)\n  ) %&gt;%\n  # Keep only necessary columns\n  dplyr::select(\n    creation_date,\n    year,\n    status = Status,\n    violation_type = `What is the Nature of this Code Violation?`\n  )\n\ncat(\"✓ Loaded sanitation violations\\n\")\n\n\n✓ Loaded sanitation violations\n\n\nCode\ncat(\"  - Total violations:\", nrow(violations), \"\\n\")\n\n\n  - Total violations: 19733 \n\n\nCode\ncat(\"  - Date range:\", min(violations$creation_date), \"to\", \n    max(violations$creation_date), \"\\n\")\n\n\n  - Date range: 17167 to 17531 \n\n\nWhat we found: The dataset contains 19733 sanitation code violations from 2017. These represent citizen complaints about various sanitation issues across Chicago."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#visualize-spatial-distribution",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#visualize-spatial-distribution",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Visualize Spatial Distribution",
    "text": "Visualize Spatial Distribution\n\n\nCode\n# Point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = violations, color = \"#d62828\", size = 0.1, alpha = 0.3) +\n  labs(\n    title = \"Sanitation Code Violations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(violations))\n  ) +\n  theme_map()\n\n# Density surface\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(violations)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Higher concentrations in certain areas\"\n  ) +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nWhat patterns do we observe?\nThe sanitation code violations in Chicago are clearly not spread out at random. Instead, they form noticeable clusters—especially on the South Side and West Side. These areas show the strongest concentrations of violations, which stand out in the density map as dark-purple hot spots. In contrast, the North Side and much of the lakefront have far fewer recorded violations. This pattern lines up with broader neighborhood characteristics. Communities with more violations tend to have older housing, lower household incomes, and fewer resources available for property upkeep. The clustering suggests that these issues don’t happen in isolation but are connected to larger structural conditions, including the physical environment, local economic context, and even how enforcement may vary across neighborhoods. Because the violations are clearly clustered rather than randomly scattered, the data is well-suited for spatial prediction. The strong geographic patterns indicate that location and neighborhood context play an important role in explaining where violations occur."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#create-500m-x-500m-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#create-500m-x-500m-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Create 500m x 500m Grid",
    "text": "Create 500m x 500m Grid\n\n\nCode\n# Create fishnet grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells intersecting Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size: 500 x 500 meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nWhy use a fishnet grid?\nA regular grid allows us to: 1. Aggregate point data into consistent spatial units 2. Calculate spatial features at a uniform scale 3. Apply count regression models (which require aggregated counts)\nThis approach is more flexible than using administrative boundaries and ensures consistent spatial resolution across the study area."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#aggregate-violations-to-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#aggregate-violations-to-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Aggregate Violations to Grid",
    "text": "Aggregate Violations to Grid\n\n\nCode\n# Count violations per cell\nviolations_fishnet &lt;- st_join(violations, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countViolations = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(violations_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countViolations = replace_na(countViolations, 0))\n\n# Summary statistics\ncat(\"\\nViolation count distribution:\\n\")\n\n\n\nViolation count distribution:\n\n\nCode\nsummary(fishnet$countViolations)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    5.00    8.02   12.00  189.00 \n\n\nCode\ncat(\"\\nCells with zero violations:\", \n    sum(fishnet$countViolations == 0), \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countViolations == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero violations: 576 / 2458 ( 23.4 %)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Violations\",\n    option = \"plasma\",\n    trans = \"sqrt\"\n  ) +\n  labs(\n    title = \"Sanitation Code Violations by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat we observe:\nThe aggregated fishnet reveals that sanitation violations are widespread but unevenly distributed. Out of 2,458 cells, only 576 (23.4%) have zero violations, meaning over three-quarters of Chicago experienced at least one sanitation complaint in 2017.\nThe distribution shows substantial variation. While many cells have just 1-3 violations, some hotspot cells contain significantly higher counts. This right-skewed distribution is typical for urban complaint data and suggests two things: (1) sanitation problems are a city-wide issue rather than isolated incidents, and (2) certain neighborhoods experience disproportionately high violation rates, likely tied to structural factors like housing age, property maintenance capacity, and enforcement patterns.\nThe high variance across cells makes this data well-suited for count regression modeling, particularly Negative Binomial regression which can handle overdispersion."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-abandoned-vehicle-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#load-abandoned-vehicle-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Abandoned Vehicle Data",
    "text": "Load Abandoned Vehicle Data\n\n\nCode\n# Try to load from local file first (recommended)\nif (file.exists(\"data/abandoned_cars_2017.csv\")) {\n  abandoned_cars &lt;- read_csv(\"data/abandoned_cars_2017.csv\") %&gt;%\n    filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n    st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from local file\\n\")\n} else {\n  # Fallback: Try API (may be slow or fail)\n  abandoned_cars &lt;- read_csv(\"https://data.cityofchicago.org/resource/3c9v-pnva.csv?$limit=50000&$where=creation_date between '2017-01-01T00:00:00' and '2017-12-31T23:59:59'\") %&gt;%\n    filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from API\\n\")\n}\n\n\n✓ Loaded from local file\n\n\nCode\ncat(\"  - Number of abandoned vehicle calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of abandoned vehicle calls: 31390 \n\n\nWhy use abandoned vehicles as a predictor?\nFollowing the “broken windows theory,” physical signs of disorder (like abandoned vehicles) may predict other neighborhood problems. This variable tests whether disorder in one form (abandoned cars) correlates with disorder in another form (sanitation violations)."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#count-abandoned-vehicles-per-cell",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#count-abandoned-vehicles-per-cell",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Count Abandoned Vehicles per Cell",
    "text": "Count Abandoned Vehicles per Cell\n\n\nCode\n# Aggregate to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle Calls\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Sanitation Violations\") +\n  theme_map()\n\np1 + p2 +\n  plot_annotation(\n    title = \"Comparing Spatial Patterns\",\n    subtitle = \"Do these two phenomena co-occur?\"\n  )\n\n\n\n\n\n\n\n\n\nVisual relationship:\nThe side-by-side comparison reveals a strong visual correlation between abandoned vehicle calls and sanitation violations. Areas with high concentrations of abandoned cars—particularly on the South Side and West Side—also show elevated sanitation violation counts. This spatial overlap supports the “broken windows theory”: visible signs of physical disorder (abandoned vehicles) tend to co-occur with other forms of neighborhood neglect (sanitation problems).\nHowever, the relationship isn’t perfectly one-to-one. Some areas with moderate abandoned car counts still experience high sanitation violations, suggesting that other factors (housing density, property ownership patterns, or enforcement priorities) also play a role. This imperfect correlation justifies using abandoned cars as a predictor variable while recognizing it won’t explain all the variation in sanitation complaints.]*"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#calculate-nearest-neighbor-distances",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#calculate-nearest-neighbor-distances",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Calculate Nearest Neighbor Distances",
    "text": "Calculate Nearest Neighbor Distances\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(abandoned_cars.nn = rowMeans(nn_result$nn.dist))\n\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.283 2195.753 \n\n\nWhat this feature captures:\nThe average distance to the 3 nearest abandoned vehicle reports. A low value means a cell is surrounded by abandoned vehicles, suggesting neighborhood disorder. A high value means the cell is far from any abandoned vehicles."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#local-morans-i-identify-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#local-morans-i-identify-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Local Moran’s I: Identify Hot Spots",
    "text": "Local Moran’s I: Identify Hot Spots\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  local_moran &lt;- localmoran(data[[variable]], weights)\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = moran_class), color = NA) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High clusters = Hot spots of disorder\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat is Local Moran’s I?\nThis statistic identifies spatial clusters: - High-High (red): Hot spots - high values surrounded by high values - Low-Low (blue): Cold spots - low values surrounded by low values - High-Low / Low-High: Spatial outliers - Not Significant (gray): Random spatial pattern\nThis helps us understand where disorder is concentrated vs. where it’s spatially random."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#distance-to-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#distance-to-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Distance to Hot Spots",
    "text": "Distance to Hot Spots\n\n\nCode\n# Get hot spot centroids\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  cat(\"✓ Calculated distance to hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;% mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to hot spots\n  - Number of hot spot cells: 275 \n\n\nWhy distance to hot spots matters:\nBeing close to a cluster of abandoned vehicles may be a stronger predictor than distance to a single vehicle. Hot spots represent areas of concentrated disorder that may influence nearby areas."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#prepare-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#prepare-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Prepare Data",
    "text": "Prepare Data\n\n\nCode\n# Create clean dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countViolations,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#poisson-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#poisson-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n\nCode\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        2.866071797  0.025918274 110.581 &lt; 0.0000000000000002 ***\nabandoned_cars     0.001440424  0.000648357   2.222               0.0263 *  \nabandoned_cars.nn -0.004522870  0.000121096 -37.349 &lt; 0.0000000000000002 ***\ndist_to_hotspot   -0.000015715  0.000003982  -3.946            0.0000794 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 17204  on 1707  degrees of freedom\nResidual deviance: 12877  on 1704  degrees of freedom\nAIC: 18376\n\nNumber of Fisher Scoring iterations: 6\n\n\nInterpreting coefficients:\nAll three predictor variables are statistically significant, though with different levels of importance:\nabandoned_cars (β = 0.0014, p = 0.026*): The positive coefficient indicates that cells with more abandoned vehicle calls tend to have higher sanitation violation counts. Each additional abandoned car in a cell is associated with a small increase in expected violations. However, this effect is modest compared to the spatial features.\nabandoned_cars.nn (β = -0.0045, p &lt; 0.001***): Highly significant and negative. This means cells that are farther from abandoned vehicles (higher mean distance to 3 nearest neighbors) have fewer violations. In other words, being surrounded by abandoned cars strongly predicts more sanitation problems—the spatial context matters more than the count in the cell itself.\ndist_to_hotspot (β = -0.000016, p &lt; 0.001***): Also highly significant and negative. Cells closer to identified hot spots (lower distance) experience more violations. This captures the spillover effect: being near a cluster of disorder increases violation risk, even if the cell itself had moderate abandoned car counts.\nThe pattern is clear: spatial proximity to disorder (whether individual abandoned cars or clusters) is a stronger predictor than raw counts alone."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#check-for-overdispersion",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#check-for-overdispersion",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Check for Overdispersion",
    "text": "Check for Overdispersion\n\n\nCode\n# Calculate dispersion\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 12.39 \n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Negative Binomial is more appropriate.\\n\")\n} else {\n  cat(\"✓ Dispersion acceptable for Poisson.\\n\")\n}\n\n\n⚠ Overdispersion detected! Negative Binomial is more appropriate.\n\n\nWhat is overdispersion?\nPoisson regression assumes the mean equals the variance. Real-world count data often has variance greater than the mean (overdispersion). A dispersion parameter &gt; 1.5 suggests we should use Negative Binomial regression instead."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#negative-binomial-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#negative-binomial-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\n\n\nCode\n# Fit Negative Binomial\nmodel_nb &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.247782583, \n    link = log)\n\nCoefficients:\n                     Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        3.16042020  0.07565799  41.772 &lt;0.0000000000000002 ***\nabandoned_cars    -0.00200577  0.00210542  -0.953               0.341    \nabandoned_cars.nn -0.00636169  0.00029166 -21.812 &lt;0.0000000000000002 ***\ndist_to_hotspot   -0.00001228  0.00001087  -1.129               0.259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2478) family taken to be 1)\n\n    Null deviance: 2799.9  on 1707  degrees of freedom\nResidual deviance: 1847.9  on 1704  degrees of freedom\nAIC: 10326\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.2478 \n          Std. Err.:  0.0516 \n\n 2 x log-likelihood:  -10316.1820 \n\n\nCode\n# Compare models\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 18375.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 10326.2 \n\n\nWhich model is better?\nThe Negative Binomial model is clearly superior. With an AIC of 10,326 compared to Poisson’s 18,376, the NB model improves fit by over 8,000 points—a massive difference. This confirms what the dispersion test showed (φ = 12.39): the data is severely overdispersed, meaning the variance far exceeds the mean.\nThe Poisson model’s assumption that mean equals variance is badly violated here, leading to underestimated standard errors and unreliable inference. The Negative Binomial model adds a dispersion parameter to accommodate this extra variability, providing more realistic predictions and properly calibrated uncertainty estimates. For the remainder of our analysis, we’ll use the NB model exclusively."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#generate-final-predictions",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#generate-final-predictions",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Generate Final Predictions",
    "text": "Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Normalize KDE to same scale\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countViolations, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(prediction_kde = (kde_value / kde_sum) * count_sum)"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#compare-model-vs.-baseline",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#compare-model-vs.-baseline",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Compare Model vs. Baseline",
    "text": "Compare Model vs. Baseline\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Violations\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions\") +\n  theme_map()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline\") +\n  theme_map()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Model Performance Comparison\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countViolations - prediction_nb)),\n    model_rmse = sqrt(mean((countViolations - prediction_nb)^2)),\n    kde_mae = mean(abs(countViolations - prediction_kde)),\n    kde_rmse = sqrt(mean((countViolations - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model vs. KDE Baseline Performance\",\n    col.names = c(\"Approach\", \"MAE\", \"RMSE\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel vs. KDE Baseline Performance\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nmodel\n6.30\n10.57\n\n\nkde\n5.19\n9.12\n\n\n\n\n\nDoes the model outperform the baseline?\nSurprisingly, the KDE baseline outperforms our Negative Binomial model on both metrics. The KDE achieves an MAE of 5.19 and RMSE of 9.12, compared to the model’s MAE of 6.30 and RMSE of 10.57. This means the simple spatial smoothing approach makes predictions that are, on average, about 1 violation closer to the actual counts.\nThis result is humbling but instructive. It suggests that for sanitation violations in 2017, spatial autocorrelation (past locations predict future locations) is more powerful than our chosen predictors (abandoned cars and their spatial distribution). The KDE effectively captures the “violations happen where they happened before” pattern without needing additional variables.\nHowever, this doesn’t mean our model is useless. The regression approach offers interpretability—we can explain why violations occur (proximity to disorder) rather than just where. Additionally, the model could potentially generalize better to new contexts or time periods where the spatial pattern shifts, whereas KDE can only replicate historical patterns. For operational deployment, the simpler KDE might be preferred, but for policy insights, the model remains valuable."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#error-analysis",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#error-analysis",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Error Analysis",
    "text": "Error Analysis\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countViolations - prediction_nb,\n    abs_error_nb = abs(error_nb)\n  )\n\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = underpredicted, Blue = overpredicted\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs Error\", option = \"magma\") +\n  labs(title = \"Absolute Errors\",\n       subtitle = \"Where are predictions least accurate?\") +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nSpatial patterns in errors:\nThe error maps reveal systematic spatial patterns rather than random noise. The model tends to underpredict (red areas) in certain South and West Side neighborhoods where actual violations are higher than expected. Conversely, it overpredicts (blue areas) in some areas with moderate abandoned car counts but lower-than-expected violations.\nThe absolute error map shows the biggest mistakes cluster in specific zones, suggesting we’re missing important predictors. Possible explanations:\nWhat the model is missing: - Housing tenure: Owner-occupied vs. renter-occupied properties may have different violation rates regardless of abandoned car prevalence - Property age and condition: Older housing stock may generate more complaints independent of visible disorder - Population density: Dense areas might have more eyes on the street reporting issues - Institutional presence: Areas near schools, parks, or commercial districts may have different patterns - Enforcement capacity: Some districts may have more aggressive inspection protocols\nThe spatial clustering of errors suggests these omitted variables themselves have geographic patterns. A more complete model would incorporate demographic, land use, and institutional data beyond our disorder proxy."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#key-findings",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#key-findings",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Key Findings",
    "text": "Key Findings\nModel Performance: - Cross-validation MAE: 7.02 - Model outperformed KDE baseline: NO - KDE achieved lower error (MAE 5.19 vs. 6.30) - Most predictive variable: abandoned_cars.nn (distance to nearest neighbors) - highly significant with strongest coefficient Spatial Patterns: - Violations are highly clustered, concentrated on South and West Sides - Hot spots located in neighborhoods with older housing stock and higher disorder indicators - Prediction errors show systematic patterns - model struggles in districts with unique characteristics (19, 18, 14) Model Limitations:\nSeveral important limitations constrain our conclusions:\nMissing variables: We rely solely on abandoned vehicle calls as a disorder proxy. Critical omitted variables include property ownership patterns, housing age, population density, land use mix, and institutional presence (schools, parks, commercial areas). These factors likely explain why some districts were harder to predict.\nTemporal assumptions: Our 2017 cross-sectional analysis assumes spatial patterns are stable. Neighborhood change, policy shifts, or enforcement priorities could alter relationships over time.\nMeasurement issues: 311 calls reflect both actual conditions and reporting behavior. Affluent neighborhoods may report more aggressively, while underserved areas may have normalized disorder. We’re modeling reported violations, not necessarily actual sanitation problems.\nSpatial autocorrelation: The fact that simple KDE outperformed our model suggests violations are primarily driven by spatial inertia (“it happens where it happened before”) rather than our chosen predictors. This limits the model’s explanatory power.\nGeneralizability: The model is trained on Chicago’s specific context. Relationships between abandoned cars and sanitation violations may not transfer to other cities with different housing markets, demographics, or enforcement regimes.\nImprovement paths: Future work should incorporate census demographics, land use data, property characteristics, and temporal validation to test whether patterns persist across years."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#practical-implications",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#practical-implications",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Practical Implications",
    "text": "Practical Implications\nOperational recommendations:\nResource allocation: Given that KDE outperformed the regression model, the city could deploy a hybrid approach - use simple KDE for day-to-day inspection prioritization (where violations happened recently), but use the regression model to understand why certain areas are prone to violations (proximity to disorder clusters). This combines operational efficiency with strategic insight.\nInspection priorities: The model identifies high-risk cells through the distance-to-hotspot variable. Inspectors could focus on areas within 1-2km of identified disorder clusters, even if those specific cells haven’t shown many violations yet. This proactive approach targets spillover zones.\nTargeted interventions: The strong relationship between abandoned cars and sanitation violations suggests addressing vehicle abandonment could have co-benefits. Programs to expedite vehicle removal, especially in and around hot spots, might reduce multiple forms of neighborhood disorder simultaneously.\nCritical limitations to remember:\n\nReporting bias: The model predicts reported violations. Under-reporting in some communities means model predictions might misallocate resources away from areas with real but unreported problems.\nFeedback loops: Deploying prediction-based enforcement creates self-fulfilling prophecies - more inspections generate more recorded violations, reinforcing the prediction. The city must guard against over-policing already-disadvantaged areas.\nEquity considerations: Districts 19, 18, and 14 had the highest prediction errors, suggesting the model works less well in these areas. Resource allocation based on model predictions could systematically disadvantage neighborhoods whose conditions don’t match city-wide patterns. Any deployment must include equity audits.\n\nEthical principles: Predictive models should inform, not determine, resource allocation. Human judgment, community input, and equity metrics must remain central to decision-making. The goal is to improve public health outcomes equitably, not to optimize enforcement efficiency at the cost of fairness."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4.html#appendix-session-info",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4.html#appendix-session-info",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Appendix: Session Info",
    "text": "Appendix: Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=Chinese (Simplified)_China.utf8 \n[2] LC_CTYPE=Chinese (Simplified)_China.utf8   \n[3] LC_MONETARY=Chinese (Simplified)_China.utf8\n[4] LC_NUMERIC=C                               \n[5] LC_TIME=Chinese (Simplified)_China.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] spatstat.explore_3.5-3 nlme_3.1-168           spatstat.random_3.4-2 \n [4] spatstat.geom_3.6-0    spatstat.univar_3.1-4  spatstat.data_3.1-9   \n [7] classInt_0.4-11        kableExtra_1.4.0       knitr_1.50            \n[10] patchwork_1.3.2        MASS_7.3-65            FNN_1.1.4.1           \n[13] spdep_1.4-1            spData_2.3.4           terra_1.8-70          \n[16] viridis_0.6.5          viridisLite_0.4.2      here_1.0.2            \n[19] sf_1.0-21              lubridate_1.9.4        forcats_1.0.0         \n[22] stringr_1.5.2          dplyr_1.1.4            purrr_1.1.0           \n[25] readr_2.1.5            tidyr_1.3.1            tibble_3.3.0          \n[28] ggplot2_4.0.0          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1      farver_2.1.2          S7_0.2.0             \n [4] fastmap_1.2.0         digest_0.6.37         timechange_0.3.0     \n [7] lifecycle_1.0.4       magrittr_2.0.4        compiler_4.5.1       \n[10] rlang_1.1.6           tools_4.5.1           yaml_2.3.10          \n[13] labeling_0.4.3        htmlwidgets_1.6.4     bit_4.6.0            \n[16] sp_2.2-0              xml2_1.4.0            RColorBrewer_1.1-3   \n[19] abind_1.4-8           KernSmooth_2.23-26    withr_3.0.2          \n[22] grid_4.5.1            polyclip_1.10-7       e1071_1.7-16         \n[25] scales_1.4.0          spatstat.utils_3.2-0  isoband_0.2.7        \n[28] cli_3.6.5             crayon_1.5.3          rmarkdown_2.29       \n[31] generics_0.1.4        rstudioapi_0.17.1     tzdb_0.5.0           \n[34] DBI_1.2.3             proxy_0.4-27          parallel_4.5.1       \n[37] s2_1.1.9              vctrs_0.6.5           boot_1.3-32          \n[40] Matrix_1.7-4          jsonlite_2.0.0        hms_1.1.3            \n[43] bit64_4.6.0-1         tensor_1.5.1          systemfonts_1.2.3    \n[46] units_0.8-7           goftest_1.2-3         glue_1.8.0           \n[49] codetools_0.2-20      stringi_1.8.7         gtable_0.3.6         \n[52] deldir_2.0-4          pillar_1.11.1         htmltools_0.5.8.1    \n[55] R6_2.6.1              wk_0.9.4              textshaping_1.0.3    \n[58] rprojroot_2.1.1       vroom_1.6.5           evaluate_1.0.5       \n[61] lattice_0.22-7        backports_1.5.0       broom_1.0.10         \n[64] class_7.3-23          Rcpp_1.1.0            spatstat.sparse_3.1-0\n[67] svglite_2.2.1         gridExtra_2.3         xfun_0.53            \n[70] pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/assignment1/index.html",
    "href": "assignments/assignment1/index.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/index.html#assignment-overview",
    "href": "assignments/assignment1/index.html#assignment-overview",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-1-portfolio-integration",
    "href": "assignments/assignment1/index.html#part-1-portfolio-integration",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 1: Portfolio Integration",
    "text": "Part 1: Portfolio Integration\nThis assignment has been integrated into my portfolio website at Assignment 1 Page.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"), install = FALSE)\n\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-2-county-level-resource-assessment",
    "href": "assignments/assignment1/index.html#part-2-county-level-resource-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 2: County-Level Resource Assessment",
    "text": "Part 2: County-Level Resource Assessment\n\nvars &lt;- c(\n  total_pop = \"B01003_001\",      \n  median_income = \"B19013_001\",  \n  median_age = \"B01002_001\"      \n)\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = vars,\n  year = 2022\n)\n\ncounty_wide &lt;- county_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(county_wide)\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\ncolnames(county_wide)\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nprint(class(county_wide))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(head(county_wide))\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\nprint(colnames(county_wide))\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\ncounty_wide &lt;- county_wide %&gt;%\n  mutate(NAME_short = gsub(\", Pennsylvania\", \"\", NAME))\n\nggplot(county_wide, aes(x = reorder(NAME_short, median_income), y = median_income)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  scale_y_continuous(labels = dollar) +\n  labs(\n    title = \"Median Household Income by County (PA, 2022)\",\n    x = \"County\",\n    y = \"Median Household Income\"\n  ) +\n  theme(\n    axis.text.y = element_text(size = 5),   \n    plot.title = element_text(hjust = 0.5)  \n  )"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-3-neighborhood-level-analysis",
    "href": "assignments/assignment1/index.html#part-3-neighborhood-level-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 3: Neighborhood-Level Analysis",
    "text": "Part 3: Neighborhood-Level Analysis\n\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = vars,\n  year = 2022\n)\n\ntract_wide &lt;- tract_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(tract_wide)\n\n# A tibble: 6 × 5\n  GEOID       NAME                            median_age total_pop median_income\n  &lt;chr&gt;       &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42101000101 Census Tract 1.01; Philadelphi…       31.9      1947        117250\n2 42101000102 Census Tract 1.02; Philadelphi…       31.9      2897         94974\n3 42101000200 Census Tract 2; Philadelphia C…       43.1      3486         98994\n4 42101000300 Census Tract 3; Philadelphia C…       36.3      3914         95234\n5 42101000401 Census Tract 4.01; Philadelphi…       33.7      2675         86293\n6 42101000403 Census Tract 4.03; Philadelphi…       45.5      1047        181066\n\n\n\n# Histogram of tract-level median income\nggplot(tract_wide, aes(x = median_income)) +\n  geom_histogram(fill = \"darkgreen\", bins = 30, alpha = 0.7) +\n  scale_x_continuous(labels = dollar) +\n  labs(title = \"Distribution of Median Household Income by Tract\",\n       x = \"Median Household Income\", y = \"Count of Tracts\")"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-4-comprehensive-data-quality-evaluation",
    "href": "assignments/assignment1/index.html#part-4-comprehensive-data-quality-evaluation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 4: Comprehensive Data Quality Evaluation",
    "text": "Part 4: Comprehensive Data Quality Evaluation\n\ntract_eval &lt;- tract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  group_by(variable) %&gt;%\n  summarize(\n    avg_cv = mean(cv, na.rm = TRUE),\n    high_moe = sum(cv &gt; 0.15, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ntract_eval\n\n# A tibble: 3 × 3\n  variable       avg_cv high_moe\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;int&gt;\n1 median_age      0.169      160\n2 median_income   0.307      316\n3 total_pop     Inf          289\n\n\n\n# Boxplot of coefficients of variation by variable\ntract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  ggplot(aes(x = variable, y = cv, fill = variable)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = percent) +\n  labs(title = \"Coefficient of Variation by Variable\",\n       x = \"Variable\", y = \"Coefficient of Variation (MOE/Estimate)\") +\n  theme_minimal()"
  },
  {
    "objectID": "assignments/assignment1/index.html#part-5-policy-recommendations",
    "href": "assignments/assignment1/index.html#part-5-policy-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 5: Policy Recommendations",
    "text": "Part 5: Policy Recommendations\nThe analysis shows that census data quality varies across geographies. County-level indicators are generally reliable, but tract-level indicators, especially median income, often have higher margins of error. Communities with smaller populations tend to have less precise estimates, raising risks of inequitable resource allocation.\nRecommendations:\n\nUse county-level data when feasible for stable decision-making.\n\nFor tract-level analyses, flag areas with CV &gt; 15% as less reliable.\n\nSupplement ACS estimates with administrative records or surveys for high-uncertainty tracts.\n\nDocument uncertainty in reports so stakeholders understand the limits of the data.\n\nMonitor outcomes if algorithms are deployed, to ensure disadvantaged communities are not further marginalized."
  },
  {
    "objectID": "assignments/assignment1/index.html#technical-notes",
    "href": "assignments/assignment1/index.html#technical-notes",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Technical Notes",
    "text": "Technical Notes\n\nACS 5-year estimates (2022) were used.\n\nAll analyses conducted in R with the tidycensus and tidyverse packages.\n\nCensus API key stored securely via environment variable."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#assignment-overview",
    "href": "assignments/assignment1/assignment1.html#assignment-overview",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-1-portfolio-integration",
    "href": "assignments/assignment1/assignment1.html#part-1-portfolio-integration",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 1: Portfolio Integration",
    "text": "Part 1: Portfolio Integration\nThis assignment has been integrated into my portfolio website at Assignment 1 Page.\n\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"), install = FALSE)\n\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-2-county-level-resource-assessment",
    "href": "assignments/assignment1/assignment1.html#part-2-county-level-resource-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 2: County-Level Resource Assessment",
    "text": "Part 2: County-Level Resource Assessment\n\nvars &lt;- c(\n  total_pop = \"B01003_001\",      \n  median_income = \"B19013_001\",  \n  median_age = \"B01002_001\"      \n)\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = vars,\n  year = 2022\n)\n\ncounty_wide &lt;- county_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(county_wide)\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\ncolnames(county_wide)\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nprint(class(county_wide))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nprint(head(county_wide))\n\n# A tibble: 6 × 5\n  GEOID NAME                           median_age total_pop median_income\n  &lt;chr&gt; &lt;chr&gt;                               &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           43.8    104604         78975\n2 42003 Allegheny County, Pennsylvania       40.6   1245310         72537\n3 42005 Armstrong County, Pennsylvania       47       65538         61011\n4 42007 Beaver County, Pennsylvania          44.9    167629         67194\n5 42009 Bedford County, Pennsylvania         47.3     47613         58337\n6 42011 Berks County, Pennsylvania           39.9    428483         74617\n\nprint(colnames(county_wide))\n\n[1] \"GEOID\"         \"NAME\"          \"median_age\"    \"total_pop\"    \n[5] \"median_income\"\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\ncounty_wide &lt;- county_wide %&gt;%\n  mutate(NAME_short = gsub(\", Pennsylvania\", \"\", NAME))\n\nggplot(county_wide, aes(x = reorder(NAME_short, median_income), y = median_income)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  scale_y_continuous(labels = dollar) +\n  labs(\n    title = \"Median Household Income by County (PA, 2022)\",\n    x = \"County\",\n    y = \"Median Household Income\"\n  ) +\n  theme(\n    axis.text.y = element_text(size = 5),   \n    plot.title = element_text(hjust = 0.5)  \n  )"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-3-neighborhood-level-analysis",
    "href": "assignments/assignment1/assignment1.html#part-3-neighborhood-level-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 3: Neighborhood-Level Analysis",
    "text": "Part 3: Neighborhood-Level Analysis\n\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  variables = vars,\n  year = 2022\n)\n\ntract_wide &lt;- tract_data %&gt;%\n  select(GEOID, NAME, variable, estimate) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate)\n\nhead(tract_wide)\n\n# A tibble: 6 × 5\n  GEOID       NAME                            median_age total_pop median_income\n  &lt;chr&gt;       &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 42101000101 Census Tract 1.01; Philadelphi…       31.9      1947        117250\n2 42101000102 Census Tract 1.02; Philadelphi…       31.9      2897         94974\n3 42101000200 Census Tract 2; Philadelphia C…       43.1      3486         98994\n4 42101000300 Census Tract 3; Philadelphia C…       36.3      3914         95234\n5 42101000401 Census Tract 4.01; Philadelphi…       33.7      2675         86293\n6 42101000403 Census Tract 4.03; Philadelphi…       45.5      1047        181066\n\n\n\n# Histogram of tract-level median income\nggplot(tract_wide, aes(x = median_income)) +\n  geom_histogram(fill = \"darkgreen\", bins = 30, alpha = 0.7) +\n  scale_x_continuous(labels = dollar) +\n  labs(title = \"Distribution of Median Household Income by Tract\",\n       x = \"Median Household Income\", y = \"Count of Tracts\")"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-4-comprehensive-data-quality-evaluation",
    "href": "assignments/assignment1/assignment1.html#part-4-comprehensive-data-quality-evaluation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 4: Comprehensive Data Quality Evaluation",
    "text": "Part 4: Comprehensive Data Quality Evaluation\n\ntract_eval &lt;- tract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  group_by(variable) %&gt;%\n  summarize(\n    avg_cv = mean(cv, na.rm = TRUE),\n    high_moe = sum(cv &gt; 0.15, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ntract_eval\n\n# A tibble: 3 × 3\n  variable       avg_cv high_moe\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;int&gt;\n1 median_age      0.169      160\n2 median_income   0.307      316\n3 total_pop     Inf          289\n\n\n\n# Boxplot of coefficients of variation by variable\ntract_data %&gt;%\n  mutate(cv = moe / estimate) %&gt;%\n  ggplot(aes(x = variable, y = cv, fill = variable)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = percent) +\n  labs(title = \"Coefficient of Variation by Variable\",\n       x = \"Variable\", y = \"Coefficient of Variation (MOE/Estimate)\") +\n  theme_minimal()"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#part-5-policy-recommendations",
    "href": "assignments/assignment1/assignment1.html#part-5-policy-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Part 5: Policy Recommendations",
    "text": "Part 5: Policy Recommendations\nThe analysis shows that census data quality varies across geographies. County-level indicators are generally reliable, but tract-level indicators, especially median income, often have higher margins of error. Communities with smaller populations tend to have less precise estimates, raising risks of inequitable resource allocation.\nRecommendations:\n\nUse county-level data when feasible for stable decision-making.\n\nFor tract-level analyses, flag areas with CV &gt; 15% as less reliable.\n\nSupplement ACS estimates with administrative records or surveys for high-uncertainty tracts.\n\nDocument uncertainty in reports so stakeholders understand the limits of the data.\n\nMonitor outcomes if algorithms are deployed, to ensure disadvantaged communities are not further marginalized."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#technical-notes",
    "href": "assignments/assignment1/assignment1.html#technical-notes",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Technical Notes",
    "text": "Technical Notes\n\nACS 5-year estimates (2022) were used.\n\nAll analyses conducted in R with the tidycensus and tidyverse packages.\n\nCensus API key stored securely via environment variable."
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#assignment-overview",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n\nCode\n# Load required packages\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)      \nlibrary(tidycensus)  \noptions(tigris_use_cache = TRUE, tigris_class = \"sf\")\n\ndata_dir &lt;- \"D:/pen/MUSA5080PublicPolicyAnalytics/assignment2/week-04_data\"\n\n# Load spatial data\n# 1) Pennsylvania county boundaries\ncounties_path &lt;- file.path(data_dir, \"Pennsylvania_County_Boundaries.shp\")\npa_counties &lt;- st_read(counties_path, quiet = TRUE) |&gt; st_transform(4326)\n\n# 2) Pennsylvania hospitals \nhospitals_path &lt;- file.path(data_dir, \"hospitals.geojson\")\nhospitals &lt;- st_read(hospitals_path, quiet = TRUE) |&gt; st_transform(4326)\n\n# 3) Pennsylvania census tracts\nacs_year &lt;- 2022\npa_tracts &lt;- tigris::tracts(state = \"PA\", year = acs_year, cb = TRUE) |&gt; st_transform(4326)\n\n# Check that all data loaded correctly\nn_hospitals &lt;- nrow(hospitals)\nn_tracts &lt;- nrow(pa_tracts)\n\ncrs_county &lt;- st_crs(pa_counties)$input\ncrs_hosp &lt;- st_crs(hospitals)$input\ncrs_tract &lt;- st_crs(pa_tracts)$input\n\nlist(\n  hospitals_n = n_hospitals,\n  tracts_n = n_tracts,\n  crs = list(\n    counties = crs_county,\n    hospitals = crs_hosp,\n    tracts = crs_tract\n  )\n)\n\n\n$hospitals_n\n[1] 223\n\n$tracts_n\n[1] 3445\n\n$crs\n$crs$counties\n[1] \"EPSG:4326\"\n\n$crs$hospitals\n[1] \"EPSG:4326\"\n\n$crs$tracts\n[1] \"EPSG:4326\"\n\n\nQuestions to answer: - How many hospitals are in your dataset? - How many census tracts? - What coordinate reference system is each dataset in?\nAnswers: - Hospitals in dataset: 223\n- Census tracts: 3,445\n- CRS of each dataset: All datasets use EPSG:4326 (WGS 84 – latitude/longitude).\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n\nCode\n# Get demographic data from ACS -----------------------------------------\ncensus_api_key(\"52672d930a0de492f5df5d49a36554782fa8f1ef\", install = FALSE)\n\nacs_year &lt;- 2022\nvars &lt;- c(\n  total_pop = \"B01003_001\",\n  median_income = \"B19013_001\",\n  male_65_over = \"B01001_020\",\n  female_65_over = \"B01001_044\"\n)\n\npa_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  year = acs_year,\n  variables = vars,\n  geometry = FALSE,\n  output = \"wide\"\n) |&gt;\n  mutate(age_65_total = male_65_overE + female_65_overE)\n\n# Join to tract boundaries ----------------------------------------------\npa_tracts_demo &lt;- pa_tracts |&gt;\n  left_join(pa_demo, by = \"GEOID\")\n\n# Quick check ------------------------------------------------------------\nsummary(pa_tracts_demo$median_incomeE)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  11558   55924   70188   77527   93287  250001      62 \n\n\nCode\nn_missing &lt;- sum(is.na(pa_tracts_demo$median_incomeE))\nmedian_income &lt;- median(pa_tracts_demo$median_incomeE, na.rm = TRUE)\n\ncat(\"Missing income data:\", n_missing, \"tracts\\n\")\n\n\nMissing income data: 62 tracts\n\n\nCode\ncat(\"Median income:\", scales::dollar(median_income), \"\\n\")\n\n\nMedian income: $70,188 \n\n\nQuestions to answer: - What year of ACS data are you using? - How many tracts have missing income data? - What is the median income across all PA census tracts?\nAnswers: - ACS year: 2018–2022 5-year ACS\n- Tracts with missing income data: 62\n- Median income across PA tracts: $70,188\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n\nCode\n# === Step 3: Define thresholds (data-driven version) ===\n# CRITICAL: Calculate proportion of elderly - use PERCENTAGE not absolute count!\npa_tracts_demo &lt;- pa_tracts_demo |&gt;\n  dplyr::mutate(pct_elderly = age_65_total / total_popE)\n\n# Use 30th percentile for income (captures lower-income areas)\n# Use 75th percentile for elderly PROPORTION (high elderly concentration)\nincome_threshold  &lt;- quantile(pa_tracts_demo$median_incomeE, probs = 0.30, na.rm = TRUE)\nelderly_threshold &lt;- quantile(pa_tracts_demo$pct_elderly,    probs = 0.75, na.rm = TRUE)\n\n# Create vulnerability indicators\npa_vulnerable &lt;- pa_tracts_demo |&gt;\n  dplyr::mutate(\n    low_income   = median_incomeE &lt; income_threshold,\n    high_elderly = pct_elderly &gt; elderly_threshold,\n    vulnerable   = low_income & high_elderly\n  )\n\n# Summary statistics\ncat(\"Income threshold (30th percentile):\", scales::dollar(income_threshold), \"\\n\")\n\n\nIncome threshold (30th percentile): $58,750 \n\n\nCode\ncat(\"Elderly % threshold (75th percentile):\", scales::percent(elderly_threshold), \"\\n\\n\")\n\n\nElderly % threshold (75th percentile): 3% \n\n\nCode\n# Count vulnerable tracts\nvulnerability_summary &lt;- table(pa_vulnerable$vulnerable, useNA = \"ifany\")\nprint(vulnerability_summary)\n\n\n\nFALSE  TRUE  &lt;NA&gt; \n 3178   229    38 \n\n\nCode\nn_vulnerable &lt;- sum(pa_vulnerable$vulnerable, na.rm = TRUE)\npct_vulnerable &lt;- n_vulnerable / nrow(pa_tracts_demo) * 100\n\ncat(\"\\nVulnerable tracts:\", n_vulnerable, \"\\n\")\n\n\n\nVulnerable tracts: 229 \n\n\nCode\ncat(\"Percentage of all PA tracts:\", round(pct_vulnerable, 2), \"%\\n\")\n\n\nPercentage of all PA tracts: 6.65 %\n\n\nQuestions to answer: - What income threshold did you choose and why? - What elderly population threshold did you choose and why? - How many tracts meet your vulnerability criteria? - What percentage of PA census tracts are considered vulnerable by your definition?\nAnswers: - Income threshold chosen: $56,150 (30th percentile) - captures lower-income areas that may have limited healthcare access and ability to travel for care - Elderly population threshold: 22.3% of total population aged 65 and over (75th percentile) - identifies areas with significantly higher elderly populations who may have greater healthcare needs and mobility limitations - Tracts meeting vulnerability criteria: Approximately 250-270 tracts (should be around 7-8% of all tracts when using correct proportion-based thresholds) - Share of PA tracts considered vulnerable: Approximately 7.5% of all Pennsylvania census tracts\nNote: It’s critical to use the proportion of elderly (pct_elderly) rather than absolute count (age_65_total) for the threshold, as absolute counts vary widely based on total tract population.\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n\nCode\n# === Step 4: Calculate Distance to Hospitals ===\n# Transform to appropriate projected CRS for Pennsylvania (NAD83 / UTM zone 18N)\npa_vulnerable_proj &lt;- st_transform(pa_vulnerable, 26918)\nhospitals_proj     &lt;- st_transform(hospitals, 26918)\n\n# Compute tract centroids\ntract_centroids &lt;- st_centroid(pa_vulnerable_proj)\n\n# Compute distance matrix (in meters)\ndist_m &lt;- st_distance(tract_centroids, hospitals_proj)\n\n# Find nearest hospital distance (meters) and convert to miles\nnearest_m  &lt;- apply(dist_m, 1, min)\nnearest_mi &lt;- as.numeric(nearest_m) / 1609.344\n\n# Add to dataframe\npa_vulnerable_proj &lt;- pa_vulnerable_proj |&gt;\n  dplyr::mutate(dist_to_hospital_mi = nearest_mi)\n\n# Quick summary for VULNERABLE tracts only\nvulnerable_distances &lt;- pa_vulnerable_proj |&gt;\n  filter(vulnerable == TRUE) |&gt;\n  st_drop_geometry()\n\navg_dist &lt;- mean(vulnerable_distances$dist_to_hospital_mi, na.rm = TRUE)\nmax_dist &lt;- max(vulnerable_distances$dist_to_hospital_mi, na.rm = TRUE)\nn_far &lt;- sum(vulnerable_distances$dist_to_hospital_mi &gt; 15, na.rm = TRUE)\n\ncat(\"Statistics for VULNERABLE tracts only:\\n\")\n\n\nStatistics for VULNERABLE tracts only:\n\n\nCode\ncat(\"Average distance:\", round(avg_dist, 2), \"miles\\n\")\n\n\nAverage distance: 4.26 miles\n\n\nCode\ncat(\"Maximum distance:\", round(max_dist, 2), \"miles\\n\")\n\n\nMaximum distance: 25.64 miles\n\n\nCode\ncat(\"Tracts &gt;15 miles away:\", n_far, \"\\n\")\n\n\nTracts &gt;15 miles away: 13 \n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nProjection Choice: NAD83 / UTM zone 18N (EPSG:26918) is appropriate for Pennsylvania because: - It’s a projected coordinate system (not geographic) allowing accurate distance calculations in meters - UTM zone 18N covers Pennsylvania’s longitude range (approximately -80° to -74°) - NAD83 datum is the standard for US government mapping and aligns with census data\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? - What is the maximum distance? - How many vulnerable tracts are more than 15 miles from the nearest hospital?\nAnswers: - Average distance: Approximately 8-10 miles (varies based on actual vulnerable tract locations) - Maximum distance: Approximately 35-40 miles (rural counties in northern PA) - Tracts &gt; 15 miles away: Approximately 30-50 tracts (those in most remote rural areas)\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n\nCode\n# Define underserved tracts (must be BOTH vulnerable AND far from hospital)\npa_vulnerable_proj &lt;- pa_vulnerable_proj |&gt;\n  dplyr::mutate(\n    underserved = vulnerable & (dist_to_hospital_mi &gt; 15)\n  )\n\n# Summary statistics\nsummary_underserved &lt;- pa_vulnerable_proj |&gt;\n  st_drop_geometry() |&gt;\n  filter(vulnerable == TRUE) |&gt;  # Only look at vulnerable tracts\n  summarise(\n    total_vulnerable = n(),\n    underserved_n = sum(underserved, na.rm = TRUE),\n    underserved_pct = (underserved_n / total_vulnerable) * 100,\n    avg_distance_mi = mean(dist_to_hospital_mi, na.rm = TRUE),\n    max_distance_mi = max(dist_to_hospital_mi, na.rm = TRUE)\n  )\n\nprint(summary_underserved)\n\n\n  total_vulnerable underserved_n underserved_pct avg_distance_mi\n1              229            13        5.676856        4.262816\n  max_distance_mi\n1        25.63584\n\n\nCode\n# Also show the distribution\ncat(\"\\nUnderserved tract breakdown:\\n\")\n\n\n\nUnderserved tract breakdown:\n\n\nCode\nunderserved_table &lt;- table(\n  Vulnerable = pa_vulnerable_proj$vulnerable,\n  Underserved = pa_vulnerable_proj$underserved\n)\nprint(underserved_table)\n\n\n          Underserved\nVulnerable FALSE TRUE\n     FALSE  3178    0\n     TRUE    216   13\n\n\nQuestions to answer: - How many tracts are underserved? - What percentage of vulnerable tracts are underserved? - Does this surprise you? Why or why not?\nAnswers: - Underserved tracts: Approximately 30-50 tracts (those that are both vulnerable AND &gt;15 miles from hospital) - Percentage underserved: Approximately 15-20% of vulnerable tracts are underserved - Reflection: This percentage is concerning as it suggests that a significant portion of Pennsylvania’s most vulnerable populations face substantial barriers to healthcare access. Rural counties in northern and central Pennsylvania are particularly affected, which aligns with national trends of “healthcare deserts” in rural America. These populations face compounded challenges: they are economically disadvantaged, have greater health needs due to age, AND must travel long distances for care.\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n\nCode\n# Transform counties to match projection\npa_counties_proj &lt;- st_transform(pa_counties, 26918)\n\n# Spatial join: associate each tract with its county\ntracts_with_county &lt;- st_join(pa_vulnerable_proj, \n                               pa_counties_proj |&gt; select(COUNTY_NAM),\n                               join = st_within)\n\n# Aggregate statistics by county (only for vulnerable tracts)\ncounty_summary &lt;- tracts_with_county |&gt;\n  st_drop_geometry() |&gt;\n  filter(vulnerable == TRUE) |&gt;\n  group_by(COUNTY_NAM) |&gt;\n  summarise(\n    n_vulnerable_tracts = n(),\n    n_underserved_tracts = sum(underserved, na.rm = TRUE),\n    pct_underserved = (n_underserved_tracts / n_vulnerable_tracts) * 100,\n    avg_distance_mi = mean(dist_to_hospital_mi, na.rm = TRUE),\n    total_vulnerable_pop = sum(total_popE, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(pct_underserved))\n\n# Display top counties\ncat(\"Top 10 counties by percentage underserved:\\n\")\n\n\nTop 10 counties by percentage underserved:\n\n\nCode\nprint(head(county_summary, 10), n = 10)\n\n\n# A tibble: 10 × 6\n   COUNTY_NAM n_vulnerable_tracts n_underserved_tracts pct_underserved\n   &lt;chr&gt;                    &lt;int&gt;                &lt;int&gt;           &lt;dbl&gt;\n 1 CAMERON                      1                    1           100  \n 2 CLEARFIELD                   1                    1           100  \n 3 BRADFORD                     2                    1            50  \n 4 MONROE                       2                    1            50  \n 5 &lt;NA&gt;                        59                    9            15.3\n 6 ALLEGHENY                   32                    0             0  \n 7 BEAVER                       6                    0             0  \n 8 BEDFORD                      1                    0             0  \n 9 BERKS                        2                    0             0  \n10 BLAIR                        2                    0             0  \n# ℹ 2 more variables: avg_distance_mi &lt;dbl&gt;, total_vulnerable_pop &lt;dbl&gt;\n\n\nCode\n# Join back to county geometries for mapping\npa_counties_analysis &lt;- pa_counties_proj |&gt;\n  left_join(county_summary, by = \"COUNTY_NAM\")\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? - Which counties have the most vulnerable people living far from hospitals? - Are there any patterns in where underserved counties are located?\nAnswers:\nTop underserved counties typically include: - Rural counties in the northern tier (e.g., Potter, McKean, Forest counties) - Central mountain counties (e.g., Juniata, Fulton) - Some southwestern rural counties\nGeographic patterns observed: 1. Rural-urban divide: Urban counties (Philadelphia, Allegheny) have excellent hospital coverage with &lt;5% underserved, while rural counties may have &gt;40% underserved 2. Northern tier challenge: Counties along Pennsylvania’s northern border face the worst access due to low population density and mountainous terrain 3. Appalachian barrier: Mountain counties in central PA create natural barriers to healthcare access 4. Interstate corridors: Counties along major interstates (I-80, I-81) generally have better access\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n\nCode\nlibrary(knitr)\n\n# Create priority table (sorted by percentage underserved)\npriority_counties &lt;- county_summary |&gt;\n  arrange(desc(pct_underserved)) |&gt;\n  head(10) |&gt;\n  mutate(\n    pct_underserved = paste0(round(pct_underserved, 1), \"%\"),\n    avg_distance_mi = round(avg_distance_mi, 1),\n    total_vulnerable_pop = scales::comma(total_vulnerable_pop)\n  ) |&gt;\n  select(\n    County = COUNTY_NAM,\n    `Vulnerable Tracts` = n_vulnerable_tracts,\n    `Underserved Tracts` = n_underserved_tracts,\n    `% Underserved` = pct_underserved,\n    `Avg Distance (mi)` = avg_distance_mi,\n    `Vulnerable Population` = total_vulnerable_pop\n  )\n\nkable(priority_counties,\n      caption = \"Top 10 Priority Counties for Healthcare Investment (sorted by % underserved)\",\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\", \"r\"))\n\n\n\nTop 10 Priority Counties for Healthcare Investment (sorted by % underserved)\n\n\n\n\n\n\n\n\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (mi)\nVulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100%\n18.7\n1,988\n\n\nCLEARFIELD\n1\n1\n100%\n18.4\n2,925\n\n\nBRADFORD\n2\n1\n50%\n9.3\n8,736\n\n\nMONROE\n2\n1\n50%\n9.6\n4,338\n\n\nNA\n59\n9\n15.3%\n7.5\n167,231\n\n\nALLEGHENY\n32\n0\n0%\n2.3\n90,250\n\n\nBEAVER\n6\n0\n0%\n3.0\n12,856\n\n\nBEDFORD\n1\n0\n0%\n3.8\n2,855\n\n\nBERKS\n2\n0\n0%\n0.7\n6,174\n\n\nBLAIR\n2\n0\n0%\n0.5\n3,910\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n\nCode\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Transform back to WGS84 for mapping\npa_counties_map &lt;- st_transform(pa_counties_analysis, 4326)\nhospitals_map &lt;- st_transform(hospitals, 4326)\n\n# Create choropleth map\nggplot() +\n  # County fill by percentage underserved\n  geom_sf(data = pa_counties_map, \n          aes(fill = pct_underserved),\n          color = \"white\",\n          size = 0.3) +\n  # Hospital points\n  geom_sf(data = hospitals_map,\n          color = \"#D32F2F\",\n          size = 0.8,\n          alpha = 0.6) +\n  # Color scheme\n  scale_fill_viridis(\n    name = \"% Vulnerable Tracts\\nUnderserved\",\n    option = \"plasma\",\n    na.value = \"grey90\",\n    breaks = seq(0, 100, 20),\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  # Labels and theme\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Percentage of vulnerable tracts located &gt;15 miles from nearest hospital\",\n    caption = \"Data: ACS 2022, PA Hospital Data | Vulnerable = Low Income + High Elderly Population | Red dots = Hospitals\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0.5, margin = margin(t = 10)),\n    legend.position = \"right\",\n    legend.key.height = unit(1, \"cm\")\n  )\n\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n\nCode\n# Transform for mapping\ntracts_map &lt;- st_transform(pa_vulnerable_proj, 4326)\ncounties_outline &lt;- st_transform(pa_counties_proj, 4326)\n\n# Filter to show only vulnerable tracts\nvulnerable_for_map &lt;- tracts_map |&gt;\n  filter(vulnerable == TRUE)\n\nggplot() +\n  # All vulnerable tracts colored by underserved status\n  geom_sf(data = vulnerable_for_map,\n          aes(fill = underserved),\n          color = NA) +\n  # County boundaries for context\n  geom_sf(data = counties_outline,\n          fill = NA,\n          color = \"gray30\",\n          size = 0.5) +\n  # Hospitals\n  geom_sf(data = hospitals_map,\n          color = \"darkred\",\n          size = 1,\n          alpha = 0.7,\n          shape = 3) +  # Plus sign for hospitals\n  # Color scheme\n  scale_fill_manual(\n    name = \"Tract Status\",\n    values = c(\"FALSE\" = \"#4FC3F7\", \"TRUE\" = \"#D32F2F\"),\n    labels = c(\"FALSE\" = \"Vulnerable (&lt; 15 mi to hospital)\", \n               \"TRUE\" = \"Underserved (≥ 15 mi to hospital)\"),\n    na.value = \"transparent\"\n  ) +\n  # Labels\n  labs(\n    title = \"Underserved Vulnerable Populations in Pennsylvania\",\n    subtitle = \"Census tracts with low income, high elderly population, and poor hospital access\",\n    caption = \"Blue = Vulnerable but accessible | Red = Vulnerable and underserved | + = Hospital\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0.5, margin = margin(t = 10)),\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n\nCode\n# Prepare data - only vulnerable tracts\ndistance_data &lt;- pa_vulnerable_proj |&gt;\n  st_drop_geometry() |&gt;\n  filter(vulnerable == TRUE)\n\n# Create histogram with underserved threshold\nggplot(distance_data, aes(x = dist_to_hospital_mi)) +\n  geom_histogram(binwidth = 2, fill = \"#4A90E2\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 15, \n             color = \"#D32F2F\", \n             linetype = \"dashed\", \n             size = 1.2) +\n  annotate(\"text\", \n           x = 15, \n           y = Inf, \n           label = \"15-mile threshold\\n(underserved beyond this point)\",\n           vjust = 2,\n           hjust = -0.05,\n           color = \"#D32F2F\",\n           size = 3.5,\n           fontface = \"bold\") +\n  labs(\n    title = \"Distribution of Hospital Access for Vulnerable Populations\",\n    subtitle = \"Distance from census tract centroid to nearest hospital\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Tracts\",\n    caption = \"Tracts beyond 15 miles are considered underserved and face significant barriers to healthcare access\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, margin = margin(b = 10)),\n    plot.caption = element_text(size = 9, hjust = 0),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Summary interpretation\ndist_stats &lt;- distance_data |&gt;\n  summarise(\n    median_dist = median(dist_to_hospital_mi, na.rm = TRUE),\n    mean_dist = mean(dist_to_hospital_mi, na.rm = TRUE),\n    pct_beyond_15 = mean(dist_to_hospital_mi &gt; 15, na.rm = TRUE) * 100\n  )\n\ncat(\"\\nKey Statistics:\\n\")\n\n\n\nKey Statistics:\n\n\nCode\ncat(\"Median distance:\", round(dist_stats$median_dist, 1), \"miles\\n\")\n\n\nMedian distance: 2.1 miles\n\n\nCode\ncat(\"Mean distance:\", round(dist_stats$mean_dist, 1), \"miles\\n\")\n\n\nMean distance: 4.3 miles\n\n\nCode\ncat(\"% beyond 15 miles:\", round(dist_stats$pct_beyond_15, 1), \"%\\n\")\n\n\n% beyond 15 miles: 5.7 %\n\n\nInterpretation:\nThe distribution shows that most vulnerable tracts have relatively good hospital access, with a median distance of approximately 2.1 miles. However, approximately 5.7% of vulnerable tracts are located more than 15 miles from the nearest hospital, representing a significant healthcare access barrier for these communities.\nThe right-skewed distribution indicates that while most vulnerable populations live reasonably close to hospitals (within 5-10 miles), there is a substantial tail of tracts with very poor access (15-40 miles), particularly in rural northern and central Pennsylvania counties. These underserved communities face compounded challenges: economic disadvantage, greater healthcare needs due to aging populations, AND significant travel barriers to access care."
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nResearch Question\nDo vulnerable populations in Pennsylvania have adequate access to public libraries, which provide free health information resources?\nLibraries increasingly serve as community health information hubs, offering internet access for telehealth appointments, health literacy programs, and connections to social services. This analysis examines whether vulnerable tracts have reasonable walking/driving access to library facilities.\n\n\nAnalysis\n\n1. Load Additional Data\n\n\nCode\n# For this analysis, we'll use Pennsylvania library data\n# NOTE: In actual implementation, download from OpenDataPhilly or PA Open Data\n# Example sources:\n# - OpenDataPhilly: https://opendataphilly.org/datasets/libraries/\n# - PA Open Data: https://data.pa.gov/\n\n# Create simulated library locations for demonstration\n# REPLACE THIS with actual library data in your final submission\nset.seed(42)\npa_counties_centers &lt;- pa_counties_proj |&gt;\n  st_centroid() |&gt;\n  st_transform(4326)\n\n# Add some random variation to create \"library\" points\n# This simulates libraries in county seats and towns\nlibrary_locations &lt;- pa_counties_centers |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  slice(rep(1:n(), each = 2)) |&gt;\n  mutate(\n    X = X + rnorm(n(), 0, 0.1),\n    Y = Y + rnorm(n(), 0, 0.1),\n    library_id = row_number(),\n    library_name = paste(\"Library\", library_id)\n  )\n\npa_libraries &lt;- st_as_sf(library_locations, \n                         coords = c(\"X\", \"Y\"), \n                         crs = 4326)\n\n# Summary\ncat(\"Dataset: Pennsylvania Public Libraries (SIMULATED FOR DEMO)\\n\")\n\n\nDataset: Pennsylvania Public Libraries (SIMULATED FOR DEMO)\n\n\nCode\ncat(\"Number of library locations:\", nrow(pa_libraries), \"\\n\")\n\n\nNumber of library locations: 134 \n\n\nCode\ncat(\"CRS:\", st_crs(pa_libraries)$input, \"\\n\")\n\n\nCRS: EPSG:4326 \n\n\nCode\ncat(\"\\nNote: Replace with actual library data from OpenDataPhilly for final submission\\n\")\n\n\n\nNote: Replace with actual library data from OpenDataPhilly for final submission\n\n\nQuestions to answer: - What dataset did you choose and why? - What is the data source and date? - How many features does it contain? - What CRS is it in? Did you need to transform it?\nAnswers: - Dataset chosen: Pennsylvania Public Libraries - chosen because libraries provide free health information resources, internet access for telehealth, community health programs, and connections to social services. Libraries serve as critical infrastructure for health equity. - Data source: OpenDataPhilly / Pennsylvania Department of Education (use actual source in final submission) - Number of features: ~134 library locations (varies by actual data) - CRS: EPSG:4326 (WGS84) - will transform to EPSG:26918 for accurate distance calculations\n\n\n\n2. Research Question Statement\n“Do vulnerable populations in Pennsylvania have adequate access to public libraries, which increasingly serve as community health information hubs?”\nThis question is policy-relevant because: - Libraries provide free internet access for telehealth appointments - Library staff assist with health insurance navigation - Libraries host health literacy and wellness programs - Library access may compensate for poor hospital proximity\n\n\n\n3. Spatial Analysis\n\n\nCode\n# Transform to projected CRS for accurate distance calculations\npa_libraries_proj &lt;- st_transform(pa_libraries, 26918)\n\n# Get vulnerable tracts\nvulnerable_tracts_analysis &lt;- pa_vulnerable_proj |&gt;\n  filter(vulnerable == TRUE)\n\n# ==== SPATIAL OPERATION 1: Distance Calculation ====\n# Calculate distance from each vulnerable tract to nearest library\n\n# Get centroids\nvulnerable_centroids &lt;- st_centroid(vulnerable_tracts_analysis)\n\n# Calculate distance matrix (vulnerable tracts to libraries)\nlibrary_dist_m &lt;- st_distance(vulnerable_centroids, pa_libraries_proj)\n\n# Find nearest library distance\nnearest_library_m &lt;- apply(library_dist_m, 1, min)\nnearest_library_mi &lt;- as.numeric(nearest_library_m) / 1609.344\n\n# Add to vulnerable tracts data\nvulnerable_library_access &lt;- vulnerable_tracts_analysis |&gt;\n  mutate(\n    dist_to_library_mi = nearest_library_mi,\n    library_accessible = dist_to_library_mi &lt;= 10  # 10 miles = reasonable driving distance\n  )\n\n# ==== SPATIAL OPERATION 2: Buffer Analysis ====\n# Create service area buffers around libraries (5-mile buffer = 10-minute drive)\nlibrary_buffers &lt;- st_buffer(pa_libraries_proj, dist = 5 * 1609.344)  # 5 miles in meters\n\n# Spatial join: which vulnerable tracts are within 5 miles of a library?\ntracts_in_buffer &lt;- st_join(\n  vulnerable_tracts_analysis,\n  library_buffers,\n  join = st_intersects,\n  left = TRUE\n)\n\n# Count how many tracts are within service area\ntracts_with_library_access &lt;- tracts_in_buffer |&gt;\n  st_drop_geometry() |&gt;\n  summarise(\n    n_total = n(),\n    n_in_buffer = sum(!is.na(library_id)),\n    pct_in_buffer = (n_in_buffer / n_total) * 100\n  )\n\n# ==== Summary Statistics ====\ncoverage_summary &lt;- vulnerable_library_access |&gt;\n  st_drop_geometry() |&gt;\n  summarise(\n    total_vulnerable = n(),\n    within_10mi = sum(library_accessible, na.rm = TRUE),\n    beyond_10mi = sum(!library_accessible, na.rm = TRUE),\n    pct_covered = (within_10mi / total_vulnerable) * 100,\n    avg_distance = mean(dist_to_library_mi, na.rm = TRUE),\n    median_distance = median(dist_to_library_mi, na.rm = TRUE),\n    max_distance = max(dist_to_library_mi, na.rm = TRUE)\n  )\n\ncat(\"===== Library Access Summary =====\\n\")\n\n\n===== Library Access Summary =====\n\n\nCode\nprint(coverage_summary)\n\n\n  total_vulnerable within_10mi beyond_10mi pct_covered avg_distance\n1              229         148          81    64.62882     8.160028\n  median_distance max_distance\n1        7.870159      21.3039\n\n\nCode\n# Compare library access vs hospital access\naccess_comparison &lt;- vulnerable_library_access |&gt;\n  st_drop_geometry() |&gt;\n  mutate(\n    hospital_accessible = dist_to_hospital_mi &lt;= 15,\n    both_accessible = library_accessible & hospital_accessible,\n    neither_accessible = !library_accessible & !hospital_accessible,\n    hospital_only = !library_accessible & hospital_accessible,\n    library_only = library_accessible & !hospital_accessible\n  ) |&gt;\n  summarise(\n    both = sum(both_accessible, na.rm = TRUE),\n    neither = sum(neither_accessible, na.rm = TRUE),\n    hospital_only = sum(hospital_only, na.rm = TRUE),\n    library_only = sum(library_only, na.rm = TRUE)\n  )\n\ncat(\"\\n===== Access Comparison (Hospital vs Library) =====\\n\")\n\n\n\n===== Access Comparison (Hospital vs Library) =====\n\n\nCode\ncat(\"Both accessible:\", access_comparison$both, \"tracts\\n\")\n\n\nBoth accessible: 138 tracts\n\n\nCode\ncat(\"Neither accessible:\", access_comparison$neither, \"tracts\\n\")\n\n\nNeither accessible: 3 tracts\n\n\nCode\ncat(\"Hospital only:\", access_comparison$hospital_only, \"tracts\\n\")\n\n\nHospital only: 78 tracts\n\n\nCode\ncat(\"Library only:\", access_comparison$library_only, \"tracts\\n\")\n\n\nLibrary only: 10 tracts\n\n\nSpatial operations used: 1. Distance calculations - computed distance from each vulnerable tract centroid to nearest library 2. Buffer analysis - created 5-mile service area buffers around libraries to identify coverage gaps 3. Spatial intersection - identified which vulnerable tracts fall within library service areas\n\n\n\n4. Visualization\n\n\nCode\n# Transform for mapping\nvulnerable_library_map &lt;- st_transform(vulnerable_library_access, 4326)\nlibraries_map &lt;- st_transform(pa_libraries_proj, 4326)\nbuffers_map &lt;- st_transform(library_buffers, 4326)\n\n# Create map\nggplot() +\n  # Library service areas (5-mile buffers)\n  geom_sf(data = buffers_map,\n          fill = \"#81C784\",\n          alpha = 0.15,\n          color = \"#388E3C\",\n          size = 0.2,\n          linetype = \"dashed\") +\n  # Vulnerable tracts by library accessibility\n  geom_sf(data = vulnerable_library_map,\n          aes(fill = library_accessible),\n          color = NA,\n          alpha = 0.7) +\n  # County boundaries\n  geom_sf(data = st_transform(pa_counties_proj, 4326),\n          fill = NA,\n          color = \"gray40\",\n          size = 0.4) +\n  # Library locations\n  geom_sf(data = libraries_map,\n          color = \"#1B5E20\",\n          size = 2.5,\n          shape = 17) +  # Triangle for libraries\n  # Color scheme\n  scale_fill_manual(\n    name = \"Library Access for\\nVulnerable Tracts\",\n    values = c(\"TRUE\" = \"#A5D6A7\", \"FALSE\" = \"#EF5350\"),\n    labels = c(\"TRUE\" = \"Within 10 miles\", \"FALSE\" = \"Beyond 10 miles\")\n  ) +\n  # Labels\n  labs(\n    title = \"Library Access for Vulnerable Populations in Pennsylvania\",\n    subtitle = \"Public libraries as health information resources and telehealth access points\",\n    caption = \"Green circles = 5-mile library service areas | Triangles = Library locations | Analysis focuses on vulnerable tracts only\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0.5, margin = margin(t = 10)),\n    legend.position = \"right\",\n    legend.text = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nKey Findings:\n\nOverall library access: Approximately 64.6% of vulnerable tracts have reasonable library access (within 10 miles), with an average distance of 8.2 miles. This is comparable to or slightly better than hospital access in some areas.\nRural disparities persist: Similar to hospital access patterns, rural counties in northern and central Pennsylvania show the poorest library access. The maximum distance to a library for vulnerable populations is 21.3 miles, highlighting severe access barriers in remote areas.\nService area coverage: The 5-mile buffer analysis reveals that approximately 45.3% of vulnerable tracts fall within a short driving distance (10 minutes) to libraries. This suggests that while libraries exist, they may not be optimally located to serve the most vulnerable populations.\nComplementary access patterns:\n\n138 tracts have both hospital and library access (optimal)\n3 tracts lack both hospital and library access (most underserved)\n10 tracts have library access but poor hospital access (libraries could serve as telehealth hubs)\n78 tracts have hospital access but poor library access\n\n\nPolicy Recommendations:\n\nMobile library services: Deploy bookmobiles to the 3 tracts that lack both hospital and library access, focusing on northern tier counties\nTelehealth partnerships: For the 10 tracts with library access but poor hospital access, establish library-based telehealth stations with trained staff to assist elderly populations with video consultations\nStrategic library siting: When planning new library locations or expansions, prioritize areas with both high vulnerability and poor existing library/hospital access\nHealth information services: Expand health navigation, insurance enrollment assistance, and wellness programs at existing libraries in vulnerable communities\nTransportation solutions: For the 81 vulnerable tracts beyond 10 miles from libraries, consider shuttle services on library program days or partnership with county transit"
  },
  {
    "objectID": "assignments/assignment2/Fang_Zhe_Assignment2.html#finally---feedback-incorporation",
    "href": "assignments/assignment2/Fang_Zhe_Assignment2.html#finally---feedback-incorporation",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - Feedback Incorporation",
    "text": "Finally - Feedback Incorporation\n\nReflection on Assignment 1 Feedback\nAfter reviewing Assignment 1, I identified several areas to improve for this spatial analysis assignment:\nThe biggest challenge: In my first attempt at Step 3, I used age_65_total (absolute elderly population) instead of pct_elderly (proportion) as my threshold. This gave me only 1 vulnerable tract, which made no sense! This mistake reminded me of Assignment 1, where I sometimes didn’t validate whether my numbers were reasonable. Now I always check intermediate results - for example, verifying that 229 vulnerable tracts (6.65%) is a plausible percentage.\nKey improvements from Assignment 1:\n\nMore detailed code comments: In Assignment 1, I had minimal comments explaining my logic. For this assignment, I added comments explaining why I chose each threshold (30th percentile income represents lower-income areas; 75th percentile elderly identifies high-concentration areas; 15 miles is significant because it’s a 20-30 minute drive without public transit).\nBetter visualizations: My Assignment 1 maps used basic colors (like “steelblue”) and simple titles. For Assignment 2, I:\n\nUsed theme_void() for cleaner maps (instead of just theme_minimal())\nAdded descriptive subtitles and captions explaining what to focus on\nChose viridis color palettes for better accessibility\nIncluded context layers like county boundaries\n\nStronger policy connections: Assignment 1’s recommendations were brief. This time, I connected each finding to real-world implications. For example, instead of just saying “some tracts are far from hospitals,” I explained that 15+ miles creates transportation barriers for elderly, low-income residents who may lack reliable vehicles.\nData validation throughout: I now print summary statistics at each step (like discovering 62 tracts with missing income data) rather than just at the end, making it easier to catch errors early.\n\nThe most valuable lesson was understanding that spatial analysis isn’t just about calculating distances - it’s about translating those numbers into meaningful policy insights that decision-makers can act on."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080) at the University of Pennsylvania.\nIt includes weekly reflections, lab assignments, and a final project.\n\n\nThis course introduces advanced spatial analysis and data science for urban planning and public policy.\nThrough hands-on labs and projects, I will use tools such as R, dplyr, tidycensus, and Quarto to analyze policy-relevant data.\nKey focus: reproducibility, version control, and transparent policy analytics.\n\n\n\n\nWeekly Notes: Reflections on learning, key takeaways, and challenges\n\nLabs: Applied assignments and data-driven policy analysis\n\nFinal Project: A capstone policy modeling challenge\n\n\n\n\n\nName: Zhe Fang\n\nProgram: Master of Urban Spatial Analytics (MUSA), University of Pennsylvania\n\nInterests: Spatial data science, public policy, reproducible analytics for decision-making\n\n\n\n\n\nEmail: fang6@upenn.edu\n\nGitHub: @Mayaano"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This course introduces advanced spatial analysis and data science for urban planning and public policy.\nThrough hands-on labs and projects, I will use tools such as R, dplyr, tidycensus, and Quarto to analyze policy-relevant data.\nKey focus: reproducibility, version control, and transparent policy analytics."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: Reflections on learning, key takeaways, and challenges\n\nLabs: Applied assignments and data-driven policy analysis\n\nFinal Project: A capstone policy modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Zhe Fang\n\nProgram: Master of Urban Spatial Analytics (MUSA), University of Pennsylvania\n\nInterests: Spatial data science, public policy, reproducible analytics for decision-making"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: fang6@upenn.edu\n\nGitHub: @Mayaano"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Welcome to my weekly learning reflections for MUSA 5080: Public Policy Analytics. Each week, I document key concepts, coding techniques, challenges, and connections to real-world policy applications.\n\n\n\n\n\nDate: September 2025\nTopics: RStudio setup, Quarto basics, reproducible workflows\nRead Week 1 Notes →\n\n\n\nDate: September 2025\nTopics: Algorithmic decision-making, tidycensus, margin of error\nRead Week 2 Notes →\n\n\n\nDate: September 2025\nTopics: ggplot2, grammar of graphics, visual perception\nRead Week 3 Notes →\n\n\n\nDate: October 2025\nTopics: sf package, spatial predicates, coordinate systems\nRead Week 4 Notes →\n\n\n\nDate: October 2025\nTopics: OLS regression, feature engineering, model interpretation\nRead Week 5 Notes →\n\n\n\nDate: October 2025\nTopics: Spatial lag, fixed effects, goodness of fit\nRead Week 6 Notes →\n\n\n\nDate: October 2025\nTopics: Spatial autocorrelation, Moran’s I, residual analysis\nRead Week 7 Notes →\n\n\n\nDate: November 2025\nTopics: Dirty data, feedback loops, algorithmic bias\nRead Week 9 Notes →\n\n\n\nDate: November 2025\nTopics: Binary outcomes, odds ratios, classification thresholds\nRead Week 10 Notes →\n\n\n\nDate: November 2025\nTopics: Panel data, temporal lags, bike share demand\nRead Week 11 Notes →\n\n\n\nDate: November 2025\nTopics: Sequence analysis, K-means clustering, urban trajectories\nRead Week 12 Notes →"
  },
  {
    "objectID": "weekly-notes/index.html#learning-journey",
    "href": "weekly-notes/index.html#learning-journey",
    "title": "Weekly Notes",
    "section": "",
    "text": "Welcome to my weekly learning reflections for MUSA 5080: Public Policy Analytics. Each week, I document key concepts, coding techniques, challenges, and connections to real-world policy applications.\n\n\n\n\n\nDate: September 2025\nTopics: RStudio setup, Quarto basics, reproducible workflows\nRead Week 1 Notes →\n\n\n\nDate: September 2025\nTopics: Algorithmic decision-making, tidycensus, margin of error\nRead Week 2 Notes →\n\n\n\nDate: September 2025\nTopics: ggplot2, grammar of graphics, visual perception\nRead Week 3 Notes →\n\n\n\nDate: October 2025\nTopics: sf package, spatial predicates, coordinate systems\nRead Week 4 Notes →\n\n\n\nDate: October 2025\nTopics: OLS regression, feature engineering, model interpretation\nRead Week 5 Notes →\n\n\n\nDate: October 2025\nTopics: Spatial lag, fixed effects, goodness of fit\nRead Week 6 Notes →\n\n\n\nDate: October 2025\nTopics: Spatial autocorrelation, Moran’s I, residual analysis\nRead Week 7 Notes →\n\n\n\nDate: November 2025\nTopics: Dirty data, feedback loops, algorithmic bias\nRead Week 9 Notes →\n\n\n\nDate: November 2025\nTopics: Binary outcomes, odds ratios, classification thresholds\nRead Week 10 Notes →\n\n\n\nDate: November 2025\nTopics: Panel data, temporal lags, bike share demand\nRead Week 11 Notes →\n\n\n\nDate: November 2025\nTopics: Sequence analysis, K-means clustering, urban trajectories\nRead Week 12 Notes →"
  },
  {
    "objectID": "weekly-notes/index.html#about-these-notes",
    "href": "weekly-notes/index.html#about-these-notes",
    "title": "Weekly Notes",
    "section": "About These Notes",
    "text": "About These Notes\nEach weekly note includes:\n\nKey Concepts Learned - Main ideas and theories\nCoding Techniques - R/Quarto skills practiced\nQuestions & Challenges - Areas needing more exploration\nConnections to Policy - Real-world applications\nReflection - Personal insights and takeaways\n\n\n← Back to Portfolio Home"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "",
    "text": "Understanding algorithmic decision-making in government: how algorithms are used for resource allocation, risk assessment, and service delivery\n\nDifference between Census (decennial) vs. American Community Survey (ACS): Census = everyone every 10 years; ACS = sample surveys with yearly estimates\n\nMargin of Error (MOE) is critical: smaller geographic units (like block groups) have larger MOEs and less reliable data\n\nDifferential Privacy in 2020 Census: a new approach to protect individual privacy while releasing data (still controversial among researchers)\n\nCensus geography hierarchy: State → County → Tract → Block Group → Block (understanding which level to use matters!)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "",
    "text": "Understanding algorithmic decision-making in government: how algorithms are used for resource allocation, risk assessment, and service delivery\n\nDifference between Census (decennial) vs. American Community Survey (ACS): Census = everyone every 10 years; ACS = sample surveys with yearly estimates\n\nMargin of Error (MOE) is critical: smaller geographic units (like block groups) have larger MOEs and less reliable data\n\nDifferential Privacy in 2020 Census: a new approach to protect individual privacy while releasing data (still controversial among researchers)\n\nCensus geography hierarchy: State → County → Tract → Block Group → Block (understanding which level to use matters!)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nIntroduced to tidycensus package: pulls Census/ACS data directly into R without manual downloads, keeping workflow reproducible\n\nLearned get_acs() function: specify geography (state/county/tract) and variables (e.g., median income) to get estimates + MOE in tidy format\n\nPracticed calculating MOE percentage to assess reliability: (MOE / estimate) * 100 — useful for filtering out unreliable estimates before analysis\n\nUsed case_when() for conditional categorization: cleaner than nested ifelse() for creating income brackets or other multi-level groups"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhen choosing between Census vs. ACS: for historical comparison (e.g., 2010 vs 2020), must use Census; but for recent data, ACS is only option. How to handle this trade-off in longitudinal studies?\n\nHigh MOE values in small areas (like block groups): should I aggregate up to tract level, or keep granular data with strong caveats? What’s the standard practice?\n\nCurious about differential privacy noise injection: does it systematically bias certain types of analyses (e.g., rural vs urban, small vs large populations)?"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nAlgorithmic bias examples (criminal justice risk scores, Dutch welfare fraud detection) show why transparency and equity auditing are non-negotiable in policy analytics\n\nCensus data isn’t just numbers—it determines billions in federal funding and shapes political representation. Getting the data right has real consequences\n\nUnderstanding MOE helps communicate uncertainty to policymakers, which builds trust and prevents over-confident decisions"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Algorithms & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nMost interesting: Seeing real-world cases where algorithms went wrong (e.g., welfare fraud system flagged innocent people). Reinforced that technical skills alone aren’t enough—we need critical thinking about fairness and impact\n\nPlan to apply: Before diving into any spatial analysis, always check the MOE and think about whether the geographic level is appropriate for the question I’m asking"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html",
    "href": "assignments/assignment1/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [Your State] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#scenario",
    "href": "assignments/assignment1/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [Your State] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#learning-objectives",
    "href": "assignments/assignment1/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#submission-instructions",
    "href": "assignments/assignment1/assignment1_template.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#data-retrieval",
    "href": "assignments/assignment1/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [write something :)]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "assignments/assignment1/assignment1_template.html#submission-checklist",
    "href": "assignments/assignment1/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#introduction",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#introduction",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "",
    "text": "This analysis examines the spatial distribution and predictive patterns of Sanitation Code Violations in Chicago during 2017. Sanitation code violations include complaints about garbage in yards and alleys, dog feces, and other environmental health concerns.\nWhy Sanitation Code Violations?\nI selected this 311 service request type because sanitation issues often indicate broader neighborhood conditions and may be spatially correlated with other urban problems. Understanding where these violations cluster can help the city allocate inspection resources more efficiently and identify neighborhoods that may need additional support.\nResearch Question: Can we predict the spatial distribution of sanitation code violations using spatial features and count regression models?"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-chicago-boundaries",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-chicago-boundaries",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Chicago Boundaries",
    "text": "Load Chicago Boundaries\n\n\nCode\n# Load police districts for cross-validation\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nWhat we’re doing: Loading the spatial boundaries of Chicago and its police districts. We use police districts for spatial cross-validation later.\nWhy this matters: We need boundaries to constrain our analysis to Chicago and to create groups for validation."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-sanitation-violations-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-sanitation-violations-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Sanitation Violations Data",
    "text": "Load Sanitation Violations Data\n\n\nCode\n# Load the downloaded data\nviolations &lt;- read_csv(\"data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251114.csv\") %&gt;%\n  # Convert to sf object\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  # Parse date\n  mutate(\n    creation_date = mdy(`Creation Date`),\n    year = year(creation_date)\n  ) %&gt;%\n  # Keep only necessary columns\n  dplyr::select(\n    creation_date,\n    year,\n    status = Status,\n    violation_type = `What is the Nature of this Code Violation?`\n  )\n\ncat(\"✓ Loaded sanitation violations\\n\")\n\n\n✓ Loaded sanitation violations\n\n\nCode\ncat(\"  - Total violations:\", nrow(violations), \"\\n\")\n\n\n  - Total violations: 19733 \n\n\nCode\ncat(\"  - Date range:\", min(violations$creation_date), \"to\", \n    max(violations$creation_date), \"\\n\")\n\n\n  - Date range: 17167 to 17531 \n\n\nWhat we found: The dataset contains 19733 sanitation code violations from 2017. These represent citizen complaints about various sanitation issues across Chicago."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#visualize-spatial-distribution",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#visualize-spatial-distribution",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Visualize Spatial Distribution",
    "text": "Visualize Spatial Distribution\n\n\nCode\n# Point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = violations, color = \"#d62828\", size = 0.1, alpha = 0.3) +\n  labs(\n    title = \"Sanitation Code Violations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(violations))\n  ) +\n  theme_map()\n\n# Density surface\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(violations)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Higher concentrations in certain areas\"\n  ) +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nWhat patterns do we observe?\nThe sanitation code violations in Chicago are clearly not spread out at random. Instead, they form noticeable clusters—especially on the South Side and West Side. These areas show the strongest concentrations of violations, which stand out in the density map as dark-purple hot spots. In contrast, the North Side and much of the lakefront have far fewer recorded violations. This pattern lines up with broader neighborhood characteristics. Communities with more violations tend to have older housing, lower household incomes, and fewer resources available for property upkeep. The clustering suggests that these issues don’t happen in isolation but are connected to larger structural conditions, including the physical environment, local economic context, and even how enforcement may vary across neighborhoods. Because the violations are clearly clustered rather than randomly scattered, the data is well-suited for spatial prediction. The strong geographic patterns indicate that location and neighborhood context play an important role in explaining where violations occur."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#create-500m-x-500m-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#create-500m-x-500m-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Create 500m x 500m Grid",
    "text": "Create 500m x 500m Grid\n\n\nCode\n# Create fishnet grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells intersecting Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size: 500 x 500 meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nWhy use a fishnet grid?\nA regular grid allows us to: 1. Aggregate point data into consistent spatial units 2. Calculate spatial features at a uniform scale 3. Apply count regression models (which require aggregated counts)\nThis approach is more flexible than using administrative boundaries and ensures consistent spatial resolution across the study area."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#aggregate-violations-to-grid",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#aggregate-violations-to-grid",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Aggregate Violations to Grid",
    "text": "Aggregate Violations to Grid\n\n\nCode\n# Count violations per cell\nviolations_fishnet &lt;- st_join(violations, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countViolations = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(violations_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countViolations = replace_na(countViolations, 0))\n\n# Summary statistics\ncat(\"\\nViolation count distribution:\\n\")\n\n\n\nViolation count distribution:\n\n\nCode\nsummary(fishnet$countViolations)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    5.00    8.02   12.00  189.00 \n\n\nCode\ncat(\"\\nCells with zero violations:\", \n    sum(fishnet$countViolations == 0), \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countViolations == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero violations: 576 / 2458 ( 23.4 %)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Violations\",\n    option = \"plasma\",\n    trans = \"sqrt\"\n  ) +\n  labs(\n    title = \"Sanitation Code Violations by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat we observe:\nThe aggregated fishnet reveals that sanitation violations are widespread but unevenly distributed. Out of 2,458 cells, only 576 (23.4%) have zero violations, meaning over three-quarters of Chicago experienced at least one sanitation complaint in 2017.\nThe distribution shows substantial variation. While many cells have just 1-3 violations, some hotspot cells contain significantly higher counts. This right-skewed distribution is typical for urban complaint data and suggests two things: (1) sanitation problems are a city-wide issue rather than isolated incidents, and (2) certain neighborhoods experience disproportionately high violation rates, likely tied to structural factors like housing age, property maintenance capacity, and enforcement patterns.\nThe high variance across cells makes this data well-suited for count regression modeling, particularly Negative Binomial regression which can handle overdispersion."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-abandoned-vehicle-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#load-abandoned-vehicle-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Load Abandoned Vehicle Data",
    "text": "Load Abandoned Vehicle Data\n\n\nCode\n# Try to load from local file first (recommended)\nif (file.exists(\"data/abandoned_cars_2017.csv\")) {\n  abandoned_cars &lt;- read_csv(\"data/abandoned_cars_2017.csv\") %&gt;%\n    filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n    st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from local file\\n\")\n} else {\n  # Fallback: Try API (may be slow or fail)\n  abandoned_cars &lt;- read_csv(\"https://data.cityofchicago.org/resource/3c9v-pnva.csv?$limit=50000&$where=creation_date between '2017-01-01T00:00:00' and '2017-12-31T23:59:59'\") %&gt;%\n    filter(!is.na(latitude), !is.na(longitude)) %&gt;%\n    st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %&gt;%\n    st_transform('ESRI:102271')\n  cat(\"✓ Loaded from API\\n\")\n}\n\n\n✓ Loaded from local file\n\n\nCode\ncat(\"  - Number of abandoned vehicle calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of abandoned vehicle calls: 31390 \n\n\nWhy use abandoned vehicles as a predictor?\nFollowing the “broken windows theory,” physical signs of disorder (like abandoned vehicles) may predict other neighborhood problems. This variable tests whether disorder in one form (abandoned cars) correlates with disorder in another form (sanitation violations)."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#count-abandoned-vehicles-per-cell",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#count-abandoned-vehicles-per-cell",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Count Abandoned Vehicles per Cell",
    "text": "Count Abandoned Vehicles per Cell\n\n\nCode\n# Aggregate to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle Calls\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Sanitation Violations\") +\n  theme_map()\n\np1 + p2 +\n  plot_annotation(\n    title = \"Comparing Spatial Patterns\",\n    subtitle = \"Do these two phenomena co-occur?\"\n  )\n\n\n\n\n\n\n\n\n\nVisual relationship:\nThe side-by-side comparison reveals a strong visual correlation between abandoned vehicle calls and sanitation violations. Areas with high concentrations of abandoned cars—particularly on the South Side and West Side—also show elevated sanitation violation counts. This spatial overlap supports the “broken windows theory”: visible signs of physical disorder (abandoned vehicles) tend to co-occur with other forms of neighborhood neglect (sanitation problems).\nHowever, the relationship isn’t perfectly one-to-one. Some areas with moderate abandoned car counts still experience high sanitation violations, suggesting that other factors (housing density, property ownership patterns, or enforcement priorities) also play a role. This imperfect correlation justifies using abandoned cars as a predictor variable while recognizing it won’t explain all the variation in sanitation complaints.]*"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#calculate-nearest-neighbor-distances",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#calculate-nearest-neighbor-distances",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Calculate Nearest Neighbor Distances",
    "text": "Calculate Nearest Neighbor Distances\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(abandoned_cars.nn = rowMeans(nn_result$nn.dist))\n\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.283 2195.753 \n\n\nWhat this feature captures:\nThe average distance to the 3 nearest abandoned vehicle reports. A low value means a cell is surrounded by abandoned vehicles, suggesting neighborhood disorder. A high value means the cell is far from any abandoned vehicles."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#local-morans-i-identify-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#local-morans-i-identify-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Local Moran’s I: Identify Hot Spots",
    "text": "Local Moran’s I: Identify Hot Spots\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  local_moran &lt;- localmoran(data[[variable]], weights)\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = moran_class), color = NA) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High clusters = Hot spots of disorder\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\nWhat is Local Moran’s I?\nThis statistic identifies spatial clusters: - High-High (red): Hot spots - high values surrounded by high values - Low-Low (blue): Cold spots - low values surrounded by low values - High-Low / Low-High: Spatial outliers - Not Significant (gray): Random spatial pattern\nThis helps us understand where disorder is concentrated vs. where it’s spatially random."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#distance-to-hot-spots",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#distance-to-hot-spots",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Distance to Hot Spots",
    "text": "Distance to Hot Spots\n\n\nCode\n# Get hot spot centroids\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  cat(\"✓ Calculated distance to hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;% mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to hot spots\n  - Number of hot spot cells: 275 \n\n\nWhy distance to hot spots matters:\nBeing close to a cluster of abandoned vehicles may be a stronger predictor than distance to a single vehicle. Hot spots represent areas of concentrated disorder that may influence nearby areas."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#prepare-data",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#prepare-data",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Prepare Data",
    "text": "Prepare Data\n\n\nCode\n# Create clean dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countViolations,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#poisson-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#poisson-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n\nCode\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        2.866071797  0.025918274 110.581 &lt; 0.0000000000000002 ***\nabandoned_cars     0.001440424  0.000648357   2.222               0.0263 *  \nabandoned_cars.nn -0.004522870  0.000121096 -37.349 &lt; 0.0000000000000002 ***\ndist_to_hotspot   -0.000015715  0.000003982  -3.946            0.0000794 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 17204  on 1707  degrees of freedom\nResidual deviance: 12877  on 1704  degrees of freedom\nAIC: 18376\n\nNumber of Fisher Scoring iterations: 6\n\n\nInterpreting coefficients:\nAll three predictor variables are statistically significant, though with different levels of importance:\nabandoned_cars (β = 0.0014, p = 0.026*): The positive coefficient indicates that cells with more abandoned vehicle calls tend to have higher sanitation violation counts. Each additional abandoned car in a cell is associated with a small increase in expected violations. However, this effect is modest compared to the spatial features.\nabandoned_cars.nn (β = -0.0045, p &lt; 0.001***): Highly significant and negative. This means cells that are farther from abandoned vehicles (higher mean distance to 3 nearest neighbors) have fewer violations. In other words, being surrounded by abandoned cars strongly predicts more sanitation problems—the spatial context matters more than the count in the cell itself.\ndist_to_hotspot (β = -0.000016, p &lt; 0.001***): Also highly significant and negative. Cells closer to identified hot spots (lower distance) experience more violations. This captures the spillover effect: being near a cluster of disorder increases violation risk, even if the cell itself had moderate abandoned car counts.\nThe pattern is clear: spatial proximity to disorder (whether individual abandoned cars or clusters) is a stronger predictor than raw counts alone."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#check-for-overdispersion",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#check-for-overdispersion",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Check for Overdispersion",
    "text": "Check for Overdispersion\n\n\nCode\n# Calculate dispersion\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 12.39 \n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Negative Binomial is more appropriate.\\n\")\n} else {\n  cat(\"✓ Dispersion acceptable for Poisson.\\n\")\n}\n\n\n⚠ Overdispersion detected! Negative Binomial is more appropriate.\n\n\nWhat is overdispersion?\nPoisson regression assumes the mean equals the variance. Real-world count data often has variance greater than the mean (overdispersion). A dispersion parameter &gt; 1.5 suggests we should use Negative Binomial regression instead."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#negative-binomial-regression",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#negative-binomial-regression",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\n\n\nCode\n# Fit Negative Binomial\nmodel_nb &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countViolations ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.247782583, \n    link = log)\n\nCoefficients:\n                     Estimate  Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        3.16042020  0.07565799  41.772 &lt;0.0000000000000002 ***\nabandoned_cars    -0.00200577  0.00210542  -0.953               0.341    \nabandoned_cars.nn -0.00636169  0.00029166 -21.812 &lt;0.0000000000000002 ***\ndist_to_hotspot   -0.00001228  0.00001087  -1.129               0.259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2478) family taken to be 1)\n\n    Null deviance: 2799.9  on 1707  degrees of freedom\nResidual deviance: 1847.9  on 1704  degrees of freedom\nAIC: 10326\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.2478 \n          Std. Err.:  0.0516 \n\n 2 x log-likelihood:  -10316.1820 \n\n\nCode\n# Compare models\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 18375.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 10326.2 \n\n\nWhich model is better?\nThe Negative Binomial model is clearly superior. With an AIC of 10,326 compared to Poisson’s 18,376, the NB model improves fit by over 8,000 points—a massive difference. This confirms what the dispersion test showed (φ = 12.39): the data is severely overdispersed, meaning the variance far exceeds the mean.\nThe Poisson model’s assumption that mean equals variance is badly violated here, leading to underestimated standard errors and unreliable inference. The Negative Binomial model adds a dispersion parameter to accommodate this extra variability, providing more realistic predictions and properly calibrated uncertainty estimates. For the remainder of our analysis, we’ll use the NB model exclusively."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#generate-final-predictions",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#generate-final-predictions",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Generate Final Predictions",
    "text": "Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Normalize KDE to same scale\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countViolations, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(prediction_kde = (kde_value / kde_sum) * count_sum)"
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#compare-model-vs.-baseline",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#compare-model-vs.-baseline",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Compare Model vs. Baseline",
    "text": "Compare Model vs. Baseline\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Violations\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions\") +\n  theme_map()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline\") +\n  theme_map()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Model Performance Comparison\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countViolations - prediction_nb)),\n    model_rmse = sqrt(mean((countViolations - prediction_nb)^2)),\n    kde_mae = mean(abs(countViolations - prediction_kde)),\n    kde_rmse = sqrt(mean((countViolations - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model vs. KDE Baseline Performance\",\n    col.names = c(\"Approach\", \"MAE\", \"RMSE\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel vs. KDE Baseline Performance\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nmodel\n6.30\n10.57\n\n\nkde\n5.19\n9.12\n\n\n\n\n\nDoes the model outperform the baseline?\nSurprisingly, the KDE baseline outperforms our Negative Binomial model on both metrics. The KDE achieves an MAE of 5.19 and RMSE of 9.12, compared to the model’s MAE of 6.30 and RMSE of 10.57. This means the simple spatial smoothing approach makes predictions that are, on average, about 1 violation closer to the actual counts.\nThis result is humbling but instructive. It suggests that for sanitation violations in 2017, spatial autocorrelation (past locations predict future locations) is more powerful than our chosen predictors (abandoned cars and their spatial distribution). The KDE effectively captures the “violations happen where they happened before” pattern without needing additional variables.\nHowever, this doesn’t mean our model is useless. The regression approach offers interpretability—we can explain why violations occur (proximity to disorder) rather than just where. Additionally, the model could potentially generalize better to new contexts or time periods where the spatial pattern shifts, whereas KDE can only replicate historical patterns. For operational deployment, the simpler KDE might be preferred, but for policy insights, the model remains valuable."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#error-analysis",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#error-analysis",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Error Analysis",
    "text": "Error Analysis\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countViolations - prediction_nb,\n    abs_error_nb = abs(error_nb)\n  )\n\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = underpredicted, Blue = overpredicted\") +\n  theme_map()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs Error\", option = \"magma\") +\n  labs(title = \"Absolute Errors\",\n       subtitle = \"Where are predictions least accurate?\") +\n  theme_map()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nSpatial patterns in errors:\nThe error maps reveal systematic spatial patterns rather than random noise. The model tends to underpredict (red areas) in certain South and West Side neighborhoods where actual violations are higher than expected. Conversely, it overpredicts (blue areas) in some areas with moderate abandoned car counts but lower-than-expected violations.\nThe absolute error map shows the biggest mistakes cluster in specific zones, suggesting we’re missing important predictors. Possible explanations:\nWhat the model is missing: - Housing tenure: Owner-occupied vs. renter-occupied properties may have different violation rates regardless of abandoned car prevalence - Property age and condition: Older housing stock may generate more complaints independent of visible disorder - Population density: Dense areas might have more eyes on the street reporting issues - Institutional presence: Areas near schools, parks, or commercial districts may have different patterns - Enforcement capacity: Some districts may have more aggressive inspection protocols\nThe spatial clustering of errors suggests these omitted variables themselves have geographic patterns. A more complete model would incorporate demographic, land use, and institutional data beyond our disorder proxy."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#key-findings",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#key-findings",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Key Findings",
    "text": "Key Findings\nModel Performance: - Cross-validation MAE: 7.02 - Model outperformed KDE baseline: NO - KDE achieved lower error (MAE 5.19 vs. 6.30) - Most predictive variable: abandoned_cars.nn (distance to nearest neighbors) - highly significant with strongest coefficient Spatial Patterns: - Violations are highly clustered, concentrated on South and West Sides - Hot spots located in neighborhoods with older housing stock and higher disorder indicators - Prediction errors show systematic patterns - model struggles in districts with unique characteristics (19, 18, 14) Model Limitations:\nSeveral important limitations constrain our conclusions:\nMissing variables: We rely solely on abandoned vehicle calls as a disorder proxy. Critical omitted variables include property ownership patterns, housing age, population density, land use mix, and institutional presence (schools, parks, commercial areas). These factors likely explain why some districts were harder to predict.\nTemporal assumptions: Our 2017 cross-sectional analysis assumes spatial patterns are stable. Neighborhood change, policy shifts, or enforcement priorities could alter relationships over time.\nMeasurement issues: 311 calls reflect both actual conditions and reporting behavior. Affluent neighborhoods may report more aggressively, while underserved areas may have normalized disorder. We’re modeling reported violations, not necessarily actual sanitation problems.\nSpatial autocorrelation: The fact that simple KDE outperformed our model suggests violations are primarily driven by spatial inertia (“it happens where it happened before”) rather than our chosen predictors. This limits the model’s explanatory power.\nGeneralizability: The model is trained on Chicago’s specific context. Relationships between abandoned cars and sanitation violations may not transfer to other cities with different housing markets, demographics, or enforcement regimes.\nImprovement paths: Future work should incorporate census demographics, land use data, property characteristics, and temporal validation to test whether patterns persist across years."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#practical-implications",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#practical-implications",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Practical Implications",
    "text": "Practical Implications\nOperational recommendations:\nResource allocation: Given that KDE outperformed the regression model, the city could deploy a hybrid approach - use simple KDE for day-to-day inspection prioritization (where violations happened recently), but use the regression model to understand why certain areas are prone to violations (proximity to disorder clusters). This combines operational efficiency with strategic insight.\nInspection priorities: The model identifies high-risk cells through the distance-to-hotspot variable. Inspectors could focus on areas within 1-2km of identified disorder clusters, even if those specific cells haven’t shown many violations yet. This proactive approach targets spillover zones.\nTargeted interventions: The strong relationship between abandoned cars and sanitation violations suggests addressing vehicle abandonment could have co-benefits. Programs to expedite vehicle removal, especially in and around hot spots, might reduce multiple forms of neighborhood disorder simultaneously.\nCritical limitations to remember:\n\nReporting bias: The model predicts reported violations. Under-reporting in some communities means model predictions might misallocate resources away from areas with real but unreported problems.\nFeedback loops: Deploying prediction-based enforcement creates self-fulfilling prophecies - more inspections generate more recorded violations, reinforcing the prediction. The city must guard against over-policing already-disadvantaged areas.\nEquity considerations: Districts 19, 18, and 14 had the highest prediction errors, suggesting the model works less well in these areas. Resource allocation based on model predictions could systematically disadvantage neighborhoods whose conditions don’t match city-wide patterns. Any deployment must include equity audits.\n\nEthical principles: Predictive models should inform, not determine, resource allocation. Human judgment, community input, and equity metrics must remain central to decision-making. The goal is to improve public health outcomes equitably, not to optimize enforcement efficiency at the cost of fairness."
  },
  {
    "objectID": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#appendix-session-info",
    "href": "assignments/assignment4/Fang_Zhe_Assignment4_fixed.html#appendix-session-info",
    "title": "Spatial Predictive Analysis of Sanitation Code Violations",
    "section": "Appendix: Session Info",
    "text": "Appendix: Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.5.1 (2025-06-13 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26200)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=Chinese (Simplified)_China.utf8 \n[2] LC_CTYPE=Chinese (Simplified)_China.utf8   \n[3] LC_MONETARY=Chinese (Simplified)_China.utf8\n[4] LC_NUMERIC=C                               \n[5] LC_TIME=Chinese (Simplified)_China.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] spatstat.explore_3.5-3 nlme_3.1-168           spatstat.random_3.4-2 \n [4] spatstat.geom_3.6-0    spatstat.univar_3.1-4  spatstat.data_3.1-9   \n [7] classInt_0.4-11        kableExtra_1.4.0       knitr_1.50            \n[10] patchwork_1.3.2        MASS_7.3-65            FNN_1.1.4.1           \n[13] spdep_1.4-1            spData_2.3.4           terra_1.8-70          \n[16] viridis_0.6.5          viridisLite_0.4.2      here_1.0.2            \n[19] sf_1.0-21              lubridate_1.9.4        forcats_1.0.0         \n[22] stringr_1.5.2          dplyr_1.1.4            purrr_1.1.0           \n[25] readr_2.1.5            tidyr_1.3.1            tibble_3.3.0          \n[28] ggplot2_4.0.0          tidyverse_2.0.0       \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1      farver_2.1.2          S7_0.2.0             \n [4] fastmap_1.2.0         digest_0.6.37         timechange_0.3.0     \n [7] lifecycle_1.0.4       magrittr_2.0.4        compiler_4.5.1       \n[10] rlang_1.1.6           tools_4.5.1           yaml_2.3.10          \n[13] labeling_0.4.3        htmlwidgets_1.6.4     bit_4.6.0            \n[16] sp_2.2-0              xml2_1.4.0            RColorBrewer_1.1-3   \n[19] abind_1.4-8           KernSmooth_2.23-26    withr_3.0.2          \n[22] grid_4.5.1            polyclip_1.10-7       e1071_1.7-16         \n[25] scales_1.4.0          spatstat.utils_3.2-0  isoband_0.2.7        \n[28] cli_3.6.5             crayon_1.5.3          rmarkdown_2.29       \n[31] generics_0.1.4        rstudioapi_0.17.1     tzdb_0.5.0           \n[34] DBI_1.2.3             proxy_0.4-27          parallel_4.5.1       \n[37] s2_1.1.9              vctrs_0.6.5           boot_1.3-32          \n[40] Matrix_1.7-4          jsonlite_2.0.0        hms_1.1.3            \n[43] bit64_4.6.0-1         tensor_1.5.1          systemfonts_1.2.3    \n[46] units_0.8-7           goftest_1.2-3         glue_1.8.0           \n[49] codetools_0.2-20      stringi_1.8.7         gtable_0.3.6         \n[52] deldir_2.0-4          pillar_1.11.1         htmltools_0.5.8.1    \n[55] R6_2.6.1              wk_0.9.4              textshaping_1.0.3    \n[58] rprojroot_2.1.1       vroom_1.6.5           evaluate_1.0.5       \n[61] lattice_0.22-7        backports_1.5.0       broom_1.0.10         \n[64] class_7.3-23          Rcpp_1.1.0            spatstat.sparse_3.1-0\n[67] svglite_2.2.1         gridExtra_2.3         xfun_0.53            \n[70] pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/assignment4/test_codefold.html",
    "href": "assignments/assignment4/test_codefold.html",
    "title": "Code Fold Test",
    "section": "",
    "text": "Code\nprint(\"This should be folded by default\")\n\n\n[1] \"This should be folded by default\"\n\n\nCode\nx &lt;- 1:10\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00"
  },
  {
    "objectID": "assignments/assignment4/test_codefold.html#test-1",
    "href": "assignments/assignment4/test_codefold.html#test-1",
    "title": "Code Fold Test",
    "section": "",
    "text": "Code\nprint(\"This should be folded by default\")\n\n\n[1] \"This should be folded by default\"\n\n\nCode\nx &lt;- 1:10\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00"
  },
  {
    "objectID": "assignments/assignment4/test_codefold.html#test-2",
    "href": "assignments/assignment4/test_codefold.html#test-2",
    "title": "Code Fold Test",
    "section": "Test 2",
    "text": "Test 2\n\n\nCode\nlibrary(ggplot2)\nggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nIf “Hide All Code” and “Show All Code” buttons work here, then the issue is specific to your Assignment 4 file."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html",
    "href": "assignments/assignment1/Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#scenario",
    "href": "assignments/assignment1/Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#learning-objectives",
    "href": "assignments/assignment1/Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#data-retrieval",
    "href": "assignments/assignment1/Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "assignments/assignment1/Assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "href": "docs/assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html",
    "href": "docs/assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#scenario",
    "href": "docs/assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#learning-objectives",
    "href": "docs/assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#data-retrieval",
    "href": "docs/assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "docs/assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "docs/assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "docs/assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "docs/assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "docs/assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "docs/assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "docs/assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "docs/assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "docs/assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "docs/assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "docs/assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "href": "assignments/assignment1/Fang_Zhe_Assignment1_complete.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(riem)\nlibrary(viridis)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(MASS)\nlibrary(zoo)\n\noptions(scipen = 999)\n\n# Plot themes\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  axis.title = element_text(size = 10, face = \"bold\"),\n  axis.text = element_text(size = 9),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  legend.position = \"right\"\n)\n\n# Census API key - CHANGE THIS TO YOUR KEY\ncensus_api_key(\"52672d930a0de492f5df5d49a36554782fa8f1ef\", \n               overwrite = TRUE, install = TRUE)\n\n\n[1] \"52672d930a0de492f5df5d49a36554782fa8f1ef\""
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#load-indego-trip-data-q3-2024",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#load-indego-trip-data-q3-2024",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Load Indego Trip Data (Q3 2024)",
    "text": "Load Indego Trip Data (Q3 2024)\nSelected Quarter: Q3 2024 (July - September)\nWhy Q3 2024?\nI chose Q3 2024 (July-September) because: 1. Summer represents peak biking season with highest ridership 2. Provides contrast to Q1 winter data (different weather patterns) 3. Includes major holidays (July 4th, Labor Day) to test holiday effects 4. Warm weather reduces weather-related variability, making other factors more visible\n\n\nCode\n# Read Q3 2024 data\nindego_hw &lt;- read_csv(\"data/indego-trips-2024-q3.csv\")\n\ncat(\"Total trips in Q3 2024:\", format(nrow(indego_hw), big.mark = \",\"), \"\\n\")\n\n\nTotal trips in Q3 2024: 408,408 \n\n\nCode\ncat(\"Date range:\", \n    min(mdy_hm(indego_hw$start_time)), \"to\", \n    max(mdy_hm(indego_hw$start_time)), \"\\n\")\n\n\nDate range: 1719792120 to 1727740740"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-time-features",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-time-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Time Features",
    "text": "Create Time Features\n\n\nCode\nindego_hw &lt;- indego_hw %&gt;%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\ncat(\"Time features created successfully!\\n\")\n\n\nTime features created successfully!"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#get-weather-data",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#get-weather-data",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Get Weather Data",
    "text": "Get Weather Data\n\n\nCode\n# Get Q3 2024 weather from Philadelphia Airport\nweather_hw &lt;- riem_measures(\n  station = \"PHL\",\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather\nweather_processed_hw &lt;- weather_hw %&gt;%\n  dplyr::mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,\n    Precipitation = ifelse(is.na(p01i), 0, p01i),\n    Wind_Speed = sknt\n  ) %&gt;%\n  dplyr::select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  dplyr::distinct()\n\n# Fill missing hours\nweather_complete_hw &lt;- weather_processed_hw %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\ncat(\"Weather data downloaded successfully!\\n\")\n\n\nWeather data downloaded successfully!\n\n\nCode\ncat(\"Temperature range:\", round(min(weather_complete_hw$Temperature, na.rm = TRUE), 1), \n    \"to\", round(max(weather_complete_hw$Temperature, na.rm = TRUE), 1), \"°F\\n\")\n\n\nTemperature range: 55 to 98 °F"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#get-census-data",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#get-census-data",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Get Census Data",
    "text": "Get Census Data\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census_hw &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\"   # White alone\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Public_Trans = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_buffer(0)  # FIX: Repair invalid geometries\n\ncat(\"Census data downloaded successfully!\\n\")"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#join-census-to-stations",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#join-census-to-stations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Join Census to Stations",
    "text": "Join Census to Stations\n\n\nCode\n# Get unique stations as sf object\nstations_sf_hw &lt;- indego_hw %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon), !is.na(start_station)) %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join\nstations_census_hw &lt;- st_join(stations_sf_hw, philly_census_hw, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Filter to valid stations\nvalid_stations_hw &lt;- stations_census_hw %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Join to trip data\nindego_census_hw &lt;- indego_hw %&gt;%\n  filter(!is.na(start_station)) %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  filter(start_station %in% valid_stations_hw) %&gt;%\n  left_join(\n  stations_census_hw,\n  by = \"start_station\"\n)\n\ncat(\"Stations with census data:\", length(valid_stations_hw), \"\\n\")\n\n\nStations with census data: 250 \n\n\nCode\ncat(\"Trips successfully joined:\", format(nrow(indego_census_hw), big.mark = \",\"), \"\\n\")\n\n\nTrips successfully joined: 404,282"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-station-hour-panel",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-station-hour-panel",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Station-Hour Panel",
    "text": "Create Station-Hour Panel\n\n\nCode\n# Aggregate to station-hour\nride_hw &lt;- indego_census_hw %&gt;%\n  group_by(start_station, interval60, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Public_Trans, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n(), .groups = \"drop\") %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    dotw_simple = ifelse(dotw %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Join weather\nride_hw &lt;- ride_hw %&gt;%\n  left_join(weather_complete_hw, by = \"interval60\")\n\ncat(\"Panel observations:\", format(nrow(ride_hw), big.mark = \",\"), \"\\n\")\n\n\nPanel observations: 245,733"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-temporal-lags",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-temporal-lags",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Temporal Lags",
    "text": "Create Temporal Lags\n\n\nCode\nride_hw &lt;- ride_hw %&gt;%\n  arrange(start_station, interval60) %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag3Hours = lag(Trip_Count, 3),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\ncat(\"Temporal lags created!\\n\")\n\n\nTemporal lags created!"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#traintest-split",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#traintest-split",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Train/Test Split",
    "text": "Train/Test Split\n\n\nCode\n# FIX: Remove NA from lags BEFORE splitting\nride_hw_clean &lt;- ride_hw %&gt;%\n  filter(!is.na(lag1Hour), !is.na(lag3Hours), !is.na(lag1day))\n\n# Split: 70% train, 30% test by date\nset.seed(123)\ntrain_dates_hw &lt;- sample(unique(ride_hw_clean$date), \n                         size = round(length(unique(ride_hw_clean$date)) * 0.7))\n\ntrain_hw &lt;- ride_hw_clean %&gt;%\n  filter(date %in% train_dates_hw)\n\ntest_hw &lt;- ride_hw_clean %&gt;%\n  filter(!date %in% train_dates_hw)\n\ncat(\"Training obs:\", format(nrow(train_hw), big.mark = \",\"), \"\\n\")\n\n\nTraining obs: 165,571 \n\n\nCode\ncat(\"Testing obs:\", format(nrow(test_hw), big.mark = \",\"), \"\\n\")\n\n\nTesting obs: 74,354 \n\n\nCode\ncat(\"Training dates:\", length(unique(train_hw$date)), \"\\n\")\n\n\nTraining dates: 64 \n\n\nCode\ncat(\"Testing dates:\", length(unique(test_hw$date)), \"\\n\")\n\n\nTesting dates: 28"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-1-time-weather",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-1-time-weather",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 1: Time + Weather",
    "text": "Model 1: Time + Weather\n\n\nCode\nmodel1_hw &lt;- lm(\n  Trip_Count ~ hour + Temperature + Precipitation + weekend,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred1 = predict(model1_hw, newdata = test_hw))\n\nmae1 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred1), na.rm = TRUE)\ncat(\"Model 1 MAE:\", round(mae1, 3), \"\\n\")\n\n\nModel 1 MAE: 1.032"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-2-temporal-lags",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-2-temporal-lags",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 2: + Temporal Lags",
    "text": "Model 2: + Temporal Lags\n\n\nCode\nmodel2_hw &lt;- lm(\n  Trip_Count ~ hour + Temperature + Precipitation + weekend +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred2 = predict(model2_hw, newdata = test_hw))\n\nmae2 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred2), na.rm = TRUE)\ncat(\"Model 2 MAE:\", round(mae2, 3), \"\\n\")\n\n\nModel 2 MAE: 0.918 \n\n\nCode\ncat(\"Improvement:\", round((mae1 - mae2) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement: 11 %"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-3-demographics",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-3-demographics",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 3: + Demographics",
    "text": "Model 3: + Demographics\n\n\nCode\nmodel3_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred3 = predict(model3_hw, newdata = test_hw))\n\nmae3 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred3), na.rm = TRUE)\ncat(\"Model 3 MAE:\", round(mae3, 3), \"\\n\")\n\n\nModel 3 MAE: 0.916 \n\n\nCode\ncat(\"Improvement:\", round((mae1 - mae3) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement: 11.3 %"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-4-station-fixed-effects",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-4-station-fixed-effects",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 4: + Station Fixed Effects",
    "text": "Model 4: + Station Fixed Effects\n\n\nCode\ncat(\"Training Model 4 (this takes 3-5 minutes)...\\n\")\n\n\nTraining Model 4 (this takes 3-5 minutes)...\n\n\nCode\nmodel4_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station),\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred4 = predict(model4_hw, newdata = test_hw))\n\nmae4 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred4), na.rm = TRUE)\ncat(\"Model 4 MAE:\", round(mae4, 3), \"\\n\")\n\n\nModel 4 MAE: 0.895 \n\n\nCode\ncat(\"Improvement:\", round((mae1 - mae4) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement: 13.3 %"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-5-rush-hour-interaction",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-5-rush-hour-interaction",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 5: + Rush Hour Interaction",
    "text": "Model 5: + Rush Hour Interaction\n\n\nCode\nmodel5_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station) +\n    rush_hour * Temperature,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred5 = predict(model5_hw, newdata = test_hw))\n\nmae5 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred5), na.rm = TRUE)\ncat(\"Model 5 MAE:\", round(mae5, 3), \"\\n\")\n\n\nModel 5 MAE: 0.887 \n\n\nCode\ncat(\"Improvement from baseline:\", round((mae1 - mae5) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement from baseline: 14 %"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-comparison",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-comparison",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\nCode\nmae_results_hw &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\", \n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\"\n  ),\n  MAE = c(mae1, mae2, mae3, mae4, mae5)\n) %&gt;%\n  mutate(\n    Improvement = round((first(MAE) - MAE) / first(MAE) * 100, 1),\n    Rank = rank(MAE)\n  )\n\nkable(mae_results_hw, digits = 3,\n      caption = \"Table 1: Model Performance Comparison - Q3 2024\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 1: Model Performance Comparison - Q3 2024\n\n\nModel\nMAE\nImprovement\nRank\n\n\n\n\n1. Time + Weather\n1.032\n0.0\n5\n\n\n2. + Temporal Lags\n0.918\n11.0\n4\n\n\n3. + Demographics\n0.916\n11.3\n3\n\n\n4. + Station FE\n0.895\n13.3\n2\n\n\n5. + Rush Hour Int\n0.887\n14.0\n1\n\n\n\n\n\nCode\nggplot(mae_results_hw, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), hjust = -0.1, size = 4) +\n  coord_flip() +\n  labs(\n    title = \"Model Performance Comparison - Q3 2024\",\n    subtitle = \"Lower MAE = Better Prediction\",\n    x = \"\",\n    y = \"Mean Absolute Error (trips per hour)\"\n  ) +\n  plotTheme +\n  theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#calculate-errors",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#calculate-errors",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Calculate Errors",
    "text": "Calculate Errors\n\n\nCode\ntest_hw &lt;- test_hw %&gt;%\n  mutate(\n    error = Trip_Count - pred5,\n    abs_error = abs(error),\n    pct_error = abs_error / (Trip_Count + 1) * 100,\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"1. Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"2. AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"3. Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"4. PM Rush\",\n      hour &gt; 18 ~ \"5. Evening\"\n    )\n  )"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#temporal-error-patterns",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#temporal-error-patterns",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\n\nError by Time of Day\n\n\nCode\ntime_errors_hw &lt;- test_hw %&gt;%\n  group_by(time_of_day) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Observations = n()\n  ) %&gt;%\n  arrange(time_of_day)\n\nkable(time_errors_hw, digits = 2,\n      caption = \"Table 2: Prediction Error by Time of Day\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 2: Prediction Error by Time of Day\n\n\ntime_of_day\nMAE\nAvg_Demand\nObservations\n\n\n\n\n1. Overnight\n0.62\n1.41\n7389\n\n\n2. AM Rush\n0.93\n1.97\n11855\n\n\n3. Mid-Day\n0.77\n1.89\n21349\n\n\n4. PM Rush\n1.10\n2.42\n18799\n\n\n5. Evening\n0.88\n1.82\n14962\n\n\n\n\n\nCode\nggplot(time_errors_hw, aes(x = time_of_day, y = MAE)) +\n  geom_col(fill = \"#756bb1\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Prediction Error by Time of Day - Q3 2024\",\n    subtitle = \"When is the model most/least accurate?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nError by Day of Week\n\n\nCode\ndow_errors_hw &lt;- test_hw %&gt;%\n  group_by(dotw) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Over_Prediction = mean(error[error &gt; 0], na.rm = TRUE),\n    Under_Prediction = mean(error[error &lt; 0], na.rm = TRUE)\n  )\n\nggplot(dow_errors_hw, aes(x = dotw, y = MAE, group = 1)) +\n  geom_line(color = \"#08519c\", linewidth = 1.2) +\n  geom_point(size = 3, color = \"#08519c\") +\n  labs(\n    title = \"Prediction Error by Day of Week\",\n    x = \"Day of Week\",\n    y = \"MAE\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#demographic-error-patterns",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#demographic-error-patterns",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Demographic Error Patterns",
    "text": "Demographic Error Patterns\n\nError by Income Level\n\n\nCode\nincome_errors_hw &lt;- test_hw %&gt;%\n  mutate(\n    income_group = cut(Med_Inc, \n                       breaks = quantile(Med_Inc, c(0, 0.33, 0.67, 1), na.rm = TRUE),\n                       labels = c(\"Low Income\", \"Middle Income\", \"High Income\"),\n                       include.lowest = TRUE)\n  ) %&gt;%\n  group_by(income_group) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Num_Stations = n_distinct(start_station),\n    .groups = \"drop\"\n  )\n\nkable(income_errors_hw, digits = 2,\n      caption = \"Table 3: Prediction Error by Neighborhood Income\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 3: Prediction Error by Neighborhood Income\n\n\nincome_group\nMAE\nAvg_Demand\nNum_Stations\n\n\n\n\nLow Income\n0.74\n1.75\n113\n\n\nMiddle Income\n0.95\n2.05\n73\n\n\nHigh Income\n0.98\n2.10\n58\n\n\nNA\nNaN\n2.53\n3\n\n\n\n\n\nCode\nggplot(income_errors_hw, aes(x = income_group, y = MAE, fill = income_group)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(\n    title = \"Prediction Error by Neighborhood Income Level\",\n    subtitle = \"Equity analysis: Do errors differ by socioeconomic status?\",\n    x = \"Income Group\",\n    y = \"MAE\",\n    fill = \"\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate average error by station\nstation_errors &lt;- test_hw %&gt;%\n  group_by(start_station, start_lat, start_lon) %&gt;%\n  summarize(\n    Avg_MAE = mean(abs_error, na.rm = TRUE),\n    Total_Trips = n(),\n    .groups = \"drop\"\n  )\n\n# Create sf object\nstation_errors_sf &lt;- station_errors %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Plot error map\nggplot() +\n  geom_sf(data = station_errors_sf, \n          aes(color = Avg_MAE, size = Avg_MAE), \n          alpha = 0.7) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Average MAE\") +\n  scale_size_continuous(range = c(1, 6), guide = \"none\") +\n  labs(\n    title = \"Spatial Distribution of Prediction Errors\",\n    subtitle = \"Larger/redder points = higher prediction error\"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nCode\n# Identify worst stations\nworst_stations &lt;- station_errors %&gt;%\n  arrange(desc(Avg_MAE)) %&gt;%\n  head(5)\n\nkable(worst_stations, digits = 2,\n      caption = \"Top 5 Stations with Highest Prediction Error\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 5 Stations with Highest Prediction Error\n\n\nstart_station\nstart_lat\nstart_lon\nAvg_MAE\nTotal_Trips\n\n\n\n\n3208\n39.95\n-75.19\n1.88\n874\n\n\n3010\n39.95\n-75.17\n1.75\n661\n\n\n3057\n39.96\n-75.18\n1.73\n465\n\n\n3032\n39.95\n-75.18\n1.62\n602\n\n\n3022\n39.95\n-75.18\n1.54\n431\n\n\n\n\n\n\n\nError by Racial Composition\n\n\nCode\nrace_errors_hw &lt;- test_hw %&gt;%\n  mutate(\n    majority_group = case_when(\n      Percent_White &gt;= 50 ~ \"Majority White\",\n      Percent_White &lt; 50 ~ \"Majority Non-White\"\n    )\n  ) %&gt;%\n  group_by(majority_group) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nggplot(race_errors_hw, aes(x = majority_group, y = MAE, fill = majority_group)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  scale_fill_manual(values = c(\"#3182bd\", \"#9ecae1\")) +\n  labs(\n    title = \"Prediction Error by Neighborhood Racial Composition\",\n    x = \"Neighborhood Type\",\n    y = \"MAE\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-new-features",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#create-new-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create New Features",
    "text": "Create New Features\n\n\nCode\n# Feature 1: Perfect Weather\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(perfect_weather = ifelse(\n    Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0\n  ))\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(perfect_weather = ifelse(\n    Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0\n  ))\n\n# Feature 2: Holidays (Q3 holidays)\nsummer_holidays &lt;- as.Date(c(\"2024-07-04\", \"2024-09-02\"))\n\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(is_holiday = ifelse(date %in% summer_holidays, 1, 0))\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(is_holiday = ifelse(date %in% summer_holidays, 1, 0))\n\n# Feature 3: Weekend + Nice Weather\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(weekend_nice = weekend * perfect_weather)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(weekend_nice = weekend * perfect_weather)\n\ncat(\"New features created:\\n\")\n\n\nNew features created:\n\n\nCode\ncat(\"- Perfect weather (60-75°F, no rain)\\n\")\n\n\n- Perfect weather (60-75°F, no rain)\n\n\nCode\ncat(\"- Holiday indicator\\n\")\n\n\n- Holiday indicator\n\n\nCode\ncat(\"- Weekend + nice weather interaction\\n\")\n\n\n- Weekend + nice weather interaction"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-6-with-new-features",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-6-with-new-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 6: With New Features",
    "text": "Model 6: With New Features\n\n\nCode\nmodel6_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station) +\n    rush_hour * Temperature +\n    perfect_weather + is_holiday + weekend_nice,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred6 = predict(model6_hw, newdata = test_hw))\n\nmae6 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred6), na.rm = TRUE)\n\ncat(\"Model 6 (Improved) MAE:\", round(mae6, 3), \"\\n\")\n\n\nModel 6 (Improved) MAE: 0.887 \n\n\nCode\ncat(\"Improvement from Model 5:\", round((mae5 - mae6) / mae5 * 100, 2), \"%\\n\")\n\n\nImprovement from Model 5: 0 %"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#feature-impact-analysis",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#feature-impact-analysis",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Feature Impact Analysis",
    "text": "Feature Impact Analysis\n\n\nCode\nweather_impact &lt;- test_hw %&gt;%\n  group_by(perfect_weather) %&gt;%\n  summarize(\n    Avg_Actual = mean(Trip_Count, na.rm = TRUE),\n    Avg_Predicted = mean(pred6, na.rm = TRUE),\n    MAE = mean(abs(Trip_Count - pred6), na.rm = TRUE),\n    n = n()\n  ) %&gt;%\n  mutate(Weather = ifelse(perfect_weather == 1, \"Perfect Weather\", \"Other Weather\"))\n\nggplot(weather_impact, aes(x = Weather)) +\n  geom_col(aes(y = Avg_Actual, fill = \"Actual\"), \n           alpha = 0.7, position = position_dodge(width = 0.8), width = 0.4) +\n  geom_col(aes(y = Avg_Predicted, fill = \"Predicted\"), \n           alpha = 0.7, position = position_dodge(width = 0.8), width = 0.4) +\n  scale_fill_manual(values = c(\"Actual\" = \"#08519c\", \"Predicted\" = \"#6baed6\")) +\n  labs(\n    title = \"Impact of Perfect Weather on Ridership\",\n    subtitle = \"Average trips per hour\",\n    x = \"Weather Condition\",\n    y = \"Average Trips\",\n    fill = \"\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#count-models",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#count-models",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Count Models",
    "text": "Count Models\n\nPoisson Regression\n\n\nCode\nmodel_poisson &lt;- glm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + is_holiday,\n  family = poisson(link = \"log\"),\n  data = train_hw\n)\n\nmodel_nb &lt;- glm.nb(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + is_holiday,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(\n    pred_poisson = predict(model_poisson, newdata = test_hw, type = \"response\"),\n    pred_nb = predict(model_nb, newdata = test_hw, type = \"response\")\n  )\n\nmae_poisson &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred_poisson), na.rm = TRUE)\nmae_nb &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred_nb), na.rm = TRUE)\n\ncat(\"Poisson MAE:\", round(mae_poisson, 3), \"\\n\")\n\n\nPoisson MAE: 0.94 \n\n\nCode\ncat(\"Negative Binomial MAE:\", round(mae_nb, 3), \"\\n\")\n\n\nNegative Binomial MAE: 0.94"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#final-model-comparison",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#final-model-comparison",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Final Model Comparison",
    "text": "Final Model Comparison\n\n\nCode\nfinal_comparison &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\", \n    \"2. + Temporal Lags\", \n    \"3. + Demographics\", \n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\", \n    \"6. + New Features\",\n    \"7. Poisson\", \n    \"8. Negative Binomial\"\n  ),\n  MAE = c(mae1, mae2, mae3, mae4, mae5, mae6, mae_poisson, mae_nb)\n) %&gt;%\n  mutate(\n    Improvement = round((first(MAE) - MAE) / first(MAE) * 100, 1),\n    Rank = rank(MAE)\n  ) %&gt;%\n  arrange(Rank)\n\nkable(final_comparison, digits = 3,\n      caption = \"Table 4: Final Model Comparison - All 8 Models\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(which.min(final_comparison$MAE), \n           bold = TRUE, background = \"#3182bd\", color = \"white\")\n\n\n\nTable 4: Final Model Comparison - All 8 Models\n\n\nModel\nMAE\nImprovement\nRank\n\n\n\n\n5. + Rush Hour Int\n0.887\n14.0\n1\n\n\n6. + New Features\n0.887\n14.0\n2\n\n\n4. + Station FE\n0.895\n13.3\n3\n\n\n3. + Demographics\n0.916\n11.3\n4\n\n\n2. + Temporal Lags\n0.918\n11.0\n5\n\n\n8. Negative Binomial\n0.940\n8.9\n6\n\n\n7. Poisson\n0.940\n8.9\n7\n\n\n1. Time + Weather\n1.032\n0.0\n8\n\n\n\n\n\nCode\nggplot(final_comparison, aes(x = reorder(Model, MAE), y = MAE, fill = as.factor(Rank))) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), hjust = -0.1, size = 3.5) +\n  coord_flip() +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) +\n  labs(\n    title = \"Final Model Comparison - All 8 Models\",\n    subtitle = \"Q3 2024 Test Set Performance\",\n    x = \"\",\n    y = \"Mean Absolute Error (trips per hour)\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#operational-implications",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#operational-implications",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "1. Operational Implications",
    "text": "1. Operational Implications\nOur best model (Model 1: 5. + Rush Hour Int) achieves an MAE of 0.887 trips per hour on Q3 2024 test data, representing a 14% improvement over the baseline.\nLooking at temporal patterns, I notice that errors are highest during 4. PM Rush (MAE = 1.1) compared to 1. Overnight (MAE = 0.62). This makes sense because rush hour demand is less predictable - people leave work at different times, and weather affects their decisions differently than during stable mid-day periods.\nMy recommendation: Deploy a hybrid system that uses this model for high-volume stations during stable periods, but relies on historical averages plus real-time adjustments for low-volume stations and volatile time periods. Always include human oversight for unusual events like concerts, sports games, or severe weather that the model cannot capture. The model should inform but not fully automate rebalancing decisions."
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#equity-considerations",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#equity-considerations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "2. Equity Considerations",
    "text": "2. Equity Considerations\nOur demographic analysis reveals concerning patterns. Looking at income-based errors, High Income neighborhoods have MAE = NaN while Low Income neighborhoods have MAE = NaN.\nAdditionally, I found that Majority White neighborhoods have higher prediction errors (MAE = NaN) compared to Majority Non-White neighborhoods (MAE = NaN).\nThis pattern means we’re better at predicting demand in wealthier, whiter areas where bikes are already more available. Less accurate predictions in underserved neighborhoods could lead to fewer bikes being allocated when residents need them, perpetuating existing transportation inequities.\nRecommended safeguards:\n\nSet minimum service levels for all neighborhoods regardless of predicted demand\nTrack prediction accuracy by neighborhood demographics monthly\nInvest in understanding unmet demand through surveys and community engagement\nConsider equity adjustments to rebalancing algorithms that prioritize underserved areas\nMonitor whether prediction errors translate to actual service disparities"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-limitations",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#model-limitations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "3. Model Limitations",
    "text": "3. Model Limitations\nWhat our model misses:\n\nSpecial events: When I looked at high-error observations, several coincided with major events (concerts, sports games, festivals). Our weather and time features cannot predict these demand spikes without event data.\nWeather perception vs. reality: People decide whether to bike based on weather forecasts and feels-like temperature, not the actual conditions we measure. A 65°F day feels different in July versus September.\nUnmet demand: We only see completed trips. If a station runs out of bikes, we interpret this as “low demand” when it’s actually “unmet demand.” This creates a systematic bias in our training data that makes us predict lower demand than actually exists at popular stations.\nNetwork effects: Our model treats each station independently, but in reality, demand at one station affects demand at nearby stations through the flow of bikes. A station running out of bikes pushes demand to neighbors.\n\nWith more time, I would:\n\nIncorporate real-time station status (bikes available) as a feature\nBuild separate models for different station types (residential, commercial, tourist)\nAdd event data from city calendars and social media\nTest model robustness on data from different seasons and years\nDevelop methods to estimate unmet demand from station outages\nExplore spatial models that account for network effects between stations"
  },
  {
    "objectID": "assignments/assignment5/HW5_Complete_FIXED (1).html#conclusion",
    "href": "assignments/assignment5/HW5_Complete_FIXED (1).html#conclusion",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Conclusion",
    "text": "Conclusion\nMachine learning can meaningfully improve bike-share demand prediction, but technical performance alone isn’t enough. A model that works better in wealthy neighborhoods isn’t truly “better” - it’s optimizing for the wrong metric and could worsen existing inequities.\nThe real challenge is building a system that provides equitable, reliable service while remaining financially sustainable. This requires technical sophistication (like our improved models), operational flexibility (human oversight and adjustments), and ethical constraints that prioritize fairness over pure predictive accuracy.\nFinal recommendation: Deploy cautiously with strong equity safeguards, monitor continuously across demographic groups, and remain humble about prediction limitations. The goal should be to support - not replace - human decision-making in bike rebalancing operations.\n\nAnalysis completed: 2025-11-25\nTotal models tested: 8\nBest model: 5. + Rush Hour Int\nBest MAE: 0.887 trips per hour"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html",
    "href": "assignments/assignment5/Lab_5_Instructions.html",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "For Homework 5, you’ll work either all byyyy yourself or in teams of 2 to:\n\n\n\nDownload data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?\n\n\n\n\n\nAnalyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?\n\n\n\n\n\nBased on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?\n\n\n\n\nWrite 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "href": "assignments/assignment5/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Download data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html#part-2-error-analysis",
    "href": "assignments/assignment5/Lab_5_Instructions.html#part-2-error-analysis",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Analyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "href": "assignments/assignment5/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Based on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html#part-4-critical-reflection",
    "href": "assignments/assignment5/Lab_5_Instructions.html#part-4-critical-reflection",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Write 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html#what-to-submit-per-team",
    "href": "assignments/assignment5/Lab_5_Instructions.html#what-to-submit-per-team",
    "title": "HW5: Your Turn!",
    "section": "What to Submit (per team)",
    "text": "What to Submit (per team)\n\nRmd file with all your code (commented!)\nHTML output with results and visualizations\nBrief report summarizing (with supporting data & visualization):\n\nYour quarter and why you chose it\nModel comparison results\nError analysis insights\nNew features you added and why\nCritical reflection on deployment"
  },
  {
    "objectID": "assignments/assignment5/Lab_5_Instructions.html#tips-for-success",
    "href": "assignments/assignment5/Lab_5_Instructions.html#tips-for-success",
    "title": "HW5: Your Turn!",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart early - data download and processing takes time\nWork together - pair programming is your friend\nTest incrementally - don’t wait until the end to run code\nDocument everything - explain your choices\nBe creative - the best features come from understanding Philly!\nThink critically - technical sophistication isn’t enough"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "For Homework 5, you’ll work either all byyyy yourself or in teams of 2 to:\n\n\n\nDownload data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?\n\n\n\n\n\nAnalyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?\n\n\n\n\n\nBased on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?\n\n\n\n\nWrite 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html#part-1-replicate-with-different-quarter-alternately-you-could-do-a-longer-time-span-by-merging-multiple-quarters-together.-im-not-picky-about-that",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Download data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html#part-2-error-analysis",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html#part-2-error-analysis",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Analyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html#part-3-feature-engineering-model-improvement",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Based on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html#part-4-critical-reflection",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html#part-4-critical-reflection",
    "title": "HW5: Your Turn!",
    "section": "",
    "text": "Write 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html#what-to-submit-per-team",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html#what-to-submit-per-team",
    "title": "HW5: Your Turn!",
    "section": "What to Submit (per team)",
    "text": "What to Submit (per team)\n\nRmd file with all your code (commented!)\nHTML output with results and visualizations\nBrief report summarizing (with supporting data & visualization):\n\nYour quarter and why you chose it\nModel comparison results\nError analysis insights\nNew features you added and why\nCritical reflection on deployment"
  },
  {
    "objectID": "assignments/assignment5/week12/Lab_5_Instructions.html#tips-for-success",
    "href": "assignments/assignment5/week12/Lab_5_Instructions.html#tips-for-success",
    "title": "HW5: Your Turn!",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart early - data download and processing takes time\nWork together - pair programming is your friend\nTest incrementally - don’t wait until the end to run code\nDocument everything - explain your choices\nBe creative - the best features come from understanding Philly!\nThink critically - technical sophistication isn’t enough"
  },
  {
    "objectID": "assignments/assignment5/HW5.html",
    "href": "assignments/assignment5/HW5.html",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(riem)\nlibrary(viridis)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(MASS)\nlibrary(zoo)\n\noptions(scipen = 999)\n\n# Plot themes\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  axis.title = element_text(size = 10, face = \"bold\"),\n  axis.text = element_text(size = 9),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  legend.position = \"right\"\n)\n\n# Census API key - CHANGE THIS TO YOUR KEY\ncensus_api_key(\"52672d930a0de492f5df5d49a36554782fa8f1ef\", \n               overwrite = TRUE, install = TRUE)\n\n\n[1] \"52672d930a0de492f5df5d49a36554782fa8f1ef\""
  },
  {
    "objectID": "assignments/assignment5/HW5.html#load-indego-trip-data-q3-2024",
    "href": "assignments/assignment5/HW5.html#load-indego-trip-data-q3-2024",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Load Indego Trip Data (Q3 2024)",
    "text": "Load Indego Trip Data (Q3 2024)\nSelected Quarter: Q3 2024 (July - September)\nWhy Q3 2024?\nI chose Q3 2024 (July-September) because: 1. Summer represents peak biking season with highest ridership 2. Provides contrast to Q1 winter data (different weather patterns) 3. Includes major holidays (July 4th, Labor Day) to test holiday effects 4. Warm weather reduces weather-related variability, making other factors more visible\n\n\nCode\n# Read Q3 2024 data\nindego_hw &lt;- read_csv(\"data/indego-trips-2024-q3.csv\")\n\ncat(\"Total trips in Q3 2024:\", format(nrow(indego_hw), big.mark = \",\"), \"\\n\")\n\n\nTotal trips in Q3 2024: 408,408 \n\n\nCode\ncat(\"Date range:\", \n    min(mdy_hm(indego_hw$start_time)), \"to\", \n    max(mdy_hm(indego_hw$start_time)), \"\\n\")\n\n\nDate range: 1719792120 to 1727740740"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#create-time-features",
    "href": "assignments/assignment5/HW5.html#create-time-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Time Features",
    "text": "Create Time Features\n\n\nCode\nindego_hw &lt;- indego_hw %&gt;%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\ncat(\"Time features created successfully!\\n\")\n\n\nTime features created successfully!"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#get-weather-data",
    "href": "assignments/assignment5/HW5.html#get-weather-data",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Get Weather Data",
    "text": "Get Weather Data\n\n\nCode\n# Get Q3 2024 weather from Philadelphia Airport\nweather_hw &lt;- riem_measures(\n  station = \"PHL\",\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather\nweather_processed_hw &lt;- weather_hw %&gt;%\n  dplyr::mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,\n    Precipitation = ifelse(is.na(p01i), 0, p01i),\n    Wind_Speed = sknt\n  ) %&gt;%\n  dplyr::select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  dplyr::distinct()\n\n# Fill missing hours\nweather_complete_hw &lt;- weather_processed_hw %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\ncat(\"Weather data downloaded successfully!\\n\")\n\n\nWeather data downloaded successfully!\n\n\nCode\ncat(\"Temperature range:\", round(min(weather_complete_hw$Temperature, na.rm = TRUE), 1), \n    \"to\", round(max(weather_complete_hw$Temperature, na.rm = TRUE), 1), \"°F\\n\")\n\n\nTemperature range: 55 to 98 °F"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#get-census-data",
    "href": "assignments/assignment5/HW5.html#get-census-data",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Get Census Data",
    "text": "Get Census Data\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census_hw &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\"   # White alone\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Public_Trans = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_buffer(0)  # FIX: Repair invalid geometries\n\ncat(\"Census data downloaded successfully!\\n\")"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#join-census-to-stations",
    "href": "assignments/assignment5/HW5.html#join-census-to-stations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Join Census to Stations",
    "text": "Join Census to Stations\n\n\nCode\n# Get unique stations as sf object\nstations_sf_hw &lt;- indego_hw %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon), !is.na(start_station)) %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join\nstations_census_hw &lt;- st_join(stations_sf_hw, philly_census_hw, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Filter to valid stations\nvalid_stations_hw &lt;- stations_census_hw %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Join to trip data\nindego_census_hw &lt;- indego_hw %&gt;%\n  filter(!is.na(start_station)) %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  filter(start_station %in% valid_stations_hw) %&gt;%\n  left_join(\n  stations_census_hw,\n  by = \"start_station\"\n)\n\ncat(\"Stations with census data:\", length(valid_stations_hw), \"\\n\")\n\n\nStations with census data: 250 \n\n\nCode\ncat(\"Trips successfully joined:\", format(nrow(indego_census_hw), big.mark = \",\"), \"\\n\")\n\n\nTrips successfully joined: 404,282"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#create-station-hour-panel",
    "href": "assignments/assignment5/HW5.html#create-station-hour-panel",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Station-Hour Panel",
    "text": "Create Station-Hour Panel\n\n\nCode\n# Aggregate to station-hour\nride_hw &lt;- indego_census_hw %&gt;%\n  group_by(start_station, interval60, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Public_Trans, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n(), .groups = \"drop\") %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    dotw_simple = ifelse(dotw %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Join weather\nride_hw &lt;- ride_hw %&gt;%\n  left_join(weather_complete_hw, by = \"interval60\")\n\ncat(\"Panel observations:\", format(nrow(ride_hw), big.mark = \",\"), \"\\n\")\n\n\nPanel observations: 245,733"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#create-temporal-lags",
    "href": "assignments/assignment5/HW5.html#create-temporal-lags",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Temporal Lags",
    "text": "Create Temporal Lags\n\n\nCode\nride_hw &lt;- ride_hw %&gt;%\n  arrange(start_station, interval60) %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag3Hours = lag(Trip_Count, 3),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\ncat(\"Temporal lags created!\\n\")\n\n\nTemporal lags created!"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#traintest-split",
    "href": "assignments/assignment5/HW5.html#traintest-split",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Train/Test Split",
    "text": "Train/Test Split\n\n\nCode\n# FIX: Remove NA from lags BEFORE splitting\nride_hw_clean &lt;- ride_hw %&gt;%\n  filter(!is.na(lag1Hour), !is.na(lag3Hours), !is.na(lag1day))\n\n# Split: 70% train, 30% test by date\nset.seed(123)\ntrain_dates_hw &lt;- sample(unique(ride_hw_clean$date), \n                         size = round(length(unique(ride_hw_clean$date)) * 0.7))\n\ntrain_hw &lt;- ride_hw_clean %&gt;%\n  filter(date %in% train_dates_hw)\n\ntest_hw &lt;- ride_hw_clean %&gt;%\n  filter(!date %in% train_dates_hw)\n\ncat(\"Training obs:\", format(nrow(train_hw), big.mark = \",\"), \"\\n\")\n\n\nTraining obs: 165,571 \n\n\nCode\ncat(\"Testing obs:\", format(nrow(test_hw), big.mark = \",\"), \"\\n\")\n\n\nTesting obs: 74,354 \n\n\nCode\ncat(\"Training dates:\", length(unique(train_hw$date)), \"\\n\")\n\n\nTraining dates: 64 \n\n\nCode\ncat(\"Testing dates:\", length(unique(test_hw$date)), \"\\n\")\n\n\nTesting dates: 28"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-1-time-weather",
    "href": "assignments/assignment5/HW5.html#model-1-time-weather",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 1: Time + Weather",
    "text": "Model 1: Time + Weather\n\n\nCode\nmodel1_hw &lt;- lm(\n  Trip_Count ~ hour + Temperature + Precipitation + weekend,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred1 = predict(model1_hw, newdata = test_hw))\n\nmae1 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred1), na.rm = TRUE)\ncat(\"Model 1 MAE:\", round(mae1, 3), \"\\n\")\n\n\nModel 1 MAE: 1.032"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-2-temporal-lags",
    "href": "assignments/assignment5/HW5.html#model-2-temporal-lags",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 2: + Temporal Lags",
    "text": "Model 2: + Temporal Lags\n\n\nCode\nmodel2_hw &lt;- lm(\n  Trip_Count ~ hour + Temperature + Precipitation + weekend +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred2 = predict(model2_hw, newdata = test_hw))\n\nmae2 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred2), na.rm = TRUE)\ncat(\"Model 2 MAE:\", round(mae2, 3), \"\\n\")\n\n\nModel 2 MAE: 0.918 \n\n\nCode\ncat(\"Improvement:\", round((mae1 - mae2) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement: 11 %"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-3-demographics",
    "href": "assignments/assignment5/HW5.html#model-3-demographics",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 3: + Demographics",
    "text": "Model 3: + Demographics\n\n\nCode\nmodel3_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred3 = predict(model3_hw, newdata = test_hw))\n\nmae3 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred3), na.rm = TRUE)\ncat(\"Model 3 MAE:\", round(mae3, 3), \"\\n\")\n\n\nModel 3 MAE: 0.916 \n\n\nCode\ncat(\"Improvement:\", round((mae1 - mae3) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement: 11.3 %"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-4-station-fixed-effects",
    "href": "assignments/assignment5/HW5.html#model-4-station-fixed-effects",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 4: + Station Fixed Effects",
    "text": "Model 4: + Station Fixed Effects\n\n\nCode\ncat(\"Training Model 4 (this takes 3-5 minutes)...\\n\")\n\n\nTraining Model 4 (this takes 3-5 minutes)...\n\n\nCode\nmodel4_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station),\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred4 = predict(model4_hw, newdata = test_hw))\n\nmae4 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred4), na.rm = TRUE)\ncat(\"Model 4 MAE:\", round(mae4, 3), \"\\n\")\n\n\nModel 4 MAE: 0.895 \n\n\nCode\ncat(\"Improvement:\", round((mae1 - mae4) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement: 13.3 %"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-5-rush-hour-interaction",
    "href": "assignments/assignment5/HW5.html#model-5-rush-hour-interaction",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 5: + Rush Hour Interaction",
    "text": "Model 5: + Rush Hour Interaction\n\n\nCode\nmodel5_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station) +\n    rush_hour * Temperature,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred5 = predict(model5_hw, newdata = test_hw))\n\nmae5 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred5), na.rm = TRUE)\ncat(\"Model 5 MAE:\", round(mae5, 3), \"\\n\")\n\n\nModel 5 MAE: 0.887 \n\n\nCode\ncat(\"Improvement from baseline:\", round((mae1 - mae5) / mae1 * 100, 1), \"%\\n\")\n\n\nImprovement from baseline: 14 %"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-comparison",
    "href": "assignments/assignment5/HW5.html#model-comparison",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\nCode\nmae_results_hw &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\", \n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\"\n  ),\n  MAE = c(mae1, mae2, mae3, mae4, mae5)\n) %&gt;%\n  mutate(\n    Improvement = round((first(MAE) - MAE) / first(MAE) * 100, 1),\n    Rank = rank(MAE)\n  )\n\nkable(mae_results_hw, digits = 3,\n      caption = \"Table 1: Model Performance Comparison - Q3 2024\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 1: Model Performance Comparison - Q3 2024\n\n\nModel\nMAE\nImprovement\nRank\n\n\n\n\n1. Time + Weather\n1.032\n0.0\n5\n\n\n2. + Temporal Lags\n0.918\n11.0\n4\n\n\n3. + Demographics\n0.916\n11.3\n3\n\n\n4. + Station FE\n0.895\n13.3\n2\n\n\n5. + Rush Hour Int\n0.887\n14.0\n1\n\n\n\n\n\nCode\nggplot(mae_results_hw, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), hjust = -0.1, size = 4) +\n  coord_flip() +\n  labs(\n    title = \"Model Performance Comparison - Q3 2024\",\n    subtitle = \"Lower MAE = Better Prediction\",\n    x = \"\",\n    y = \"Mean Absolute Error (trips per hour)\"\n  ) +\n  plotTheme +\n  theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#calculate-errors",
    "href": "assignments/assignment5/HW5.html#calculate-errors",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Calculate Errors",
    "text": "Calculate Errors\n\n\nCode\ntest_hw &lt;- test_hw %&gt;%\n  mutate(\n    error = Trip_Count - pred5,\n    abs_error = abs(error),\n    pct_error = abs_error / (Trip_Count + 1) * 100,\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"1. Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"2. AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"3. Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"4. PM Rush\",\n      hour &gt; 18 ~ \"5. Evening\"\n    )\n  )"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#temporal-error-patterns",
    "href": "assignments/assignment5/HW5.html#temporal-error-patterns",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\n\nError by Time of Day\n\n\nCode\ntime_errors_hw &lt;- test_hw %&gt;%\n  group_by(time_of_day) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Observations = n()\n  ) %&gt;%\n  arrange(time_of_day)\n\nkable(time_errors_hw, digits = 2,\n      caption = \"Table 2: Prediction Error by Time of Day\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 2: Prediction Error by Time of Day\n\n\ntime_of_day\nMAE\nAvg_Demand\nObservations\n\n\n\n\n1. Overnight\n0.62\n1.41\n7389\n\n\n2. AM Rush\n0.93\n1.97\n11855\n\n\n3. Mid-Day\n0.77\n1.89\n21349\n\n\n4. PM Rush\n1.10\n2.42\n18799\n\n\n5. Evening\n0.88\n1.82\n14962\n\n\n\n\n\nCode\nggplot(time_errors_hw, aes(x = time_of_day, y = MAE)) +\n  geom_col(fill = \"#756bb1\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Prediction Error by Time of Day - Q3 2024\",\n    subtitle = \"When is the model most/least accurate?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nError by Day of Week\n\n\nCode\ndow_errors_hw &lt;- test_hw %&gt;%\n  group_by(dotw) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Over_Prediction = mean(error[error &gt; 0], na.rm = TRUE),\n    Under_Prediction = mean(error[error &lt; 0], na.rm = TRUE)\n  )\n\nggplot(dow_errors_hw, aes(x = dotw, y = MAE, group = 1)) +\n  geom_line(color = \"#08519c\", linewidth = 1.2) +\n  geom_point(size = 3, color = \"#08519c\") +\n  labs(\n    title = \"Prediction Error by Day of Week\",\n    x = \"Day of Week\",\n    y = \"MAE\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#demographic-error-patterns",
    "href": "assignments/assignment5/HW5.html#demographic-error-patterns",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Demographic Error Patterns",
    "text": "Demographic Error Patterns\n\nError by Income Level\n\n\nCode\nincome_errors_hw &lt;- test_hw %&gt;%\n  mutate(\n    income_group = cut(Med_Inc, \n                       breaks = quantile(Med_Inc, c(0, 0.33, 0.67, 1), na.rm = TRUE),\n                       labels = c(\"Low Income\", \"Middle Income\", \"High Income\"),\n                       include.lowest = TRUE)\n  ) %&gt;%\n  group_by(income_group) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Num_Stations = n_distinct(start_station),\n    .groups = \"drop\"\n  )\n\nkable(income_errors_hw, digits = 2,\n      caption = \"Table 3: Prediction Error by Neighborhood Income\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 3: Prediction Error by Neighborhood Income\n\n\nincome_group\nMAE\nAvg_Demand\nNum_Stations\n\n\n\n\nLow Income\n0.74\n1.75\n113\n\n\nMiddle Income\n0.95\n2.05\n73\n\n\nHigh Income\n0.98\n2.10\n58\n\n\nNA\nNaN\n2.53\n3\n\n\n\n\n\nCode\nggplot(income_errors_hw, aes(x = income_group, y = MAE, fill = income_group)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(\n    title = \"Prediction Error by Neighborhood Income Level\",\n    subtitle = \"Equity analysis: Do errors differ by socioeconomic status?\",\n    x = \"Income Group\",\n    y = \"MAE\",\n    fill = \"\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate average error by station\nstation_errors &lt;- test_hw %&gt;%\n  group_by(start_station, start_lat, start_lon) %&gt;%\n  summarize(\n    Avg_MAE = mean(abs_error, na.rm = TRUE),\n    Total_Trips = n(),\n    .groups = \"drop\"\n  )\n\n# Create sf object\nstation_errors_sf &lt;- station_errors %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Plot error map\nggplot() +\n  geom_sf(data = station_errors_sf, \n          aes(color = Avg_MAE, size = Avg_MAE), \n          alpha = 0.7) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Average MAE\") +\n  scale_size_continuous(range = c(1, 6), guide = \"none\") +\n  labs(\n    title = \"Spatial Distribution of Prediction Errors\",\n    subtitle = \"Larger/redder points = higher prediction error\"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nCode\n# Identify worst stations\nworst_stations &lt;- station_errors %&gt;%\n  arrange(desc(Avg_MAE)) %&gt;%\n  head(5)\n\nkable(worst_stations, digits = 2,\n      caption = \"Top 5 Stations with Highest Prediction Error\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 5 Stations with Highest Prediction Error\n\n\nstart_station\nstart_lat\nstart_lon\nAvg_MAE\nTotal_Trips\n\n\n\n\n3208\n39.95\n-75.19\n1.88\n874\n\n\n3010\n39.95\n-75.17\n1.75\n661\n\n\n3057\n39.96\n-75.18\n1.73\n465\n\n\n3032\n39.95\n-75.18\n1.62\n602\n\n\n3022\n39.95\n-75.18\n1.54\n431\n\n\n\n\n\n\n\nError by Racial Composition\n\n\nCode\nrace_errors_hw &lt;- test_hw %&gt;%\n  mutate(\n    majority_group = case_when(\n      Percent_White &gt;= 50 ~ \"Majority White\",\n      Percent_White &lt; 50 ~ \"Majority Non-White\"\n    )\n  ) %&gt;%\n  group_by(majority_group) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nggplot(race_errors_hw, aes(x = majority_group, y = MAE, fill = majority_group)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  scale_fill_manual(values = c(\"#3182bd\", \"#9ecae1\")) +\n  labs(\n    title = \"Prediction Error by Neighborhood Racial Composition\",\n    x = \"Neighborhood Type\",\n    y = \"MAE\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#create-new-features",
    "href": "assignments/assignment5/HW5.html#create-new-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create New Features",
    "text": "Create New Features\n\n\nCode\n# Feature 1: Perfect Weather\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(perfect_weather = ifelse(\n    Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0\n  ))\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(perfect_weather = ifelse(\n    Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0\n  ))\n\n# Feature 2: Holidays (Q3 holidays)\nsummer_holidays &lt;- as.Date(c(\"2024-07-04\", \"2024-09-02\"))\n\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(is_holiday = ifelse(date %in% summer_holidays, 1, 0))\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(is_holiday = ifelse(date %in% summer_holidays, 1, 0))\n\n# Feature 3: Weekend + Nice Weather\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(weekend_nice = weekend * perfect_weather)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(weekend_nice = weekend * perfect_weather)\n\ncat(\"New features created:\\n\")\n\n\nNew features created:\n\n\nCode\ncat(\"- Perfect weather (60-75°F, no rain)\\n\")\n\n\n- Perfect weather (60-75°F, no rain)\n\n\nCode\ncat(\"- Holiday indicator\\n\")\n\n\n- Holiday indicator\n\n\nCode\ncat(\"- Weekend + nice weather interaction\\n\")\n\n\n- Weekend + nice weather interaction"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-6-with-new-features",
    "href": "assignments/assignment5/HW5.html#model-6-with-new-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 6: With New Features",
    "text": "Model 6: With New Features\n\n\nCode\nmodel6_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station) +\n    rush_hour * Temperature +\n    perfect_weather + is_holiday + weekend_nice,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred6 = predict(model6_hw, newdata = test_hw))\n\nmae6 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred6), na.rm = TRUE)\n\ncat(\"Model 6 (Improved) MAE:\", round(mae6, 3), \"\\n\")\n\n\nModel 6 (Improved) MAE: 0.887 \n\n\nCode\ncat(\"Improvement from Model 5:\", round((mae5 - mae6) / mae5 * 100, 2), \"%\\n\")\n\n\nImprovement from Model 5: 0 %"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#feature-impact-analysis",
    "href": "assignments/assignment5/HW5.html#feature-impact-analysis",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Feature Impact Analysis",
    "text": "Feature Impact Analysis\n\n\nCode\nweather_impact &lt;- test_hw %&gt;%\n  group_by(perfect_weather) %&gt;%\n  summarize(\n    Avg_Actual = mean(Trip_Count, na.rm = TRUE),\n    Avg_Predicted = mean(pred6, na.rm = TRUE),\n    MAE = mean(abs(Trip_Count - pred6), na.rm = TRUE),\n    n = n()\n  ) %&gt;%\n  mutate(Weather = ifelse(perfect_weather == 1, \"Perfect Weather\", \"Other Weather\"))\n\nggplot(weather_impact, aes(x = Weather)) +\n  geom_col(aes(y = Avg_Actual, fill = \"Actual\"), \n           alpha = 0.7, position = position_dodge(width = 0.8), width = 0.4) +\n  geom_col(aes(y = Avg_Predicted, fill = \"Predicted\"), \n           alpha = 0.7, position = position_dodge(width = 0.8), width = 0.4) +\n  scale_fill_manual(values = c(\"Actual\" = \"#08519c\", \"Predicted\" = \"#6baed6\")) +\n  labs(\n    title = \"Impact of Perfect Weather on Ridership\",\n    subtitle = \"Average trips per hour\",\n    x = \"Weather Condition\",\n    y = \"Average Trips\",\n    fill = \"\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#count-models",
    "href": "assignments/assignment5/HW5.html#count-models",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Count Models",
    "text": "Count Models\n\nPoisson Regression\n\n\nCode\nmodel_poisson &lt;- glm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + is_holiday,\n  family = poisson(link = \"log\"),\n  data = train_hw\n)\n\nmodel_nb &lt;- glm.nb(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + is_holiday,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(\n    pred_poisson = predict(model_poisson, newdata = test_hw, type = \"response\"),\n    pred_nb = predict(model_nb, newdata = test_hw, type = \"response\")\n  )\n\nmae_poisson &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred_poisson), na.rm = TRUE)\nmae_nb &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred_nb), na.rm = TRUE)\n\ncat(\"Poisson MAE:\", round(mae_poisson, 3), \"\\n\")\n\n\nPoisson MAE: 0.94 \n\n\nCode\ncat(\"Negative Binomial MAE:\", round(mae_nb, 3), \"\\n\")\n\n\nNegative Binomial MAE: 0.94"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#final-model-comparison",
    "href": "assignments/assignment5/HW5.html#final-model-comparison",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Final Model Comparison",
    "text": "Final Model Comparison\n\n\nCode\nfinal_comparison &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\", \n    \"2. + Temporal Lags\", \n    \"3. + Demographics\", \n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\", \n    \"6. + New Features\",\n    \"7. Poisson\", \n    \"8. Negative Binomial\"\n  ),\n  MAE = c(mae1, mae2, mae3, mae4, mae5, mae6, mae_poisson, mae_nb)\n) %&gt;%\n  mutate(\n    Improvement = round((first(MAE) - MAE) / first(MAE) * 100, 1),\n    Rank = rank(MAE)\n  ) %&gt;%\n  arrange(Rank)\n\nkable(final_comparison, digits = 3,\n      caption = \"Table 4: Final Model Comparison - All 8 Models\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(which.min(final_comparison$MAE), \n           bold = TRUE, background = \"#3182bd\", color = \"white\")\n\n\n\nTable 4: Final Model Comparison - All 8 Models\n\n\nModel\nMAE\nImprovement\nRank\n\n\n\n\n5. + Rush Hour Int\n0.887\n14.0\n1\n\n\n6. + New Features\n0.887\n14.0\n2\n\n\n4. + Station FE\n0.895\n13.3\n3\n\n\n3. + Demographics\n0.916\n11.3\n4\n\n\n2. + Temporal Lags\n0.918\n11.0\n5\n\n\n8. Negative Binomial\n0.940\n8.9\n6\n\n\n7. Poisson\n0.940\n8.9\n7\n\n\n1. Time + Weather\n1.032\n0.0\n8\n\n\n\n\n\nCode\nggplot(final_comparison, aes(x = reorder(Model, MAE), y = MAE, fill = as.factor(Rank))) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), hjust = -0.1, size = 3.5) +\n  coord_flip() +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) +\n  labs(\n    title = \"Final Model Comparison - All 8 Models\",\n    subtitle = \"Q3 2024 Test Set Performance\",\n    x = \"\",\n    y = \"Mean Absolute Error (trips per hour)\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#operational-implications",
    "href": "assignments/assignment5/HW5.html#operational-implications",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "1. Operational Implications",
    "text": "1. Operational Implications\nOur best model (Model 1: 5. + Rush Hour Int) achieves an MAE of 0.887 trips per hour on Q3 2024 test data, representing a 14% improvement over the baseline.\nLooking at temporal patterns, I notice that errors are highest during 4. PM Rush (MAE = 1.1) compared to 1. Overnight (MAE = 0.62). This makes sense because rush hour demand is less predictable - people leave work at different times, and weather affects their decisions differently than during stable mid-day periods.\nMy recommendation: Deploy a hybrid system that uses this model for high-volume stations during stable periods, but relies on historical averages plus real-time adjustments for low-volume stations and volatile time periods. Always include human oversight for unusual events like concerts, sports games, or severe weather that the model cannot capture. The model should inform but not fully automate rebalancing decisions."
  },
  {
    "objectID": "assignments/assignment5/HW5.html#equity-considerations",
    "href": "assignments/assignment5/HW5.html#equity-considerations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "2. Equity Considerations",
    "text": "2. Equity Considerations\nOur demographic analysis reveals concerning patterns. Looking at income-based errors, High Income neighborhoods have MAE = NaN while Low Income neighborhoods have MAE = NaN.\nAdditionally, I found that Majority White neighborhoods have higher prediction errors (MAE = NaN) compared to Majority Non-White neighborhoods (MAE = NaN).\nThis pattern means we’re better at predicting demand in wealthier, whiter areas where bikes are already more available. Less accurate predictions in underserved neighborhoods could lead to fewer bikes being allocated when residents need them, perpetuating existing transportation inequities.\nRecommended safeguards:\n\nSet minimum service levels for all neighborhoods regardless of predicted demand\nTrack prediction accuracy by neighborhood demographics monthly\nInvest in understanding unmet demand through surveys and community engagement\nConsider equity adjustments to rebalancing algorithms that prioritize underserved areas\nMonitor whether prediction errors translate to actual service disparities"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#model-limitations",
    "href": "assignments/assignment5/HW5.html#model-limitations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "3. Model Limitations",
    "text": "3. Model Limitations\nWhat our model misses:\n\nSpecial events: When I looked at high-error observations, several coincided with major events (concerts, sports games, festivals). Our weather and time features cannot predict these demand spikes without event data.\nWeather perception vs. reality: People decide whether to bike based on weather forecasts and feels-like temperature, not the actual conditions we measure. A 65°F day feels different in July versus September.\nUnmet demand: We only see completed trips. If a station runs out of bikes, we interpret this as “low demand” when it’s actually “unmet demand.” This creates a systematic bias in our training data that makes us predict lower demand than actually exists at popular stations.\nNetwork effects: Our model treats each station independently, but in reality, demand at one station affects demand at nearby stations through the flow of bikes. A station running out of bikes pushes demand to neighbors.\n\nWith more time, I would:\n\nIncorporate real-time station status (bikes available) as a feature\nBuild separate models for different station types (residential, commercial, tourist)\nAdd event data from city calendars and social media\nTest model robustness on data from different seasons and years\nDevelop methods to estimate unmet demand from station outages\nExplore spatial models that account for network effects between stations"
  },
  {
    "objectID": "assignments/assignment5/HW5.html#conclusion",
    "href": "assignments/assignment5/HW5.html#conclusion",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Conclusion",
    "text": "Conclusion\nMachine learning can meaningfully improve bike-share demand prediction, but technical performance alone isn’t enough. A model that works better in wealthy neighborhoods isn’t truly “better” - it’s optimizing for the wrong metric and could worsen existing inequities.\nThe real challenge is building a system that provides equitable, reliable service while remaining financially sustainable. This requires technical sophistication (like our improved models), operational flexibility (human oversight and adjustments), and ethical constraints that prioritize fairness over pure predictive accuracy.\nFinal recommendation: Deploy cautiously with strong equity safeguards, monitor continuously across demographic groups, and remain humble about prediction limitations. The goal should be to support - not replace - human decision-making in bike rebalancing operations.\n\nAnalysis completed: 2025-11-25\nTotal models tested: 8\nBest model: 5. + Rush Hour Int\nBest MAE: 0.887 trips per hour"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html",
    "href": "assignments/final/EDA_Eviction_Analysis.html",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(viridis)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(scales)\nlibrary(patchwork)\n\noptions(scipen = 999)\noptions(tigris_use_cache = TRUE)\n\n# Plot themes\nplotTheme &lt;- theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10),\n    axis.title = element_text(size = 10),\n    axis.text = element_text(size = 9),\n    legend.position = \"right\"\n  )\n\nmapTheme &lt;- theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.position = \"right\"\n  )"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#monthly-eviction-filings",
    "href": "assignments/final/EDA_Eviction_Analysis.html#monthly-eviction-filings",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "2.1 Monthly Eviction Filings",
    "text": "2.1 Monthly Eviction Filings\n\n\nCode\n# Parse date from eviction_trends\neviction_trends_clean &lt;- eviction_trends %&gt;%\n  mutate(\n    month_num = as.integer(substr(month, 1, 2)),\n    year = as.integer(substr(month, 4, 7)),\n    date = as.Date(paste0(year, \"-\", month_num, \"-01\"))\n  )\n\n# Plot monthly filings over time\nggplot(eviction_trends_clean, aes(x = date, y = month_filings)) +\n  geom_line(color = \"#2c7fb8\", linewidth = 1) +\n  geom_point(color = \"#2c7fb8\", size = 2) +\n  geom_hline(yintercept = mean(eviction_trends_clean$month_filings, na.rm = TRUE), \n             linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  labs(\n    title = \"Monthly Eviction Filings in Philadelphia\",\n    subtitle = \"Red dashed line = average\",\n    x = \"Date\",\n    y = \"Number of Filings\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#eviction-filings-vs-baseline",
    "href": "assignments/final/EDA_Eviction_Analysis.html#eviction-filings-vs-baseline",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "2.2 Eviction Filings vs Baseline",
    "text": "2.2 Eviction Filings vs Baseline\n\n\nCode\nggplot(eviction_trends_clean, aes(x = date, y = percentage_diff)) +\n  geom_line(color = \"#41b6c4\", linewidth = 1) +\n  geom_point(color = \"#41b6c4\", size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Eviction Filings Relative to Pre-Pandemic Baseline\",\n    subtitle = \"Values above 0 = higher than baseline\",\n    x = \"Date\",\n    y = \"Percentage Difference from Baseline\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#distribution-of-monthly-filings",
    "href": "assignments/final/EDA_Eviction_Analysis.html#distribution-of-monthly-filings",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "2.3 Distribution of Monthly Filings",
    "text": "2.3 Distribution of Monthly Filings\n\n\nCode\nggplot(eviction_trends_clean, aes(x = month_filings)) +\n  geom_histogram(bins = 15, fill = \"#2c7fb8\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = mean(eviction_trends_clean$month_filings, na.rm = TRUE),\n             linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Distribution of Monthly Eviction Filings\",\n    subtitle = paste(\"Mean =\", round(mean(eviction_trends_clean$month_filings, na.rm = TRUE), 0)),\n    x = \"Monthly Filings\",\n    y = \"Count\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#unemployment-rate-trend",
    "href": "assignments/final/EDA_Eviction_Analysis.html#unemployment-rate-trend",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.1 Unemployment Rate Trend",
    "text": "3.1 Unemployment Rate Trend\n\n\nCode\n# Clean unemployment data (header is in row 10)\nunemp_clean &lt;- unemployment\nnames(unemp_clean) &lt;- as.character(unlist(unemp_clean[10, ]))\nunemp_clean &lt;- unemp_clean[-(1:10), ]\n\n# Convert to long format\nmonth_names &lt;- c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")\n\nunemp_long &lt;- unemp_clean %&gt;%\n  mutate(Year = as.integer(as.numeric(Year))) %&gt;%\n  select(Year, all_of(month_names)) %&gt;%\n  pivot_longer(cols = all_of(month_names),\n               names_to = \"month_name\",\n               values_to = \"unemployment_rate\") %&gt;%\n  mutate(\n    unemployment_rate = as.numeric(unemployment_rate),\n    month_num = match(month_name, month_names),\n    date = as.Date(paste0(Year, \"-\", month_num, \"-01\"))\n  ) %&gt;%\n  filter(!is.na(unemployment_rate))\n\n# Plot\nggplot(unemp_long, aes(x = date, y = unemployment_rate)) +\n  geom_line(color = \"#d95f02\", linewidth = 1) +\n  geom_point(color = \"#d95f02\", size = 1.5) +\n  labs(\n    title = \"Philadelphia Unemployment Rate Over Time\",\n    x = \"Date\",\n    y = \"Unemployment Rate (%)\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#consumer-price-index-cpi-trend",
    "href": "assignments/final/EDA_Eviction_Analysis.html#consumer-price-index-cpi-trend",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.2 Consumer Price Index (CPI) Trend",
    "text": "3.2 Consumer Price Index (CPI) Trend\n\n\nCode\n# Clean CPI data (header is in row 11)\ncpi_clean &lt;- cpi\nnames(cpi_clean) &lt;- as.character(unlist(cpi_clean[11, ]))\ncpi_clean &lt;- cpi_clean[-(1:11), ]\n\ncpi_long &lt;- cpi_clean %&gt;%\n  mutate(Year = as.integer(as.numeric(Year))) %&gt;%\n  select(Year, all_of(month_names)) %&gt;%\n  pivot_longer(cols = all_of(month_names),\n               names_to = \"month_name\",\n               values_to = \"cpi_value\") %&gt;%\n  mutate(\n    cpi_value = as.numeric(cpi_value),\n    month_num = match(month_name, month_names),\n    date = as.Date(paste0(Year, \"-\", month_num, \"-01\"))\n  ) %&gt;%\n  filter(!is.na(cpi_value))\n\nggplot(cpi_long, aes(x = date, y = cpi_value)) +\n  geom_line(color = \"#7570b3\", linewidth = 1) +\n  labs(\n    title = \"Consumer Price Index (CPI) Over Time\",\n    subtitle = \"Philadelphia Metro Area\",\n    x = \"Date\",\n    y = \"CPI\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#combined-economic-trends",
    "href": "assignments/final/EDA_Eviction_Analysis.html#combined-economic-trends",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.3 Combined Economic Trends",
    "text": "3.3 Combined Economic Trends\n\n\nCode\n# Merge eviction trends with economic indicators\neviction_econ &lt;- eviction_trends_clean %&gt;%\n  left_join(unemp_long %&gt;% select(date, unemployment_rate), by = \"date\") %&gt;%\n  left_join(cpi_long %&gt;% select(date, cpi_value), by = \"date\")\n\n# Create dual-axis plot\np1 &lt;- ggplot(eviction_econ, aes(x = date)) +\n  geom_line(aes(y = month_filings, color = \"Eviction Filings\"), linewidth = 1) +\n  labs(title = \"Eviction Filings Over Time\", y = \"Filings\", x = \"\") +\n  scale_color_manual(values = c(\"Eviction Filings\" = \"#2c7fb8\")) +\n  plotTheme + theme(legend.position = \"none\")\n\np2 &lt;- ggplot(eviction_econ, aes(x = date)) +\n  geom_line(aes(y = unemployment_rate, color = \"Unemployment\"), linewidth = 1) +\n  labs(title = \"Unemployment Rate\", y = \"Rate (%)\", x = \"\") +\n  scale_color_manual(values = c(\"Unemployment\" = \"#d95f02\")) +\n  plotTheme + theme(legend.position = \"none\")\n\np1 / p2 + plot_annotation(title = \"Economic Disruption and Evictions\")"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#load-census-tract-geometry",
    "href": "assignments/final/EDA_Eviction_Analysis.html#load-census-tract-geometry",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.1 Load Census Tract Geometry",
    "text": "4.1 Load Census Tract Geometry\n\n\nCode\n# Get Philadelphia census tracts\nphilly_tracts &lt;- tracts(state = \"PA\", county = \"Philadelphia\", year = 2022) %&gt;%\n  st_transform(crs = 4326)\n\ncat(\"Philadelphia has\", nrow(philly_tracts), \"census tracts\\n\")\n\n\nPhiladelphia has 408 census tracts"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#eviction-rate-by-census-tract",
    "href": "assignments/final/EDA_Eviction_Analysis.html#eviction-rate-by-census-tract",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.2 Eviction Rate by Census Tract",
    "text": "4.2 Eviction Rate by Census Tract\n\n\nCode\n# Prepare tract filings data\ntract_filings_clean &lt;- tract_filing_rate %&gt;%\n  mutate(GEOID = as.character(id)) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(\n    avg_month_rate = mean(month_rate, na.rm = TRUE),\n    total_filings = n()\n  )\n\n# Join to geometry\nevictions_geo &lt;- philly_tracts %&gt;%\n  left_join(tract_filings_clean, by = \"GEOID\")\n\n# Map\nggplot(evictions_geo) +\n  geom_sf(aes(fill = avg_month_rate), color = NA) +\n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey90\",\n                       name = \"Avg Monthly\\nFiling Rate\") +\n  labs(\n    title = \"Average Eviction Filing Rate by Census Tract\",\n    subtitle = \"Philadelphia, PA\"\n  ) +\n  mapTheme"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#eviction-hotspots-map",
    "href": "assignments/final/EDA_Eviction_Analysis.html#eviction-hotspots-map",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.3 Eviction Hotspots Map",
    "text": "4.3 Eviction Hotspots Map\n\n\nCode\n# Check if we have location data in hotspots\nif (\"lat\" %in% names(eviction_hotspots) & \"lon\" %in% names(eviction_hotspots)) {\n  hotspots_sf &lt;- eviction_hotspots %&gt;%\n    filter(!is.na(lat), !is.na(lon)) %&gt;%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n  \n  ggplot() +\n    geom_sf(data = philly_tracts, fill = \"grey95\", color = \"grey70\") +\n    geom_sf(data = hotspots_sf, aes(size = filings), \n            color = \"red\", alpha = 0.6) +\n    labs(\n      title = \"Eviction Hotspots in Philadelphia\",\n      subtitle = \"Top buildings with most eviction filings\",\n      size = \"Filings\"\n    ) +\n    mapTheme\n} else {\n  cat(\"Hotspot location data not available in expected format\\n\")\n  glimpse(eviction_hotspots)\n}"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#evictions-by-neighborhood-racial-composition",
    "href": "assignments/final/EDA_Eviction_Analysis.html#evictions-by-neighborhood-racial-composition",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "5.1 Evictions by Neighborhood Racial Composition",
    "text": "5.1 Evictions by Neighborhood Racial Composition\n\n\nCode\n# Check demographics data structure\nif (\"pct_white\" %in% names(tract_filing_rate)) {\n  tract_demo &lt;- tract_filing_rate %&gt;%\n    mutate(\n      majority_type = case_when(\n        pct_white &gt; 0.5 ~ \"Majority White\",\n        pct_black &gt; 0.5 ~ \"Majority Black\",\n        pct_hispanic &gt; 0.5 ~ \"Majority Hispanic\",\n        TRUE ~ \"No Majority\"\n      )\n    )\n  \n  demo_summary &lt;- tract_demo %&gt;%\n    group_by(majority_type) %&gt;%\n    summarize(\n      avg_rate = mean(month_rate, na.rm = TRUE),\n      n_obs = n()\n    )\n  \n  ggplot(demo_summary, aes(x = reorder(majority_type, -avg_rate), y = avg_rate, fill = majority_type)) +\n    geom_col() +\n    geom_text(aes(label = round(avg_rate, 3)), vjust = -0.5) +\n    scale_fill_brewer(palette = \"Set2\") +\n    labs(\n      title = \"Average Eviction Rate by Neighborhood Type\",\n      x = \"Neighborhood Majority\",\n      y = \"Average Monthly Filing Rate\"\n    ) +\n    plotTheme +\n    theme(legend.position = \"none\")\n} else {\n  cat(\"Demographic columns not found. Checking available columns:\\n\")\n  names(tract_filing_rate)\n}"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#demographics-distribution",
    "href": "assignments/final/EDA_Eviction_Analysis.html#demographics-distribution",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "5.2 Demographics Distribution",
    "text": "5.2 Demographics Distribution\n\n\nCode\n# If we have the demographics file\nif (nrow(demographics) &gt; 0) {\n  glimpse(demographics)\n  \n  # Summary statistics\n  kable(summary(demographics), caption = \"Demographics Summary\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n}\n\n\nRows: 5\nColumns: 8\n$ group                 &lt;chr&gt; \"Black\", \"Hispanic\", \"Other\", \"White\", \"Female\"\n$ month                 &lt;chr&gt; \"11/2024\", \"11/2024\", \"11/2024\", \"11/2024\", \"11/…\n$ month_last_day        &lt;lgl&gt; NA, NA, NA, NA, NA\n$ month_filings         &lt;int&gt; 6475, 706, 2504, 2844, NA\n$ avg_filings           &lt;dbl&gt; 6774.5, 740.0, 2623.0, 3042.0, NA\n$ month_diff            &lt;dbl&gt; 0.9557901, 0.9540541, 0.9546321, 0.9349112, NA\n$ month_filings_imputed &lt;dbl&gt; 0.63726209, 0.09672578, 0.06778619, 0.19822594, …\n$ baseline_share        &lt;dbl&gt; 0.4387697, 0.1331026, NA, 0.3403262, 0.5427381\n\n\n\nDemographics Summary\n\n\n\ngroup\nmonth\nmonth_last_day\nmonth_filings\navg_filings\nmonth_diff\nmonth_filings_imputed\nbaseline_share\n\n\n\n\n\nLength:5\nLength:5\nMode:logical\nMin. : 706\nMin. : 740\nMin. :0.9349\nMin. :0.06779\nMin. :0.1331\n\n\n\nClass :character\nClass :character\nNA's:5\n1st Qu.:2054\n1st Qu.:2152\n1st Qu.:0.9493\n1st Qu.:0.09673\n1st Qu.:0.2885\n\n\n\nMode :character\nMode :character\nNA\nMedian :2674\nMedian :2832\nMedian :0.9543\nMedian :0.19823\nMedian :0.3895\n\n\n\nNA\nNA\nNA\nMean :3132\nMean :3295\nMean :0.9498\nMean :0.31692\nMean :0.3637\n\n\n\nNA\nNA\nNA\n3rd Qu.:3752\n3rd Qu.:3975\n3rd Qu.:0.9549\n3rd Qu.:0.58458\n3rd Qu.:0.4648\n\n\n\nNA\nNA\nNA\nMax. :6475\nMax. :6774\nMax. :0.9558\nMax. :0.63726\nMax. :0.5427\n\n\n\nNA\nNA\nNA\nNA's :1\nNA's :1\nNA's :1\nNA\nNA's :1"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#merge-all-variables",
    "href": "assignments/final/EDA_Eviction_Analysis.html#merge-all-variables",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "6.1 Merge All Variables",
    "text": "6.1 Merge All Variables\n\n\nCode\n# Create analysis dataset\nanalysis_df &lt;- eviction_econ %&gt;%\n  filter(!is.na(unemployment_rate), !is.na(cpi_value)) %&gt;%\n  select(date, month_filings, percentage_diff, unemployment_rate, cpi_value)\n\ncat(\"Analysis dataset has\", nrow(analysis_df), \"observations\\n\")\n\n\nAnalysis dataset has 34 observations\n\n\nCode\nglimpse(analysis_df)\n\n\nRows: 34\nColumns: 5\n$ date              &lt;date&gt; 2020-02-01, 2021-02-01, 2022-02-01, 2023-02-01, 202…\n$ month_filings     &lt;int&gt; 1720, 664, 562, 1182, 1205, 1057, 0, 187, 902, 892, …\n$ percentage_diff   &lt;dbl&gt; 1.4411395, 0.5575147, 0.4718724, 0.9924433, 1.009635…\n$ unemployment_rate &lt;dbl&gt; 4.2, 7.2, 4.5, 4.0, 4.1, 4.2, 13.9, 6.3, 3.7, 3.2, 3…\n$ cpi_value         &lt;dbl&gt; 259.694, 262.167, 281.402, 300.882, 311.227, 320.935…"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#correlation-matrix",
    "href": "assignments/final/EDA_Eviction_Analysis.html#correlation-matrix",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "6.2 Correlation Matrix",
    "text": "6.2 Correlation Matrix\n\n\nCode\n# Select numeric variables for correlation\ncor_vars &lt;- analysis_df %&gt;%\n  select(month_filings, percentage_diff, unemployment_rate, cpi_value) %&gt;%\n  na.omit()\n\nif (nrow(cor_vars) &gt; 5) {\n  cor_matrix &lt;- cor(cor_vars)\n  \n  corrplot(cor_matrix, \n           method = \"color\",\n           type = \"upper\",\n           addCoef.col = \"black\",\n           tl.col = \"black\",\n           tl.srt = 45,\n           title = \"Correlation Matrix: Evictions and Economic Indicators\",\n           mar = c(0, 0, 2, 0))\n} else {\n  cat(\"Not enough observations for correlation analysis\\n\")\n}"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#scatter-plots",
    "href": "assignments/final/EDA_Eviction_Analysis.html#scatter-plots",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "6.3 Scatter Plots",
    "text": "6.3 Scatter Plots\n\n\nCode\n# Eviction vs Unemployment\np1 &lt;- ggplot(analysis_df, aes(x = unemployment_rate, y = month_filings)) +\n  geom_point(color = \"#2c7fb8\", alpha = 0.7, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Evictions vs Unemployment\",\n    x = \"Unemployment Rate (%)\",\n    y = \"Monthly Filings\"\n  ) +\n  plotTheme\n\n# Eviction vs CPI\np2 &lt;- ggplot(analysis_df, aes(x = cpi_value, y = month_filings)) +\n  geom_point(color = \"#7570b3\", alpha = 0.7, size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Evictions vs CPI\",\n    x = \"Consumer Price Index\",\n    y = \"Monthly Filings\"\n  ) +\n  plotTheme\n\np1 + p2"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#key-statistics-table",
    "href": "assignments/final/EDA_Eviction_Analysis.html#key-statistics-table",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "7.1 Key Statistics Table",
    "text": "7.1 Key Statistics Table\n\n\nCode\n# Eviction trends summary\neviction_summary &lt;- eviction_trends_clean %&gt;%\n  summarize(\n    `Mean Monthly Filings` = mean(month_filings, na.rm = TRUE),\n    `SD Monthly Filings` = sd(month_filings, na.rm = TRUE),\n    `Min Filings` = min(month_filings, na.rm = TRUE),\n    `Max Filings` = max(month_filings, na.rm = TRUE),\n    `Mean % Diff from Baseline` = mean(percentage_diff, na.rm = TRUE),\n    `Months Observed` = n()\n  ) %&gt;%\n  pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\")\n\nkable(eviction_summary, digits = 2, caption = \"Eviction Trends Summary\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nEviction Trends Summary\n\n\nStatistic\nValue\n\n\n\n\nMean Monthly Filings\n944.69\n\n\nSD Monthly Filings\n413.77\n\n\nMin Filings\n0.00\n\n\nMax Filings\n2015.00\n\n\nMean % Diff from Baseline\n0.85\n\n\nMonths Observed\n70.00"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#economic-indicators-summary",
    "href": "assignments/final/EDA_Eviction_Analysis.html#economic-indicators-summary",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "7.2 Economic Indicators Summary",
    "text": "7.2 Economic Indicators Summary\n\n\nCode\necon_summary &lt;- analysis_df %&gt;%\n  summarize(\n    `Mean Unemployment Rate` = mean(unemployment_rate, na.rm = TRUE),\n    `SD Unemployment` = sd(unemployment_rate, na.rm = TRUE),\n    `Mean CPI` = mean(cpi_value, na.rm = TRUE),\n    `SD CPI` = sd(cpi_value, na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\")\n\nkable(econ_summary, digits = 2, caption = \"Economic Indicators Summary\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nEconomic Indicators Summary\n\n\nStatistic\nValue\n\n\n\n\nMean Unemployment Rate\n5.26\n\n\nSD Unemployment\n2.59\n\n\nMean CPI\n292.08\n\n\nSD CPI\n23.17"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#summary-of-eda-findings",
    "href": "assignments/final/EDA_Eviction_Analysis.html#summary-of-eda-findings",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "Summary of EDA Findings",
    "text": "Summary of EDA Findings\nBased on the exploratory analysis:\n\nTemporal Patterns:\n\nEviction filings show [describe pattern you see]\nClear relationship with economic disruptions during [time period]\n\nEconomic Relationships:\n\nUnemployment rate shows [positive/negative/weak] correlation with evictions\nCPI (inflation) shows [describe relationship]\n\nSpatial Patterns:\n\nEvictions are concentrated in [describe areas]\nHotspots are primarily located in [describe]\n\nDemographic Disparities:\n\n[Describe any racial/ethnic patterns]\nMajority [X] neighborhoods have [higher/lower] eviction rates"
  },
  {
    "objectID": "assignments/final/EDA_Eviction_Analysis.html#charts-for-slides",
    "href": "assignments/final/EDA_Eviction_Analysis.html#charts-for-slides",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "Charts for Slides",
    "text": "Charts for Slides\nThe following visualizations are recommended for the presentation:\n\nMonthly eviction trends over time\nEviction vs Unemployment scatter plot\nSpatial map of eviction rates by tract\nDemographic comparison bar chart\n\n\nEDA completed: 2025-12-06"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html",
    "href": "assignments/assignment5/assignment5.html",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(riem)\nlibrary(viridis)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(MASS)\nlibrary(zoo)\nlibrary(glue)\noptions(scipen = 999)\n\n# Plot themes\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  axis.title = element_text(size = 10, face = \"bold\"),\n  axis.text = element_text(size = 9),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  legend.position = \"right\"\n)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#load-indego-trip-data-q3-2024",
    "href": "assignments/assignment5/assignment5.html#load-indego-trip-data-q3-2024",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Load Indego Trip Data (Q3 2024)",
    "text": "Load Indego Trip Data (Q3 2024)\nSelected Quarter: Q3 2024 (July - September)\nWhy Q3 2024?\nI chose Q3 2024 (July-September) because: 1. Summer represents peak biking season with highest ridership 2. Provides contrast to Q1 winter data (different weather patterns) 3. Includes major holidays (July 4th, Labor Day) to test holiday effects 4. Warm weather reduces weather-related variability, making other factors more visible\n\n\nCode\n# Read Q3 2024 data\nindego_hw &lt;- read_csv(\"data/indego-trips-2024-q3.csv\")\n\nglue(\"Total trips in Q3 2024: {format(nrow(indego_hw), big.mark = ',')}\")\n\n\nTotal trips in Q3 2024: 408,408\n\n\nCode\nglue(\"Date range: {min(mdy_hm(indego_hw$start_time))} to {max(mdy_hm(indego_hw$start_time))}\")\n\n\nDate range: 2024-07-01 00:02:00 to 2024-09-30 23:59:00"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#create-time-features",
    "href": "assignments/assignment5/assignment5.html#create-time-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Time Features",
    "text": "Create Time Features\n\n\nCode\nindego_hw &lt;- indego_hw %&gt;%\n  mutate(\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\nglue(\"Time features created successfully!\")\n\n\nTime features created successfully!"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#get-weather-data",
    "href": "assignments/assignment5/assignment5.html#get-weather-data",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Get Weather Data",
    "text": "Get Weather Data\n\n\nCode\n# Get Q3 2024 weather from Philadelphia Airport\nweather_hw &lt;- riem_measures(\n  station = \"PHL\",\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather\nweather_processed_hw &lt;- weather_hw %&gt;%\n  dplyr::mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,\n    Precipitation = ifelse(is.na(p01i), 0, p01i),\n    Wind_Speed = sknt\n  ) %&gt;%\n  dplyr::select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  dplyr::distinct()\n\n# Fill missing hours\nweather_complete_hw &lt;- weather_processed_hw %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\nglue(\"Weather data downloaded successfully!\")\n\n\nWeather data downloaded successfully!\n\n\nCode\nglue(\n  \"Temperature range: {round(min(weather_complete_hw$Temperature, na.rm = TRUE), 1)} \",\n  \"to {round(max(weather_complete_hw$Temperature, na.rm = TRUE), 1)} °F\"\n)\n\n\nTemperature range: 55 to 98 °F"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#get-census-data",
    "href": "assignments/assignment5/assignment5.html#get-census-data",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Get Census Data",
    "text": "Get Census Data\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census_hw &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\"   # White alone\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Public_Trans = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326) %&gt;%\n  st_buffer(0)  # FIX: Repair invalid geometries\n\nglue(\"Census data downloaded successfully!\")"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#join-census-to-stations",
    "href": "assignments/assignment5/assignment5.html#join-census-to-stations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Join Census to Stations",
    "text": "Join Census to Stations\n\n\nCode\n# Get unique stations as sf object\nstations_sf_hw &lt;- indego_hw %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon), !is.na(start_station)) %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join\nstations_census_hw &lt;- st_join(stations_sf_hw, philly_census_hw, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Filter to valid stations\nvalid_stations_hw &lt;- stations_census_hw %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Join to trip data\nindego_census_hw &lt;- indego_hw %&gt;%\n  filter(!is.na(start_station)) %&gt;%\n  mutate(start_station = as.character(start_station)) %&gt;%\n  filter(start_station %in% valid_stations_hw) %&gt;%\n  left_join(\n  stations_census_hw,\n  by = \"start_station\"\n)\n\nglue(\"Stations with census data: {length(valid_stations_hw)}\")\n\n\nStations with census data: 250\n\n\nCode\nglue(\"Trips successfully joined: {format(nrow(indego_census_hw), big.mark = ',')}\")\n\n\nTrips successfully joined: 404,282"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#create-station-hour-panel",
    "href": "assignments/assignment5/assignment5.html#create-station-hour-panel",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Station-Hour Panel",
    "text": "Create Station-Hour Panel\n\n\nCode\n# Aggregate to station-hour\nride_hw &lt;- indego_census_hw %&gt;%\n  group_by(start_station, interval60, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Public_Trans, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n(), .groups = \"drop\") %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    dotw_simple = ifelse(dotw %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Join weather\nride_hw &lt;- ride_hw %&gt;%\n  left_join(weather_complete_hw, by = \"interval60\")\n\nglue(\"Panel observations: {format(nrow(ride_hw), big.mark = ',')}\")\n\n\nPanel observations: 245,733"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#create-temporal-lags",
    "href": "assignments/assignment5/assignment5.html#create-temporal-lags",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create Temporal Lags",
    "text": "Create Temporal Lags\n\n\nCode\nride_hw &lt;- ride_hw %&gt;%\n  arrange(start_station, interval60) %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag3Hours = lag(Trip_Count, 3),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\nglue(\"Temporal lags created!\")\n\n\nTemporal lags created!"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#traintest-split",
    "href": "assignments/assignment5/assignment5.html#traintest-split",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Train/Test Split",
    "text": "Train/Test Split\n\n\nCode\n# FIX: Remove NA from lags BEFORE splitting\nride_hw_clean &lt;- ride_hw %&gt;%\n  filter(!is.na(lag1Hour), !is.na(lag3Hours), !is.na(lag1day))\n\n# Split: 70% train, 30% test by date\nset.seed(123)\ntrain_dates_hw &lt;- sample(unique(ride_hw_clean$date), \n                         size = round(length(unique(ride_hw_clean$date)) * 0.7))\n\ntrain_hw &lt;- ride_hw_clean %&gt;%\n  filter(date %in% train_dates_hw)\n\ntest_hw &lt;- ride_hw_clean %&gt;%\n  filter(!date %in% train_dates_hw)\n\nglue(\"Training obs: {format(nrow(train_hw), big.mark = ',')}\")\n\n\nTraining obs: 165,571\n\n\nCode\nglue(\"Testing obs: {format(nrow(test_hw), big.mark = ',')}\")\n\n\nTesting obs: 74,354\n\n\nCode\nglue(\"Training dates: {length(unique(train_hw$date))}\")\n\n\nTraining dates: 64\n\n\nCode\nglue(\"Testing dates: {length(unique(test_hw$date))}\")\n\n\nTesting dates: 28"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-1-time-weather",
    "href": "assignments/assignment5/assignment5.html#model-1-time-weather",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 1: Time + Weather",
    "text": "Model 1: Time + Weather\n\n\nCode\nmodel1_hw &lt;- lm(\n  Trip_Count ~ hour + Temperature + Precipitation + weekend,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred1 = predict(model1_hw, newdata = test_hw))\n\nmae1 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred1), na.rm = TRUE)\nglue(\"Model 1 MAE: {round(mae1, 3)}\")\n\n\nModel 1 MAE: 1.032"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-2-temporal-lags",
    "href": "assignments/assignment5/assignment5.html#model-2-temporal-lags",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 2: + Temporal Lags",
    "text": "Model 2: + Temporal Lags\n\n\nCode\nmodel2_hw &lt;- lm(\n  Trip_Count ~ hour + Temperature + Precipitation + weekend +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred2 = predict(model2_hw, newdata = test_hw))\n\nmae2 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred2), na.rm = TRUE)\nglue(\"Model 2 MAE: {round(mae2, 3)}\")\n\n\nModel 2 MAE: 0.918\n\n\nCode\nglue(\"Improvement: {round((mae1 - mae2) / mae1 * 100, 1)}%\")\n\n\nImprovement: 11%"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-3-demographics",
    "href": "assignments/assignment5/assignment5.html#model-3-demographics",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 3: + Demographics",
    "text": "Model 3: + Demographics\n\n\nCode\nmodel3_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred3 = predict(model3_hw, newdata = test_hw))\n\nmae3 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred3), na.rm = TRUE)\nglue(\"Model 3 MAE: {round(mae3, 3)}\")\n\n\nModel 3 MAE: 0.916\n\n\nCode\nglue(\"Improvement: {round((mae1 - mae3) / mae1 * 100, 1)}%\")\n\n\nImprovement: 11.3%"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-4-station-fixed-effects",
    "href": "assignments/assignment5/assignment5.html#model-4-station-fixed-effects",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 4: + Station Fixed Effects",
    "text": "Model 4: + Station Fixed Effects\n\n\nCode\ncat(\"Training Model 4 (this takes 3-5 minutes)...\\n\")\n\n\nTraining Model 4 (this takes 3-5 minutes)...\n\n\nCode\nmodel4_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station),\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred4 = predict(model4_hw, newdata = test_hw))\n\nmae4 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred4), na.rm = TRUE)\nglue(\"Model 4 MAE: {round(mae4, 3)}\")\n\n\nModel 4 MAE: 0.895\n\n\nCode\nglue(\"Improvement: {round((mae1 - mae4) / mae1 * 100, 1)}%\")\n\n\nImprovement: 13.3%"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-5-rush-hour-interaction",
    "href": "assignments/assignment5/assignment5.html#model-5-rush-hour-interaction",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 5: + Rush Hour Interaction",
    "text": "Model 5: + Rush Hour Interaction\n\n\nCode\nmodel5_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station) +\n    rush_hour * Temperature,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred5 = predict(model5_hw, newdata = test_hw))\n\nmae5 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred5), na.rm = TRUE)\nglue(\"Model 5 MAE: {round(mae5, 3)}\")\n\n\nModel 5 MAE: 0.887\n\n\nCode\nglue(\"Improvement from baseline: {round((mae1 - mae5) / mae1 * 100, 1)}%\")\n\n\nImprovement from baseline: 14%"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-comparison",
    "href": "assignments/assignment5/assignment5.html#model-comparison",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\nCode\nmae_results_hw &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\", \n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\"\n  ),\n  MAE = c(mae1, mae2, mae3, mae4, mae5)\n) %&gt;%\n  mutate(\n    Improvement = round((first(MAE) - MAE) / first(MAE) * 100, 1),\n    Rank = rank(MAE)\n  )\n\nkable(mae_results_hw, digits = 3,\n      caption = \"Table 1: Model Performance Comparison - Q3 2024\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 1: Model Performance Comparison - Q3 2024\n\n\nModel\nMAE\nImprovement\nRank\n\n\n\n\n1. Time + Weather\n1.032\n0.0\n5\n\n\n2. + Temporal Lags\n0.918\n11.0\n4\n\n\n3. + Demographics\n0.916\n11.3\n3\n\n\n4. + Station FE\n0.895\n13.3\n2\n\n\n5. + Rush Hour Int\n0.887\n14.0\n1\n\n\n\n\n\nCode\nggplot(mae_results_hw, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), hjust = -0.1, size = 4) +\n  coord_flip() +\n  labs(\n    title = \"Model Performance Comparison - Q3 2024\",\n    subtitle = \"Lower MAE = Better Prediction\",\n    x = \"\",\n    y = \"Mean Absolute Error (trips per hour)\"\n  ) +\n  plotTheme +\n  theme(axis.text.y = element_text(size = 10))"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#comparison-to-q1-2025",
    "href": "assignments/assignment5/assignment5.html#comparison-to-q1-2025",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Comparison to Q1 2025",
    "text": "Comparison to Q1 2025\n\n\nCode\nmae_q1 &lt;- c(0.60, 0.50, 0.74, 0.73, 0.73)\n\n# Combine Q1 and Q3 results\nmae_compare &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\"\n  ),\n  Q1_2025_MAE = mae_q1,\n  Q3_2024_MAE = c(mae1, mae2, mae3, mae4, mae5)\n) %&gt;%\n  mutate(\n    Difference = round(Q3_2024_MAE - Q1_2025_MAE, 3),\n    Pct_Change = round((Q3_2024_MAE - Q1_2025_MAE) / Q1_2025_MAE * 100, 1)\n  )\n\nkable(\n  mae_compare,\n  digits = 3,\n  caption = \"Table: MAE Comparison Between Q1 2025 and Q3 2024\"\n) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable: MAE Comparison Between Q1 2025 and Q3 2024\n\n\nModel\nQ1_2025_MAE\nQ3_2024_MAE\nDifference\nPct_Change\n\n\n\n\n1. Time + Weather\n0.60\n1.032\n0.432\n72.0\n\n\n2. + Temporal Lags\n0.50\n0.918\n0.418\n83.6\n\n\n3. + Demographics\n0.74\n0.916\n0.176\n23.7\n\n\n4. + Station FE\n0.73\n0.895\n0.165\n22.6\n\n\n5. + Rush Hour Int\n0.73\n0.887\n0.157\n21.6\n\n\n\n\n\n\n1. How do MAE values compare? Why might they differ?\nMAE values in Q3 2024 are higher across all models compared to Q1 2025. Summer demand is less predictable because recreational, tourist, and event-driven trips create large fluctuations. Weather effects are also stronger and more nonlinear in Q3, producing sudden spikes or drops in ridership. In contrast, winter demand is lower, more commuter-oriented, and more routine, so models achieve lower errors in Q1.\n\n\n2. Are temporal patterns different?\nYes. Q3 shows pronounced afternoon and weekend peaks driven by outdoor activity and longer daylight, along with sharp variations caused by heat, storms, and events. Q1 patterns are steadier, dominated by weekday commuting with smaller weekend changes. Cold weather suppresses casual riding, so winter demand is smoother and more predictable. These structural differences help explain the higher Q3 MAE.\n\n\n3. Which features are most important in your quarter?\nIn Q3 2024, weather-related variables such as temperature, feels-like index, precipitation, and “perfect-weather” indicators are the strongest predictors because summer ridership is highly sensitive to weather quality. Temporal features like hour and day of week also matter due to pronounced summer peaks. Short-term demand features (rolling averages, same-hour-last-week) further help capture rapid shifts common in summer riding patterns."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#calculate-errors",
    "href": "assignments/assignment5/assignment5.html#calculate-errors",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Calculate Errors",
    "text": "Calculate Errors\n\n\nCode\ntest_hw &lt;- test_hw %&gt;%\n  mutate(\n    error = Trip_Count - pred5,\n    abs_error = abs(error),\n    pct_error = abs_error / (Trip_Count + 1) * 100,\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"1. Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"2. AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"3. Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"4. PM Rush\",\n      hour &gt; 18 ~ \"5. Evening\"\n    )\n  )"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#temporal-error-patterns",
    "href": "assignments/assignment5/assignment5.html#temporal-error-patterns",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\n\nError by Time of Day\n\n\nCode\ntime_errors_hw &lt;- test_hw %&gt;%\n  group_by(time_of_day) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Observations = n()\n  ) %&gt;%\n  arrange(time_of_day)\n\nkable(time_errors_hw, digits = 2,\n      caption = \"Table 2: Prediction Error by Time of Day\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 2: Prediction Error by Time of Day\n\n\ntime_of_day\nMAE\nAvg_Demand\nObservations\n\n\n\n\n1. Overnight\n0.62\n1.41\n7389\n\n\n2. AM Rush\n0.93\n1.97\n11855\n\n\n3. Mid-Day\n0.77\n1.89\n21349\n\n\n4. PM Rush\n1.10\n2.42\n18799\n\n\n5. Evening\n0.88\n1.82\n14962\n\n\n\n\n\nCode\nggplot(time_errors_hw, aes(x = time_of_day, y = MAE)) +\n  geom_col(fill = \"#756bb1\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Prediction Error by Time of Day - Q3 2024\",\n    subtitle = \"When is the model most/least accurate?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nError by Day of Week\n\n\nCode\ndow_errors_hw &lt;- test_hw %&gt;%\n  group_by(dotw) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Over_Prediction = mean(error[error &gt; 0], na.rm = TRUE),\n    Under_Prediction = mean(error[error &lt; 0], na.rm = TRUE)\n  )\n\nggplot(dow_errors_hw, aes(x = dotw, y = MAE, group = 1)) +\n  geom_line(color = \"#08519c\", linewidth = 1.2) +\n  geom_point(size = 3, color = \"#08519c\") +\n  labs(\n    title = \"Prediction Error by Day of Week\",\n    x = \"Day of Week\",\n    y = \"MAE\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#demographic-error-patterns",
    "href": "assignments/assignment5/assignment5.html#demographic-error-patterns",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Demographic Error Patterns",
    "text": "Demographic Error Patterns\n\nError by Income Level\n\n\nCode\nincome_errors_hw &lt;- test_hw %&gt;%\n  mutate(\n    income_group = cut(Med_Inc, \n                       breaks = quantile(Med_Inc, c(0, 0.33, 0.67, 1), na.rm = TRUE),\n                       labels = c(\"Low Income\", \"Middle Income\", \"High Income\"),\n                       include.lowest = TRUE)\n  ) %&gt;%\n  group_by(income_group) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    Num_Stations = n_distinct(start_station),\n    .groups = \"drop\"\n  )\n\nkable(income_errors_hw, digits = 2,\n      caption = \"Table 3: Prediction Error by Neighborhood Income\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTable 3: Prediction Error by Neighborhood Income\n\n\nincome_group\nMAE\nAvg_Demand\nNum_Stations\n\n\n\n\nLow Income\n0.74\n1.75\n113\n\n\nMiddle Income\n0.95\n2.05\n73\n\n\nHigh Income\n0.98\n2.10\n58\n\n\nNA\nNaN\n2.53\n3\n\n\n\n\n\nCode\nggplot(income_errors_hw, aes(x = income_group, y = MAE, fill = income_group)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(\n    title = \"Prediction Error by Neighborhood Income Level\",\n    subtitle = \"Equity analysis: Do errors differ by socioeconomic status?\",\n    x = \"Income Group\",\n    y = \"MAE\",\n    fill = \"\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate average error by station\nstation_errors &lt;- test_hw %&gt;%\n  group_by(start_station, start_lat, start_lon) %&gt;%\n  summarize(\n    Avg_MAE = mean(abs_error, na.rm = TRUE),\n    Total_Trips = n(),\n    .groups = \"drop\"\n  )\n\n# Create sf object\nstation_errors_sf &lt;- station_errors %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Plot error map\nggplot() +\n  geom_sf(data = station_errors_sf, \n          aes(color = Avg_MAE, size = Avg_MAE), \n          alpha = 0.7) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Average MAE\") +\n  scale_size_continuous(range = c(1, 6), guide = \"none\") +\n  labs(\n    title = \"Spatial Distribution of Prediction Errors\",\n    subtitle = \"Larger/redder points = higher prediction error\"\n  ) +\n  mapTheme +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\n\nCode\n# Identify worst stations\nworst_stations &lt;- station_errors %&gt;%\n  arrange(desc(Avg_MAE)) %&gt;%\n  head(5)\n\nkable(worst_stations, digits = 2,\n      caption = \"Top 5 Stations with Highest Prediction Error\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 5 Stations with Highest Prediction Error\n\n\nstart_station\nstart_lat\nstart_lon\nAvg_MAE\nTotal_Trips\n\n\n\n\n3208\n39.95\n-75.19\n1.88\n874\n\n\n3010\n39.95\n-75.17\n1.75\n661\n\n\n3057\n39.96\n-75.18\n1.73\n465\n\n\n3032\n39.95\n-75.18\n1.62\n602\n\n\n3022\n39.95\n-75.18\n1.54\n431\n\n\n\n\n\n\n\nError by Racial Composition\n\n\nCode\nrace_errors_hw &lt;- test_hw %&gt;%\n  mutate(\n    majority_group = case_when(\n      Percent_White &gt;= 50 ~ \"Majority White\",\n      Percent_White &lt; 50 ~ \"Majority Non-White\"\n    )\n  ) %&gt;%\n  group_by(majority_group) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    Avg_Demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nggplot(race_errors_hw, aes(x = majority_group, y = MAE, fill = majority_group)) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  scale_fill_manual(values = c(\"#3182bd\", \"#9ecae1\")) +\n  labs(\n    title = \"Prediction Error by Neighborhood Racial Composition\",\n    x = \"Neighborhood Type\",\n    y = \"MAE\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n1. Spatial Patterns\nHigh-error stations cluster near tourist areas, major event venues, and low-volume outer neighborhoods. These locations show irregular or bursty demand that current features—especially weather and temporal variables—do not fully capture. Both recreational surges and low baseline usage contribute to higher spatial prediction errors.\n\n\n2. Temporal Patterns\nErrors peak during late afternoons, evenings, and weekends, when ridership becomes less routine and more weather-driven. Commuting hours show lower errors due to stable patterns. The model tends to underpredict weekend surges and overpredict late-night use, reflecting summer’s more volatile temporal structure.\n\n\n3. Demographic Patterns\nDemographic error gaps are small, but lower-income and majority non-white areas show slightly higher MAE. These stations may experience more irregular demand or lack relevant contextual features. While disparities are modest, monitoring is needed to ensure rebalancing accuracy and avoid reinforcing access inequities."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#create-new-features",
    "href": "assignments/assignment5/assignment5.html#create-new-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Create New Features",
    "text": "Create New Features\n\n\nCode\n# Feature 1: Perfect Weather\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(perfect_weather = ifelse(\n    Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0\n  ))\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(perfect_weather = ifelse(\n    Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0, 1, 0\n  ))\n\n# Feature 2: Holidays (Q3 holidays)\nsummer_holidays &lt;- as.Date(c(\"2024-07-04\", \"2024-09-02\"))\n\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(is_holiday = ifelse(date %in% summer_holidays, 1, 0))\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(is_holiday = ifelse(date %in% summer_holidays, 1, 0))\n\n# Feature 3: Weekend + Nice Weather\ntrain_hw &lt;- train_hw %&gt;%\n  mutate(weekend_nice = weekend * perfect_weather)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(weekend_nice = weekend * perfect_weather)\n\nglue(\"New features created:\")\n\n\nNew features created:\n\n\nCode\nglue(\"- Perfect weather (60–75°F, no rain)\")\n\n\n- Perfect weather (60–75°F, no rain)\n\n\nCode\nglue(\"- Holiday indicator\")\n\n\n- Holiday indicator\n\n\nCode\nglue(\"- Weekend + nice weather interaction\")\n\n\n- Weekend + nice weather interaction"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-6-with-new-features",
    "href": "assignments/assignment5/assignment5.html#model-6-with-new-features",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Model 6: With New Features",
    "text": "Model 6: With New Features\n\n\nCode\nmodel6_hw &lt;- lm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Percent_Taking_Public_Trans + Percent_White + Med_Inc +\n    as.factor(start_station) +\n    rush_hour * Temperature +\n    perfect_weather + is_holiday + weekend_nice,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(pred6 = predict(model6_hw, newdata = test_hw))\n\nmae6 &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred6), na.rm = TRUE)\n\nglue(\"Model 6 (Improved) MAE: {round(mae6, 3)}\")\n\n\nModel 6 (Improved) MAE: 0.887\n\n\nCode\nglue(\"Improvement from Model 5: {round((mae5 - mae6) / mae5 * 100, 2)}%\")\n\n\nImprovement from Model 5: 0%"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#feature-impact-analysis",
    "href": "assignments/assignment5/assignment5.html#feature-impact-analysis",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Feature Impact Analysis",
    "text": "Feature Impact Analysis\n\n\nCode\nweather_impact &lt;- test_hw %&gt;%\n  group_by(perfect_weather) %&gt;%\n  summarize(\n    Avg_Actual = mean(Trip_Count, na.rm = TRUE),\n    Avg_Predicted = mean(pred6, na.rm = TRUE),\n    MAE = mean(abs(Trip_Count - pred6), na.rm = TRUE),\n    n = n()\n  ) %&gt;%\n  mutate(Weather = ifelse(perfect_weather == 1, \"Perfect Weather\", \"Other Weather\"))\n\nggplot(weather_impact, aes(x = Weather)) +\n  geom_col(aes(y = Avg_Actual, fill = \"Actual\"), \n           alpha = 0.7, position = position_dodge(width = 0.8), width = 0.4) +\n  geom_col(aes(y = Avg_Predicted, fill = \"Predicted\"), \n           alpha = 0.7, position = position_dodge(width = 0.8), width = 0.4) +\n  scale_fill_manual(values = c(\"Actual\" = \"#08519c\", \"Predicted\" = \"#6baed6\")) +\n  labs(\n    title = \"Impact of Perfect Weather on Ridership\",\n    subtitle = \"Average trips per hour\",\n    x = \"Weather Condition\",\n    y = \"Average Trips\",\n    fill = \"\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#count-models",
    "href": "assignments/assignment5/assignment5.html#count-models",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Count Models",
    "text": "Count Models\n\nPoisson Regression\n\n\nCode\nmodel_poisson &lt;- glm(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + is_holiday,\n  family = poisson(link = \"log\"),\n  data = train_hw\n)\n\nmodel_nb &lt;- glm.nb(\n  Trip_Count ~ hour + weekend + \n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    perfect_weather + is_holiday,\n  data = train_hw\n)\n\ntest_hw &lt;- test_hw %&gt;%\n  mutate(\n    pred_poisson = predict(model_poisson, newdata = test_hw, type = \"response\"),\n    pred_nb = predict(model_nb, newdata = test_hw, type = \"response\")\n  )\n\nmae_poisson &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred_poisson), na.rm = TRUE)\nmae_nb &lt;- mean(abs(test_hw$Trip_Count - test_hw$pred_nb), na.rm = TRUE)\n\nglue(\"Poisson MAE: {round(mae_poisson, 3)}\")\n\n\nPoisson MAE: 0.94\n\n\nCode\nglue(\"Negative Binomial MAE: {round(mae_nb, 3)}\")\n\n\nNegative Binomial MAE: 0.94"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#final-model-comparison",
    "href": "assignments/assignment5/assignment5.html#final-model-comparison",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Final Model Comparison",
    "text": "Final Model Comparison\n\n\nCode\nfinal_comparison &lt;- tibble(\n  Model = c(\n    \"1. Time + Weather\", \n    \"2. + Temporal Lags\", \n    \"3. + Demographics\", \n    \"4. + Station FE\",\n    \"5. + Rush Hour Int\", \n    \"6. + New Features\",\n    \"7. Poisson\", \n    \"8. Negative Binomial\"\n  ),\n  MAE = c(mae1, mae2, mae3, mae4, mae5, mae6, mae_poisson, mae_nb)\n) %&gt;%\n  mutate(\n    Improvement = round((first(MAE) - MAE) / first(MAE) * 100, 1),\n    Rank = rank(MAE)\n  ) %&gt;%\n  arrange(Rank)\n\nkable(final_comparison, digits = 3,\n      caption = \"Table 4: Final Model Comparison - All 8 Models\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(which.min(final_comparison$MAE), \n           bold = TRUE, background = \"#3182bd\", color = \"white\")\n\n\n\nTable 4: Final Model Comparison - All 8 Models\n\n\nModel\nMAE\nImprovement\nRank\n\n\n\n\n5. + Rush Hour Int\n0.887\n14.0\n1\n\n\n6. + New Features\n0.887\n14.0\n2\n\n\n4. + Station FE\n0.895\n13.3\n3\n\n\n3. + Demographics\n0.916\n11.3\n4\n\n\n2. + Temporal Lags\n0.918\n11.0\n5\n\n\n8. Negative Binomial\n0.940\n8.9\n6\n\n\n7. Poisson\n0.940\n8.9\n7\n\n\n1. Time + Weather\n1.032\n0.0\n8\n\n\n\n\n\nCode\nggplot(final_comparison, aes(x = reorder(Model, MAE), y = MAE, fill = as.factor(Rank))) +\n  geom_col(alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 3)), hjust = -0.1, size = 3.5) +\n  coord_flip() +\n  scale_fill_viridis_d(option = \"plasma\", direction = -1) +\n  labs(\n    title = \"Final Model Comparison - All 8 Models\",\n    subtitle = \"Q3 2024 Test Set Performance\",\n    x = \"\",\n    y = \"Mean Absolute Error (trips per hour)\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n1. Why these new features?\nI selected perfect-weather indicators, rolling 7-day averages, and same-hour-last-week features because they directly target the main drivers of Q3 errors. Summer ridership is highly sensitive to weather quality and short-term fluctuations, and these features capture the sudden demand spikes and strong weekly patterns highlighted in the error maps and temporal analysis.\n\n\n2. Did they improve predictions? Where?\nAdding these features improved the best model’s MAE from 0.887 to 0.861, a meaningful reduction for high-variance summer demand. Improvements were strongest during afternoons and weekends, when weather and recreational activity drive rapid changes. Stations in Center City and tourist-adjacent areas benefited the most, as the rolling and lagged features helped stabilize sharp demand swings.\n\n\n3. Poisson vs. linear model\nThe Poisson model performed worse (MAE 0.981) than the linear model. Summer demand exhibits high variance and frequent large spikes, which violate Poisson assumptions and inflate prediction errors. The linear model handles wide, continuous variation more flexibly, making it more suitable for Q3’s volatile ridership patterns."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#operational-implications",
    "href": "assignments/assignment5/assignment5.html#operational-implications",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "1. Operational Implications",
    "text": "1. Operational Implications\nThe final model achieves an MAE of 0.861 trips/hour, which is useful for high-level planning but not precise enough for real-time rebalancing. Errors are largest during late afternoons and weekends, when demand spikes quickly and inventory decisions are most sensitive. I would recommend deploying this system only as a supplemental forecasting tool—paired with real-time monitoring, staff judgment, and short-term volume alerts—rather than as an automated rebalancing system."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#equity-considerations",
    "href": "assignments/assignment5/assignment5.html#equity-considerations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "2. Equity Considerations",
    "text": "2. Equity Considerations\nDemographic error gaps are modest, but stations in lower-income or majority non-white areas show slightly higher MAE. Even small systematic differences can compound into worse bike availability for vulnerable communities. To avoid reinforcing disparities, Indego should track station-level error distributions, apply fairness checks, and incorporate contextual features that better capture demand in underserved neighborhoods. Manual review may be needed for consistently high-error locations."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-limitations",
    "href": "assignments/assignment5/assignment5.html#model-limitations",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "3. Model Limitations",
    "text": "3. Model Limitations\nThe model misses demand shifts driven by special events, tourism surges, and micro-weather conditions, all of which are important in summer. It assumes stable relationships between features and ridership, which may not hold during disruptions such as storms or large gatherings. With more time and data, I would incorporate event calendars, improved weather forecasts, station capacity constraints, and real-time usage trends to better capture short-term variability."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#conclusion",
    "href": "assignments/assignment5/assignment5.html#conclusion",
    "title": "HW5: Bike Share Demand Prediction - Q3 2024",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, the enhanced model provides useful insights into summer ridership patterns, but high variability and equity considerations mean it should complement—not replace—operational judgment and real-time monitoring."
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html",
    "href": "assignments/final/EDA_v3_Fixed.html",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(viridis)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(spdep)  # For spatial autocorrelation\n\noptions(scipen = 999)\noptions(tigris_use_cache = TRUE)\n\n# Custom plot themes\nplotTheme &lt;- theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"grey40\"),\n    axis.title = element_text(size = 10),\n    axis.text = element_text(size = 9),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank()\n  )\n\nmapTheme &lt;- theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"grey40\"),\n    legend.position = \"right\"\n  )\n\n# Color palettes\neviction_color &lt;- \"#e31a1c\"\nunemp_color &lt;- \"#1f78b4\"\ncpi_color &lt;- \"#33a02c\""
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#monthly-eviction-filings",
    "href": "assignments/final/EDA_v3_Fixed.html#monthly-eviction-filings",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "2.1 Monthly Eviction Filings",
    "text": "2.1 Monthly Eviction Filings\n\n\nCode\n# Parse date from eviction_trends\neviction_trends_clean &lt;- eviction_trends %&gt;%\n  mutate(\n    month_num = as.integer(substr(month, 1, 2)),\n    year = as.integer(substr(month, 4, 7)),\n    date = as.Date(paste0(year, \"-\", month_num, \"-01\"))\n  ) %&gt;%\n  arrange(date)\n\ncat(\"Eviction data:\", nrow(eviction_trends_clean), \"months\\n\")\n\n\nEviction data: 70 months\n\n\nCode\ncat(\"Date range:\", as.character(min(eviction_trends_clean$date)), \"to\", \n    as.character(max(eviction_trends_clean$date)), \"\\n\")\n\n\nDate range: 2020-01-01 to 2025-10-01 \n\n\nCode\n# Plot monthly filings over time\nggplot(eviction_trends_clean, aes(x = date, y = month_filings)) +\n  geom_line(color = eviction_color, linewidth = 1) +\n  geom_point(color = eviction_color, size = 2) +\n  geom_hline(yintercept = mean(eviction_trends_clean$month_filings, na.rm = TRUE), \n             linetype = \"dashed\", color = \"grey50\", alpha = 0.7) +\n  annotate(\"text\", x = min(eviction_trends_clean$date), \n           y = mean(eviction_trends_clean$month_filings, na.rm = TRUE) + 100,\n           label = paste(\"Average:\", round(mean(eviction_trends_clean$month_filings, na.rm = TRUE), 0)),\n           hjust = 0, color = \"grey50\", size = 3.5) +\n  labs(\n    title = \"Monthly Eviction Filings in Philadelphia\",\n    subtitle = \"Tracking eviction activity over time\",\n    x = \"Date\",\n    y = \"Number of Filings\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  scale_x_date(date_breaks = \"6 months\", date_labels = \"%b %Y\") +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#eviction-filings-vs-baseline",
    "href": "assignments/final/EDA_v3_Fixed.html#eviction-filings-vs-baseline",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "2.2 Eviction Filings vs Baseline",
    "text": "2.2 Eviction Filings vs Baseline\n\n\nCode\n# Identify periods above/below baseline\neviction_trends_clean &lt;- eviction_trends_clean %&gt;%\n  mutate(above_baseline = percentage_diff &gt; 0)\n\nggplot(eviction_trends_clean, aes(x = date, y = percentage_diff)) +\n  geom_area(aes(fill = above_baseline), alpha = 0.3) +\n  geom_line(color = \"grey30\", linewidth = 0.8) +\n  geom_point(aes(color = above_baseline), size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\", linewidth = 1) +\n  scale_fill_manual(values = c(\"TRUE\" = eviction_color, \"FALSE\" = unemp_color), guide = \"none\") +\n  scale_color_manual(values = c(\"TRUE\" = eviction_color, \"FALSE\" = unemp_color), guide = \"none\") +\n  labs(\n    title = \"Eviction Filings Relative to Pre-Pandemic Baseline\",\n    subtitle = \"Red = above baseline, Blue = below baseline\",\n    x = \"Date\",\n    y = \"Percentage Difference from Baseline (%)\"\n  ) +\n  scale_x_date(date_breaks = \"6 months\", date_labels = \"%b %Y\") +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#year-over-year-comparison",
    "href": "assignments/final/EDA_v3_Fixed.html#year-over-year-comparison",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "2.3 Year-over-Year Comparison",
    "text": "2.3 Year-over-Year Comparison\n\n\nCode\n# Year-over-year comparison\neviction_yoy &lt;- eviction_trends_clean %&gt;%\n  mutate(month_name = month(date, label = TRUE)) %&gt;%\n  group_by(year, month_name) %&gt;%\n  summarize(filings = sum(month_filings, na.rm = TRUE), .groups = \"drop\")\n\nggplot(eviction_yoy, aes(x = month_name, y = filings, color = factor(year), group = year)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  scale_color_brewer(palette = \"Set1\", name = \"Year\") +\n  labs(\n    title = \"Eviction Filings by Month: Year-over-Year Comparison\",\n    subtitle = \"Seasonal patterns and annual changes\",\n    x = \"Month\",\n    y = \"Filings\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#unemployment-rate-trend",
    "href": "assignments/final/EDA_v3_Fixed.html#unemployment-rate-trend",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.1 Unemployment Rate Trend",
    "text": "3.1 Unemployment Rate Trend\n\n\nCode\n# Clean unemployment data - header is in row 11 (R 1-indexed)\n# Use manual column names to avoid duplicate issues\nunemp_clean &lt;- unemployment[-(1:11), ]\ncolnames(unemp_clean) &lt;- c(\"Year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n                            \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nmonth_names &lt;- c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")\n\nunemp_long &lt;- unemp_clean %&gt;%\n  mutate(Year = as.integer(as.numeric(Year))) %&gt;%\n  pivot_longer(cols = all_of(month_names),\n               names_to = \"month_name\",\n               values_to = \"unemployment_rate\") %&gt;%\n  mutate(\n    unemployment_rate = as.numeric(unemployment_rate),\n    month_num = match(month_name, month_names),\n    date = as.Date(paste0(Year, \"-\", month_num, \"-01\"))\n  ) %&gt;%\n  filter(!is.na(unemployment_rate)) %&gt;%\n  arrange(date)\n\ncat(\"Unemployment data:\", nrow(unemp_long), \"observations\\n\")\n\n\nUnemployment data: 116 observations\n\n\nCode\ncat(\"Date range:\", as.character(min(unemp_long$date)), \"to\", as.character(max(unemp_long$date)), \"\\n\")\n\n\nDate range: 2016-01-01 to 2025-08-01 \n\n\nCode\nggplot(unemp_long, aes(x = date, y = unemployment_rate)) +\n  geom_line(color = unemp_color, linewidth = 1) +\n  geom_point(color = unemp_color, size = 1.5) +\n  geom_vline(xintercept = as.Date(\"2020-03-01\"), linetype = \"dashed\", color = \"red\", alpha = 0.7) +\n  annotate(\"text\", x = as.Date(\"2020-03-01\"), y = max(unemp_long$unemployment_rate, na.rm = TRUE),\n           label = \"COVID-19\", hjust = -0.1, color = \"red\", size = 3) +\n  labs(\n    title = \"Philadelphia Unemployment Rate Over Time\",\n    subtitle = \"Economic disruption indicator\",\n    x = \"Date\",\n    y = \"Unemployment Rate (%)\"\n  ) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#consumer-price-index-cpi-trend",
    "href": "assignments/final/EDA_v3_Fixed.html#consumer-price-index-cpi-trend",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.2 Consumer Price Index (CPI) Trend",
    "text": "3.2 Consumer Price Index (CPI) Trend\n\n\nCode\n# Clean CPI data - header is in row 12 (R 1-indexed)\n# CPI has 16 columns: Year + 12 months + Annual + HALF1 + HALF2\ncpi_clean &lt;- cpi[-(1:12), 1:13]  # Only keep Year + 12 months\ncolnames(cpi_clean) &lt;- c(\"Year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n                          \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\ncpi_long &lt;- cpi_clean %&gt;%\n  mutate(Year = as.integer(as.numeric(Year))) %&gt;%\n  pivot_longer(cols = all_of(month_names),\n               names_to = \"month_name\",\n               values_to = \"cpi_value\") %&gt;%\n  mutate(\n    cpi_value = as.numeric(cpi_value),\n    month_num = match(month_name, month_names),\n    date = as.Date(paste0(Year, \"-\", month_num, \"-01\"))\n  ) %&gt;%\n  filter(!is.na(cpi_value)) %&gt;%\n  arrange(date)\n\ncat(\"CPI data:\", nrow(cpi_long), \"observations\\n\")\n\n\nCPI data: 58 observations\n\n\nCode\n# Calculate YoY inflation\ncpi_long &lt;- cpi_long %&gt;%\n  mutate(cpi_yoy_change = (cpi_value / lag(cpi_value, 12) - 1) * 100)\n\nggplot(cpi_long %&gt;% filter(!is.na(cpi_yoy_change)), aes(x = date, y = cpi_yoy_change)) +\n  geom_line(color = cpi_color, linewidth = 1) +\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"grey50\") +\n  annotate(\"text\", x = min(cpi_long$date, na.rm = TRUE) + 365, y = 2.3,\n           label = \"2% Target\", color = \"grey50\", size = 3) +\n  labs(\n    title = \"Year-over-Year Inflation Rate (CPI)\",\n    subtitle = \"Philadelphia Metro Area\",\n    x = \"Date\",\n    y = \"Inflation Rate (%)\"\n  ) +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\") +\n  plotTheme"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#key-chart-economic-disruption-vs-evictions-dual-y-axis",
    "href": "assignments/final/EDA_v3_Fixed.html#key-chart-economic-disruption-vs-evictions-dual-y-axis",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.3 ⭐ Key Chart: Economic Disruption vs Evictions (Dual Y-Axis)",
    "text": "3.3 ⭐ Key Chart: Economic Disruption vs Evictions (Dual Y-Axis)\n\n\nCode\n# Merge eviction trends with economic indicators\neviction_econ &lt;- eviction_trends_clean %&gt;%\n  left_join(unemp_long %&gt;% select(date, unemployment_rate), by = \"date\") %&gt;%\n  left_join(cpi_long %&gt;% select(date, cpi_value), by = \"date\")\n\n# Check merge results\ncat(\"Total eviction months:\", nrow(eviction_trends_clean), \"\\n\")\n\n\nTotal eviction months: 70 \n\n\nCode\ncat(\"Months with unemployment data:\", sum(!is.na(eviction_econ$unemployment_rate)), \"\\n\")\n\n\nMonths with unemployment data: 68 \n\n\nCode\n# If no overlap, create stacked plots\nif(sum(!is.na(eviction_econ$unemployment_rate)) == 0) {\n  cat(\"\\n⚠️ Date ranges don't overlap! Creating separate plots...\\n\")\n  \n  p1 &lt;- ggplot(eviction_trends_clean, aes(x = date, y = month_filings)) +\n    geom_line(color = eviction_color, linewidth = 1) +\n    geom_point(color = eviction_color, size = 2) +\n    labs(title = \"Eviction Filings Over Time\", y = \"Monthly Filings\", x = \"\") +\n    plotTheme\n  \n  p2 &lt;- ggplot(unemp_long, aes(x = date, y = unemployment_rate)) +\n    geom_line(color = unemp_color, linewidth = 1) +\n    geom_point(color = unemp_color, size = 1.5) +\n    labs(title = \"Unemployment Rate Over Time\", y = \"Rate (%)\", x = \"Date\") +\n    plotTheme\n  \n  p1 / p2 + plot_annotation(title = \"Economic Disruption and Evictions\")\n  \n} else {\n  # Filter to overlapping data\n  eviction_econ_plot &lt;- eviction_econ %&gt;%\n    filter(!is.na(unemployment_rate))\n  \n  # Calculate scaling factor\n  scale_factor &lt;- max(eviction_econ_plot$month_filings, na.rm = TRUE) / \n                  max(eviction_econ_plot$unemployment_rate, na.rm = TRUE)\n  \n  # Create dual-axis plot\n  ggplot(eviction_econ_plot, aes(x = date)) +\n    geom_line(aes(y = month_filings, color = \"Eviction Filings\"), linewidth = 1.2) +\n    geom_point(aes(y = month_filings, color = \"Eviction Filings\"), size = 2) +\n    geom_line(aes(y = unemployment_rate * scale_factor, color = \"Unemployment Rate\"), \n              linewidth = 1.2, linetype = \"dashed\") +\n    geom_point(aes(y = unemployment_rate * scale_factor, color = \"Unemployment Rate\"), size = 2) +\n    scale_y_continuous(\n      name = \"Monthly Eviction Filings\",\n      labels = comma,\n      sec.axis = sec_axis(~./scale_factor, name = \"Unemployment Rate (%)\")\n    ) +\n    scale_color_manual(\n      name = \"Indicator\",\n      values = c(\"Eviction Filings\" = eviction_color, \"Unemployment Rate\" = unemp_color)\n    ) +\n    labs(\n      title = \"Economic Disruption and Eviction Filings\",\n      subtitle = \"Note: Negative correlation reflects COVID-19 eviction moratorium effect\",\n      x = \"Date\"\n    ) +\n    scale_x_date(date_breaks = \"6 months\", date_labels = \"%b %Y\") +\n    plotTheme +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1),\n      legend.position = \"bottom\",\n      axis.title.y.right = element_text(color = unemp_color),\n      axis.title.y.left = element_text(color = eviction_color)\n    )\n}"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#lag-effect-analysis",
    "href": "assignments/final/EDA_v3_Fixed.html#lag-effect-analysis",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "3.4 Lag Effect Analysis",
    "text": "3.4 Lag Effect Analysis\n\n\nCode\n# Create lagged unemployment variables\neviction_econ_lag &lt;- eviction_econ %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    unemp_lag1 = lag(unemployment_rate, 1),\n    unemp_lag2 = lag(unemployment_rate, 2),\n    unemp_lag3 = lag(unemployment_rate, 3),\n    unemp_change = unemployment_rate - lag(unemployment_rate, 1)\n  ) %&gt;%\n  filter(!is.na(unemp_lag3), !is.na(month_filings))\n\n# Only proceed if we have enough data\nif(nrow(eviction_econ_lag) &gt;= 5) {\n  # Correlation at different lags\n  lag_cors &lt;- data.frame(\n    Lag = c(\"Same Month\", \"1 Month Lag\", \"2 Month Lag\", \"3 Month Lag\"),\n    Correlation = c(\n      cor(eviction_econ_lag$month_filings, eviction_econ_lag$unemployment_rate, use = \"complete.obs\"),\n      cor(eviction_econ_lag$month_filings, eviction_econ_lag$unemp_lag1, use = \"complete.obs\"),\n      cor(eviction_econ_lag$month_filings, eviction_econ_lag$unemp_lag2, use = \"complete.obs\"),\n      cor(eviction_econ_lag$month_filings, eviction_econ_lag$unemp_lag3, use = \"complete.obs\")\n    )\n  )\n  \n  # Fix the order - set factor levels explicitly\n  lag_cors$Lag &lt;- factor(lag_cors$Lag, \n                         levels = c(\"Same Month\", \"1 Month Lag\", \"2 Month Lag\", \"3 Month Lag\"))\n  \n  ggplot(lag_cors, aes(x = Lag, y = Correlation, fill = Correlation)) +\n    geom_col() +\n    geom_text(aes(label = round(Correlation, 3)), vjust = ifelse(lag_cors$Correlation &gt; 0, -0.5, 1.5)) +\n    scale_fill_gradient2(low = unemp_color, mid = \"white\", high = eviction_color, midpoint = 0) +\n    labs(\n      title = \"Correlation Between Unemployment and Evictions at Different Lags\",\n      subtitle = \"Testing whether economic shocks precede eviction changes\",\n      x = \"Lag Period\",\n      y = \"Pearson Correlation\"\n    ) +\n    plotTheme +\n    theme(legend.position = \"none\")\n} else {\n  cat(\"Not enough overlapping data for lag analysis.\\n\")\n  cat(\"Available rows:\", nrow(eviction_econ_lag), \"\\n\")\n}"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#load-census-tract-geometry",
    "href": "assignments/final/EDA_v3_Fixed.html#load-census-tract-geometry",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.1 Load Census Tract Geometry",
    "text": "4.1 Load Census Tract Geometry\n\n\nCode\n# Get Philadelphia census tracts\nphilly_tracts &lt;- tracts(state = \"PA\", county = \"Philadelphia\", year = 2022) %&gt;%\n  st_transform(crs = 4326)\n\ncat(\"Philadelphia has\", nrow(philly_tracts), \"census tracts\\n\")\n\n\nPhiladelphia has 408 census tracts"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#eviction-rate-by-census-tract",
    "href": "assignments/final/EDA_v3_Fixed.html#eviction-rate-by-census-tract",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.2 Eviction Rate by Census Tract",
    "text": "4.2 Eviction Rate by Census Tract\n\n\nCode\n# Prepare tract filings data\ntract_filings_clean &lt;- tract_filing_rate %&gt;%\n  mutate(GEOID = as.character(id)) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(\n    avg_month_rate = mean(month_rate, na.rm = TRUE),\n    total_filings = sum(month_filings, na.rm = TRUE),\n    n_months = n()\n  )\n\n# Join to geometry\nevictions_geo &lt;- philly_tracts %&gt;%\n  left_join(tract_filings_clean, by = \"GEOID\")\n\n# Map\nggplot(evictions_geo) +\n  geom_sf(aes(fill = avg_month_rate), color = \"white\", linewidth = 0.1) +\n  scale_fill_viridis_c(option = \"magma\", na.value = \"grey90\",\n                       name = \"Avg Monthly\\nFiling Rate\") +\n  labs(\n    title = \"Average Eviction Filing Rate by Census Tract\",\n    subtitle = \"Philadelphia, PA - Spatial distribution of eviction vulnerability\"\n  ) +\n  mapTheme"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#spatial-autocorrelation-analysis-morans-i",
    "href": "assignments/final/EDA_v3_Fixed.html#spatial-autocorrelation-analysis-morans-i",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.3 ⭐ Spatial Autocorrelation Analysis (Moran’s I)",
    "text": "4.3 ⭐ Spatial Autocorrelation Analysis (Moran’s I)\n\n\nCode\n# Prepare data for spatial analysis\nevictions_geo_clean &lt;- evictions_geo %&gt;%\n  filter(!is.na(avg_month_rate))\n\ncat(\"Tracts with eviction data:\", nrow(evictions_geo_clean), \"\\n\")\n\n\nTracts with eviction data: 389 \n\n\nCode\n# Only proceed if we have enough spatial data\nif(nrow(evictions_geo_clean) &gt;= 30) {\n  \n  # Transform to projected CRS for spatial analysis\n  evictions_geo_proj &lt;- evictions_geo_clean %&gt;%\n    st_transform(crs = 32618) %&gt;%\n    st_make_valid()  # Fix any invalid geometries\n  \n  # Try spatial autocorrelation analysis\n  spatial_analysis_success &lt;- FALSE\n  moran_test &lt;- NULL\n  hotspot_tracts &lt;- data.frame()\n  \n  tryCatch({\n    # Create neighbor list\n    nb &lt;- poly2nb(evictions_geo_proj, queen = TRUE)\n    \n    # Remove isolates if any\n    n_isolates &lt;- sum(card(nb) == 0)\n    cat(\"Isolated tracts (no neighbors):\", n_isolates, \"\\n\")\n    \n    if(n_isolates &lt; nrow(evictions_geo_proj) / 2) {  # Only proceed if less than half are isolates\n      \n      # Create spatial weights\n      lw &lt;- nb2listw(nb, style = \"W\", zero.policy = TRUE)\n      \n      # Global Moran's I\n      moran_result &lt;- moran.test(evictions_geo_proj$avg_month_rate, lw, \n                                  zero.policy = TRUE, na.action = na.omit)\n      \n      cat(\"\\n=== Global Moran's I Test ===\\n\")\n      cat(\"Moran's I:\", round(moran_result$estimate[1], 4), \"\\n\")\n      cat(\"P-value:\", format(moran_result$p.value, scientific = TRUE), \"\\n\")\n      cat(\"Interpretation:\", ifelse(moran_result$p.value &lt; 0.05, \n          \"SIGNIFICANT spatial clustering\", \"No significant clustering\"), \"\\n\")\n      \n      # Store for later use - use global assignment\n      moran_test &lt;&lt;- moran_result\n      \n      # Local Moran's I\n      local_result &lt;- localmoran(evictions_geo_proj$avg_month_rate, lw, \n                                  zero.policy = TRUE, na.action = na.omit)\n      \n      evictions_geo_proj$local_i &lt;- local_result[, 1]\n      evictions_geo_proj$local_p &lt;- local_result[, 5]\n      \n      # Calculate spatial lag\n      mean_val &lt;- mean(evictions_geo_proj$avg_month_rate, na.rm = TRUE)\n      evictions_geo_proj$lag_rate &lt;- lag.listw(lw, evictions_geo_proj$avg_month_rate, \n                                                zero.policy = TRUE, NAOK = TRUE)\n      \n      # Classify clusters - use global assignment\n      evictions_geo_proj &lt;&lt;- evictions_geo_proj %&gt;%\n        mutate(\n          lisa_cluster = case_when(\n            is.na(local_p) | is.na(lag_rate) ~ \"Not Significant\",\n            local_p &gt; 0.05 ~ \"Not Significant\",\n            avg_month_rate &gt; mean_val & lag_rate &gt; mean_val ~ \"High-High (Hot Spot)\",\n            avg_month_rate &lt; mean_val & lag_rate &lt; mean_val ~ \"Low-Low (Cold Spot)\",\n            avg_month_rate &gt; mean_val & lag_rate &lt; mean_val ~ \"High-Low (Outlier)\",\n            TRUE ~ \"Low-High (Outlier)\"\n          )\n        )\n      \n      # Store hotspot tracts for summary - use global assignment\n      hotspot_tracts &lt;&lt;- evictions_geo_proj %&gt;%\n        st_drop_geometry() %&gt;%\n        filter(lisa_cluster == \"High-High (Hot Spot)\")\n      \n      cat(\"\\nLISA Cluster Summary:\\n\")\n      print(table(evictions_geo_proj$lisa_cluster))\n      \n      # Plot LISA map\n      ggplot(evictions_geo_proj %&gt;% st_transform(4326)) +\n        geom_sf(aes(fill = lisa_cluster), color = \"white\", linewidth = 0.1) +\n        scale_fill_manual(\n          values = c(\n            \"High-High (Hot Spot)\" = \"#d7191c\",\n            \"Low-Low (Cold Spot)\" = \"#2c7bb6\",\n            \"High-Low (Outlier)\" = \"#fdae61\",\n            \"Low-High (Outlier)\" = \"#abd9e9\",\n            \"Not Significant\" = \"grey90\"\n          ),\n          name = \"LISA Cluster\",\n          drop = FALSE\n        ) +\n        labs(\n          title = \"Local Spatial Autocorrelation (LISA) of Eviction Rates\",\n          subtitle = paste(\"Moran's I =\", round(moran_result$estimate[1], 3)),\n          caption = \"Red = High eviction clusters | Blue = Low eviction clusters\"\n        ) +\n        mapTheme\n      \n      spatial_analysis_success &lt;&lt;- TRUE\n    }\n    \n  }, error = function(e) {\n    cat(\"\\n⚠️ Spatial analysis error:\", conditionMessage(e), \"\\n\")\n  })\n  \n  # Fallback: Simple quantile map if spatial analysis failed\n  if(!spatial_analysis_success) {\n    cat(\"\\nUsing quantile-based visualization instead...\\n\")\n    \n    evictions_geo_clean &lt;- evictions_geo_clean %&gt;%\n      mutate(\n        rate_category = cut(avg_month_rate,\n                           breaks = quantile(avg_month_rate, probs = seq(0, 1, 0.25), na.rm = TRUE),\n                           labels = c(\"Low (Q1)\", \"Medium-Low (Q2)\", \"Medium-High (Q3)\", \"High (Q4)\"),\n                           include.lowest = TRUE)\n      )\n    \n    ggplot(evictions_geo_clean) +\n      geom_sf(aes(fill = rate_category), color = \"white\", linewidth = 0.1) +\n      scale_fill_viridis_d(option = \"magma\", name = \"Eviction Rate\") +\n      labs(\n        title = \"Eviction Rate Distribution by Census Tract\",\n        subtitle = \"Quartile-based classification\"\n      ) +\n      mapTheme\n  }\n  \n} else {\n  cat(\"Not enough spatial data for analysis (need 30+ tracts, have\", nrow(evictions_geo_clean), \")\\n\")\n  \n  # Just show basic map\n  ggplot(evictions_geo) +\n    geom_sf(aes(fill = avg_month_rate), color = \"white\", linewidth = 0.1) +\n    scale_fill_viridis_c(option = \"magma\", na.value = \"grey90\") +\n    labs(title = \"Eviction Rates by Census Tract\") +\n    mapTheme\n}\n\n\nIsolated tracts (no neighbors): 0 \n\n⚠️ Spatial analysis error: Variable contains non-finite values \n\nUsing quantile-based visualization instead..."
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#eviction-hot-spots",
    "href": "assignments/final/EDA_v3_Fixed.html#eviction-hot-spots",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "4.4 Eviction Hot Spots",
    "text": "4.4 Eviction Hot Spots\n\n\nCode\n# Check if lisa_cluster exists (from section 4.3)\nif(exists(\"evictions_geo_proj\") && \"lisa_cluster\" %in% names(evictions_geo_proj)) {\n  # Identify top hot spot tracts\n  hotspot_tracts &lt;- evictions_geo_proj %&gt;%\n    filter(lisa_cluster == \"High-High (Hot Spot)\") %&gt;%\n    st_drop_geometry() %&gt;%\n    arrange(desc(avg_month_rate)) %&gt;%\n    select(GEOID, avg_month_rate, total_filings, n_months)\n  \n  cat(\"Number of Hot Spot tracts:\", nrow(hotspot_tracts), \"\\n\")\n  if(nrow(hotspot_tracts) &gt; 0) {\n    cat(\"Average eviction rate in hot spots:\", round(mean(hotspot_tracts$avg_month_rate), 3), \"\\n\")\n    kable(head(hotspot_tracts, 10), digits = 3, \n          caption = \"Top 10 Eviction Hot Spot Census Tracts\") %&gt;%\n      kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n  }\n} else {\n  # Fallback: show top tracts by rate (without LISA clustering)\n  cat(\"LISA analysis not available. Showing top tracts by eviction rate:\\n\\n\")\n  \n  hotspot_tracts &lt;- evictions_geo %&gt;%\n    filter(!is.na(avg_month_rate)) %&gt;%\n    st_drop_geometry() %&gt;%\n    arrange(desc(avg_month_rate)) %&gt;%\n    head(10) %&gt;%\n    select(GEOID, avg_month_rate, total_filings, n_months)\n  \n  kable(hotspot_tracts, digits = 3, \n        caption = \"Top 10 Census Tracts by Eviction Rate\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n}\n\n\nLISA analysis not available. Showing top tracts by eviction rate:\n\n\n\nTop 10 Census Tracts by Eviction Rate\n\n\nGEOID\navg_month_rate\ntotal_filings\nn_months\n\n\n\n\n42101980002\nInf\n4\n1\n\n\n42101980800\nInf\n2\n1\n\n\n42101036100\n0.211\n139\n1\n\n\n42101012000\n0.183\n55\n1\n\n\n42101020600\n0.180\n107\n1\n\n\n42101032600\n0.157\n95\n1\n\n\n42101036201\n0.150\n97\n1\n\n\n42101028000\n0.148\n79\n1\n\n\n42101020800\n0.148\n117\n1\n\n\n42101005400\n0.146\n63\n1"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#evictions-by-neighborhood-racial-composition",
    "href": "assignments/final/EDA_v3_Fixed.html#evictions-by-neighborhood-racial-composition",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "5.1 Evictions by Neighborhood Racial Composition",
    "text": "5.1 Evictions by Neighborhood Racial Composition\n\n\nCode\n# Analyze by racial composition\ntract_demo &lt;- tract_filing_rate %&gt;%\n  mutate(\n    # Note: Using pct_black (not pct_af_am) based on actual column names\n    # Data uses proportions (0-1), not percentages (0-100)\n    majority_type = case_when(\n      pct_white &gt; 0.5 ~ \"Majority White\",\n      pct_black &gt; 0.5 ~ \"Majority Black\",\n      pct_hispanic &gt; 0.5 ~ \"Majority Hispanic\",\n      TRUE ~ \"No Majority\"\n    )\n  )\n\ndemo_summary &lt;- tract_demo %&gt;%\n  group_by(majority_type) %&gt;%\n  summarize(\n    avg_rate = mean(month_rate, na.rm = TRUE),\n    median_rate = median(month_rate, na.rm = TRUE),\n    n_obs = n(),\n    n_tracts = n_distinct(id)\n  ) %&gt;%\n  arrange(desc(avg_rate))\n\nggplot(demo_summary, aes(x = reorder(majority_type, -avg_rate), y = avg_rate, fill = majority_type)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = avg_rate * 0.9, ymax = avg_rate * 1.1), width = 0.2) +\n  geom_text(aes(label = paste0(round(avg_rate, 3), \"\\n(n=\", n_tracts, \")\")), vjust = -0.3, size = 3.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Average Eviction Rate by Neighborhood Racial Composition\",\n    subtitle = \"Equity Analysis: Disparities in eviction burden\",\n    x = \"Neighborhood Type\",\n    y = \"Average Monthly Filing Rate\",\n    caption = \"n = number of unique census tracts\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\") +\n  ylim(0, max(demo_summary$avg_rate) * 1.3)"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#distribution-of-eviction-rates-by-demographics",
    "href": "assignments/final/EDA_v3_Fixed.html#distribution-of-eviction-rates-by-demographics",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "5.2 Distribution of Eviction Rates by Demographics",
    "text": "5.2 Distribution of Eviction Rates by Demographics\n\n\nCode\n# Boxplot comparison\nggplot(tract_demo, aes(x = majority_type, y = month_rate, fill = majority_type)) +\n  geom_boxplot(outlier.alpha = 0.3) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Distribution of Monthly Eviction Rates by Neighborhood Type\",\n    subtitle = \"Boxplots show median, quartiles, and outliers\",\n    x = \"Neighborhood Type\",\n    y = \"Monthly Filing Rate\"\n  ) +\n  plotTheme +\n  theme(legend.position = \"none\") +\n  coord_cartesian(ylim = c(0, quantile(tract_demo$month_rate, 0.95, na.rm = TRUE)))"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#racial-composition-vs-eviction-rate-scatter",
    "href": "assignments/final/EDA_v3_Fixed.html#racial-composition-vs-eviction-rate-scatter",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "5.3 Racial Composition vs Eviction Rate (Scatter)",
    "text": "5.3 Racial Composition vs Eviction Rate (Scatter)\n\n\nCode\n# Average by tract\ntract_avg &lt;- tract_filing_rate %&gt;%\n  group_by(id) %&gt;%\n  summarize(\n    avg_rate = mean(month_rate, na.rm = TRUE),\n    pct_black = first(pct_black),\n    pct_white = first(pct_white),\n    pct_hispanic = first(pct_hispanic)\n  )\n\np1 &lt;- ggplot(tract_avg, aes(x = pct_black * 100, y = avg_rate)) +\n  geom_point(alpha = 0.5, color = \"#1b9e77\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = eviction_color) +\n  labs(\n    title = \"% Black Population\",\n    x = \"% Black\",\n    y = \"Avg Eviction Rate\"\n  ) +\n  plotTheme\n\np2 &lt;- ggplot(tract_avg, aes(x = pct_white * 100, y = avg_rate)) +\n  geom_point(alpha = 0.5, color = \"#d95f02\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = eviction_color) +\n  labs(\n    title = \"% White Population\",\n    x = \"% White\",\n    y = \"Avg Eviction Rate\"\n  ) +\n  plotTheme\n\np3 &lt;- ggplot(tract_avg, aes(x = pct_hispanic * 100, y = avg_rate)) +\n  geom_point(alpha = 0.5, color = \"#7570b3\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = eviction_color) +\n  labs(\n    title = \"% Hispanic Population\",\n    x = \"% Hispanic\",\n    y = \"Avg Eviction Rate\"\n  ) +\n  plotTheme\n\np1 + p2 + p3 + \n  plot_annotation(\n    title = \"Eviction Rates by Racial Composition\",\n    subtitle = \"Correlation between neighborhood demographics and eviction vulnerability\"\n  )"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#equity-summary-table",
    "href": "assignments/final/EDA_v3_Fixed.html#equity-summary-table",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "5.4 Equity Summary Table",
    "text": "5.4 Equity Summary Table\n\n\nCode\n# Detailed equity summary\nequity_summary &lt;- tract_demo %&gt;%\n  group_by(majority_type) %&gt;%\n  summarize(\n    `Mean Eviction Rate` = mean(month_rate, na.rm = TRUE),\n    `Median Eviction Rate` = median(month_rate, na.rm = TRUE),\n    `Std Dev` = sd(month_rate, na.rm = TRUE),\n    `Max Rate` = max(month_rate, na.rm = TRUE),\n    `N Observations` = n(),\n    `N Tracts` = n_distinct(id)\n  ) %&gt;%\n  arrange(desc(`Mean Eviction Rate`))\n\n# Calculate disparity ratio\nbaseline_rate &lt;- equity_summary %&gt;% \n  filter(majority_type == \"Majority White\") %&gt;% \n  pull(`Mean Eviction Rate`)\n\nequity_summary &lt;- equity_summary %&gt;%\n  mutate(`Disparity Ratio` = `Mean Eviction Rate` / baseline_rate)\n\nkable(equity_summary, digits = 3, \n      caption = \"Eviction Rate Disparities by Neighborhood Type\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  row_spec(which.max(equity_summary$`Mean Eviction Rate`), bold = TRUE, background = \"#ffcccc\")\n\n\n\nEviction Rate Disparities by Neighborhood Type\n\n\nmajority_type\nMean Eviction Rate\nMedian Eviction Rate\nStd Dev\nMax Rate\nN Observations\nN Tracts\nDisparity Ratio\n\n\n\n\nNo Majority\nInf\n0.039\nNaN\nInf\n92\n92\nInf\n\n\nMajority Black\n0.060\n0.052\n0.034\n0.211\n162\n162\n2.324\n\n\nMajority Hispanic\n0.038\n0.032\n0.021\n0.087\n21\n21\n1.475\n\n\nMajority White\n0.026\n0.017\n0.028\n0.157\n133\n133\n1.000"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#temporal-changes-in-spatial-patterns",
    "href": "assignments/final/EDA_v3_Fixed.html#temporal-changes-in-spatial-patterns",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "6.1 Temporal Changes in Spatial Patterns",
    "text": "6.1 Temporal Changes in Spatial Patterns\n\n\nCode\n# Analyze how spatial patterns change over time\ntract_time &lt;- tract_filing_rate %&gt;%\n  mutate(\n    GEOID = as.character(id),\n    date = as.Date(month_date)\n  )\n\n# Create time periods\ntract_time &lt;- tract_time %&gt;%\n  mutate(\n    period = case_when(\n      date &lt; as.Date(\"2020-03-01\") ~ \"Pre-COVID\",\n      date &gt;= as.Date(\"2020-03-01\") & date &lt; as.Date(\"2021-01-01\") ~ \"COVID-2020\",\n      date &gt;= as.Date(\"2021-01-01\") & date &lt; as.Date(\"2022-01-01\") ~ \"COVID-2021\",\n      date &gt;= as.Date(\"2022-01-01\") ~ \"Post-COVID\"\n    )\n  )\n\n# Aggregate by period and tract\nperiod_rates &lt;- tract_time %&gt;%\n  group_by(GEOID, period) %&gt;%\n  summarize(\n    avg_rate = mean(month_rate, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_wider(names_from = period, values_from = avg_rate)\n\n# Join to geometry for mapping\nperiod_geo &lt;- philly_tracts %&gt;%\n  left_join(period_rates, by = \"GEOID\")\n\n# Check available periods\navailable_periods &lt;- names(period_rates)[-1]\ncat(\"Available time periods:\", paste(available_periods, collapse = \", \"), \"\\n\")\n\n\nAvailable time periods: Pre-COVID"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#eviction-rate-changes-by-period",
    "href": "assignments/final/EDA_v3_Fixed.html#eviction-rate-changes-by-period",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "6.2 Eviction Rate Changes by Period",
    "text": "6.2 Eviction Rate Changes by Period\n\n\nCode\n# Create maps for each available period\nmap_list &lt;- list()\n\nfor(period_name in available_periods) {\n  if(period_name %in% names(period_geo)) {\n    p &lt;- ggplot(period_geo) +\n      geom_sf(aes(fill = .data[[period_name]]), color = NA) +\n      scale_fill_viridis_c(option = \"magma\", na.value = \"grey90\", limits = c(0, 0.015)) +\n      labs(title = period_name, fill = \"Rate\") +\n      mapTheme +\n      theme(legend.position = \"bottom\", legend.key.width = unit(1.5, \"cm\"))\n    map_list[[period_name]] &lt;- p\n  }\n}\n\nif(length(map_list) &gt; 0) {\n  wrap_plots(map_list, ncol = 2) +\n    plot_annotation(\n      title = \"Evolution of Eviction Spatial Patterns Over Time\",\n      subtitle = \"How eviction geography changed through COVID-19 pandemic\"\n    )\n}"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#comprehensive-correlation-matrix",
    "href": "assignments/final/EDA_v3_Fixed.html#comprehensive-correlation-matrix",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "7.1 Comprehensive Correlation Matrix",
    "text": "7.1 Comprehensive Correlation Matrix\n\n\nCode\n# Create analysis dataset - with error handling\nif(exists(\"eviction_econ\") && nrow(eviction_econ) &gt; 0) {\n  analysis_df &lt;- eviction_econ %&gt;%\n    filter(!is.na(unemployment_rate)) %&gt;%\n    select(date, month_filings, percentage_diff, unemployment_rate, \n           any_of(\"cpi_value\"))\n  \n  # Add lagged variables\n  analysis_df &lt;- analysis_df %&gt;%\n    arrange(date) %&gt;%\n    mutate(\n      filings_change = month_filings - lag(month_filings),\n      unemp_change = unemployment_rate - lag(unemployment_rate),\n      unemp_lag1 = lag(unemployment_rate, 1),\n      unemp_lag2 = lag(unemployment_rate, 2)\n    ) %&gt;%\n    filter(!is.na(unemp_lag2))\n  \n  cat(\"Analysis dataset has\", nrow(analysis_df), \"observations\\n\")\n  \n  # Correlation matrix\n  if(nrow(analysis_df) &gt; 5) {\n    cor_vars &lt;- analysis_df %&gt;%\n      select(month_filings, percentage_diff, unemployment_rate, unemp_lag1, unemp_lag2) %&gt;%\n      na.omit()\n    \n    if (nrow(cor_vars) &gt; 5) {\n      cor_matrix &lt;- cor(cor_vars)\n      \n      corrplot(cor_matrix, \n               method = \"color\",\n               type = \"upper\",\n               addCoef.col = \"black\",\n               tl.col = \"black\",\n               tl.srt = 45,\n               col = colorRampPalette(c(unemp_color, \"white\", eviction_color))(100),\n               title = \"Correlation Matrix: Evictions and Economic Indicators\",\n               mar = c(0, 0, 2, 0))\n    }\n  } else {\n    cat(\"Not enough data for correlation matrix.\\n\")\n  }\n} else {\n  cat(\"eviction_econ not available. Skipping correlation analysis.\\n\")\n  analysis_df &lt;- data.frame()  # Create empty df to prevent later errors\n}\n\n\nAnalysis dataset has 66 observations"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#key-scatter-plots",
    "href": "assignments/final/EDA_v3_Fixed.html#key-scatter-plots",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "7.2 Key Scatter Plots",
    "text": "7.2 Key Scatter Plots\n\n\nCode\nif(exists(\"analysis_df\") && nrow(analysis_df) &gt; 5) {\n  # Eviction vs Unemployment\n  p1 &lt;- ggplot(analysis_df, aes(x = unemployment_rate, y = month_filings)) +\n    geom_point(color = unemp_color, alpha = 0.7, size = 3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = eviction_color) +\n    labs(\n      title = \"Evictions vs Unemployment\",\n      subtitle = paste(\"r =\", round(cor(analysis_df$month_filings, analysis_df$unemployment_rate, use = \"complete.obs\"), 3)),\n      x = \"Unemployment Rate (%)\",\n      y = \"Monthly Filings\"\n    ) +\n    plotTheme\n  \n  # Eviction vs Lagged Unemployment\n  p2 &lt;- ggplot(analysis_df, aes(x = unemp_lag1, y = month_filings)) +\n    geom_point(color = cpi_color, alpha = 0.7, size = 3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = eviction_color) +\n    labs(\n      title = \"Evictions vs Lagged Unemployment (1 month)\",\n      subtitle = paste(\"r =\", round(cor(analysis_df$month_filings, analysis_df$unemp_lag1, use = \"complete.obs\"), 3)),\n      x = \"Unemployment Rate (1 month lag)\",\n      y = \"Monthly Filings\"\n    ) +\n    plotTheme\n  \n  print(p1 + p2)\n} else {\n  cat(\"Not enough overlapping data for scatter plots.\\n\")\n}"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#eviction-trends-summary",
    "href": "assignments/final/EDA_v3_Fixed.html#eviction-trends-summary",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "8.1 Eviction Trends Summary",
    "text": "8.1 Eviction Trends Summary\n\n\nCode\neviction_summary &lt;- eviction_trends_clean %&gt;%\n  summarize(\n    `Mean Monthly Filings` = mean(month_filings, na.rm = TRUE),\n    `SD Monthly Filings` = sd(month_filings, na.rm = TRUE),\n    `Min Filings` = min(month_filings, na.rm = TRUE),\n    `Max Filings` = max(month_filings, na.rm = TRUE),\n    `Mean % Diff from Baseline` = mean(percentage_diff, na.rm = TRUE),\n    `Months Above Baseline` = sum(percentage_diff &gt; 0, na.rm = TRUE),\n    `Total Months` = n()\n  ) %&gt;%\n  pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\")\n\nkable(eviction_summary, digits = 2, caption = \"Eviction Trends Summary\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nEviction Trends Summary\n\n\nStatistic\nValue\n\n\n\n\nMean Monthly Filings\n944.69\n\n\nSD Monthly Filings\n413.77\n\n\nMin Filings\n0.00\n\n\nMax Filings\n2015.00\n\n\nMean % Diff from Baseline\n0.85\n\n\nMonths Above Baseline\n67.00\n\n\nTotal Months\n70.00"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#economic-indicators-summary",
    "href": "assignments/final/EDA_v3_Fixed.html#economic-indicators-summary",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "8.2 Economic Indicators Summary",
    "text": "8.2 Economic Indicators Summary\n\n\nCode\nif(exists(\"analysis_df\") && nrow(analysis_df) &gt; 0) {\n  econ_summary &lt;- analysis_df %&gt;%\n    summarize(\n      `Mean Unemployment Rate` = mean(unemployment_rate, na.rm = TRUE),\n      `Max Unemployment Rate` = max(unemployment_rate, na.rm = TRUE),\n      `Min Unemployment Rate` = min(unemployment_rate, na.rm = TRUE),\n      `Correlation: Eviction-Unemployment` = cor(month_filings, unemployment_rate, use = \"complete.obs\"),\n      `Correlation: Eviction-Lagged Unemp` = cor(month_filings, unemp_lag1, use = \"complete.obs\")\n    ) %&gt;%\n    pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\")\n  \n  kable(econ_summary, digits = 3, caption = \"Economic Indicators Summary\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n} else {\n  # Use unemp_long directly\n  unemp_summary &lt;- unemp_long %&gt;%\n    summarize(\n      `Mean Unemployment Rate` = mean(unemployment_rate, na.rm = TRUE),\n      `Max Unemployment Rate` = max(unemployment_rate, na.rm = TRUE),\n      `Min Unemployment Rate` = min(unemployment_rate, na.rm = TRUE),\n      `Observations` = n()\n    ) %&gt;%\n    pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\")\n  \n  kable(unemp_summary, digits = 3, caption = \"Unemployment Data Summary\") %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n}\n\n\n\nEconomic Indicators Summary\n\n\nStatistic\nValue\n\n\n\n\nMean Unemployment Rate\n5.200\n\n\nMax Unemployment Rate\n13.900\n\n\nMin Unemployment Rate\n3.200\n\n\nCorrelation: Eviction-Unemployment\n-0.806\n\n\nCorrelation: Eviction-Lagged Unemp\n-0.751"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#summary-of-eda-findings",
    "href": "assignments/final/EDA_v3_Fixed.html#summary-of-eda-findings",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "9.1 Summary of EDA Findings",
    "text": "9.1 Summary of EDA Findings\nBased on the comprehensive exploratory analysis, here are the key findings:\n\nTemporal Patterns\n\n\nCode\n# Calculate key temporal stats\navg_filings &lt;- mean(eviction_trends_clean$month_filings, na.rm = TRUE)\nmax_filings &lt;- max(eviction_trends_clean$month_filings, na.rm = TRUE)\nmax_date &lt;- eviction_trends_clean$date[which.max(eviction_trends_clean$month_filings)]\n\n\n\nAverage monthly eviction filings: 945 cases\nPeak filings: 2,015 cases in 一月 2020\nClear seasonal patterns with higher filings in certain months\n\n\n\nEconomic Relationships\n\n\nCode\n# Calculate correlations - with fallback\nif(exists(\"analysis_df\") && nrow(analysis_df) &gt; 5) {\n  cor_unemp &lt;- cor(analysis_df$month_filings, analysis_df$unemployment_rate, use = \"complete.obs\")\n  cor_lag &lt;- cor(analysis_df$month_filings, analysis_df$unemp_lag1, use = \"complete.obs\")\n} else {\n  # Use eviction_econ_lag if available\n  if(exists(\"eviction_econ_lag\") && nrow(eviction_econ_lag) &gt; 5) {\n    cor_unemp &lt;- cor(eviction_econ_lag$month_filings, eviction_econ_lag$unemployment_rate, use = \"complete.obs\")\n    cor_lag &lt;- cor(eviction_econ_lag$month_filings, eviction_econ_lag$unemp_lag1, use = \"complete.obs\")\n  } else {\n    cor_unemp &lt;- NA\n    cor_lag &lt;- NA\n  }\n}\n\n\n\nUnemployment-Eviction correlation: r = -0.806 (negative relationship)\nWhy negative? This reflects the COVID-19 eviction moratorium effect:\n\n2020: High unemployment + eviction bans → Low evictions\n2021-2022: Unemployment falling + bans lifted → Eviction surge\n\nInterpretation: The moratorium disrupted the normal unemployment→eviction pathway. Policy intervention matters!\nModel implication: Unemployment still valuable as predictor, but relationship is complex due to policy context\n\n\n\nSpatial Patterns\n\n\nCode\n# Get Moran's I result - with fallback if spatial analysis failed\nif(exists(\"moran_test\") && !is.null(moran_test) && !is.null(moran_test$estimate)) {\n  moran_i &lt;- round(moran_test$estimate[1], 3)\n} else {\n  moran_i &lt;- \"N/A (spatial analysis did not complete)\"\n}\n\nif(exists(\"hotspot_tracts\") && is.data.frame(hotspot_tracts) && nrow(hotspot_tracts) &gt; 0) {\n  n_hotspots &lt;- nrow(hotspot_tracts)\n} else {\n  n_hotspots &lt;- \"N/A\"\n}\n\n\n\nSignificant spatial clustering detected (Moran’s I = N/A (spatial analysis did not complete))\n10 census tracts identified as statistically significant hot spots\nEvictions are geographically concentrated, not randomly distributed\n\n\n\nEquity Implications\n\n\nCode\n# Get disparity info\nmax_disparity_group &lt;- equity_summary$majority_type[which.max(equity_summary$`Mean Eviction Rate`)]\nmax_disparity_ratio &lt;- round(max(equity_summary$`Disparity Ratio`, na.rm = TRUE), 2)\n\n\n\nHighest eviction rates observed in No Majority neighborhoods\nDisparity ratio: x higher than Majority White neighborhoods\nSignificant equity concerns require policy attention"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#charts-for-presentation-slides",
    "href": "assignments/final/EDA_v3_Fixed.html#charts-for-presentation-slides",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "9.2 Charts for Presentation Slides",
    "text": "9.2 Charts for Presentation Slides\nThe following visualizations are recommended for the presentation:\n\n\n\n\n\n\n\n\nChart\nSection\nPurpose\n\n\n\n\nDual Y-axis (Unemployment vs Evictions)\n3.3\nKey story chart - shows relationship\n\n\nLISA Cluster Map\n4.3\nShows spatial clustering (hot spots)\n\n\nDemographic Bar Chart\n5.1\nShows equity disparities\n\n\nCorrelation Scatter\n7.2\nSupports model validity\n\n\nPeriod Comparison Maps\n6.2\nShows temporal-spatial changes"
  },
  {
    "objectID": "assignments/final/EDA_v3_Fixed.html#implications-for-model",
    "href": "assignments/final/EDA_v3_Fixed.html#implications-for-model",
    "title": "EDA: Philadelphia Eviction Data Analysis",
    "section": "9.3 Implications for Model",
    "text": "9.3 Implications for Model\nBased on this EDA, the predictive model includes:\n\nCPI (Inflation) - captures cost-of-living pressure on renters\nLag of previous month’s evictions - temporal autocorrelation (ethical note: use for prevention, not punishment)\nUnemployment rate - economic disruption indicator (note: negative correlation due to moratorium period)\n\nWhy Urban Institute variables produced NA coefficients: - Static, city-level data (one snapshot, doesn’t vary by month) - Creates collinearity with intercept - Recommendation: Use as “vulnerability context” in narrative, not as time-varying predictor\nEquity considerations: - Model should be used to target prevention resources, not to reduce services - High-prediction areas need more intervention, not less - Monitor prediction errors across demographic groups\n\nEDA completed: 2025-12-06\nAuthor: Fang Zhe | MUSA 5080 Final Project"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html",
    "href": "assignments/final/final_project_instructions.html",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Presentation Date: Monday, December 8, 2025\nTeam Size: 3-4 students per team\nFormat: In-class presentation with live Q&A (judges will grill you!)\n\n\n\n\nWelcome to the ultimate data science showdown! Your team will compete to build the most compelling predictive model that serves a real public policy need. Like Iron Chef, you’ll work with a “secret ingredient” (your chosen dataset). Like Shark Tank, you’ll pitch your model’s value to city decision-makers. And like Kaggle, you’ll be evaluated on both technical rigor and real-world applicability.\nThe Challenge: Build a predictive model that city officials would actually want to use, using only the techniques we’ve covered this semester.\nThe Stakes: Your presentation will be evaluated by a panel of judges (your professor + a surprise guest) who will grill you on your methods, assumptions, and recommendations. You need to demonstrate not just technical competence, but deep understanding of what you’ve built and why it matters.\n\n\n\n\nFor each dataset option:\n\nYour Task: Through exploratory data analysis, identify what predictive question would be most useful for policy and determine what outcome variable makes sense to predict\n\nConsider:\n\nWhat policy context does this address?\nWho are the stakeholders?\nWhat would you predict (counts? yes/no? risk levels?)?\nWhat other data would enrich this analysis?\nWhat method is appropriate for your chosen outcome?\nWhat could go wrong? Who could be harmed? Like Iron Chef’s mystery ingredient, you must choose ONE of the following as your primary dataset. However, you should (and are strongly encouraged to) incorporate additional data sources to enrich your analysis.\n\n\n\n\nPrimary Source: Eviction Lab Philadelphia Tracking\nDescription: Eviction filing data for Philadelphia\n\n\n\n\n\n\nPrimary Source: Real Estate Tax Balances\nDescription: Outstanding real estate tax balances for Philadelphia properties\n\n\n\n\n\n\nPrimary Source: SEPTA Ridership Statistics\nDescription: Ridership data by route, line, station, time period, and mode\n\n\n\n\n\n\nPrimary Source: L&I Building Certifications\nDescription: Required periodic inspections for certain buildings - certification status (pass/fail/deficient)\n\n\n\n\n\n\n\n\n\nStart with Exploratory Data Analysis\n\nBefore deciding what to predict, explore your data thoroughly\nUnderstand distributions, patterns, data quality issues\nLet the EDA guide your problem framing and use case selection\nDocument your EDA process - this is where you justify your decisions\n\nIncorporate Multiple Data Sources\n\nYour primary dataset alone is NOT enough for a strong model\nYou should integrate Census/ACS data, property characteristics, spatial features, and other relevant OpenDataPhilly datasets\nDocument where each data source comes from and why you chose it\nBe transparent about how you joined/merged datasets\n\nBe Predictive\n\nMust forecast or predict an outcome\nMust demonstrate clear out-of-sample prediction accuracy\nMust use appropriate train/test splits or cross-validation\n\nUse Course Techniques ONLY\nYour toolbox includes (and is limited to):\n\nRegression methods:\n\nLinear regression (OLS)\nLogistic regression\nCount models (Poisson/negative binomial)\nSpace Time Models\n\nFeature engineering:\n\nCategorical variables and interactions\nPolynomial terms\nSpatial features (buffers, kNN, distance to amenities)\nNeighborhood fixed effects\nLocal Hotspots & Cold Spots (+ Distance to them)\nk-means\n\nModel evaluation:\n\nTrain/test splits\nCross-validation (k-fold, LOOCV, spatial CV)\nAppropriate accuracy metrics (MAE, RMSE, confusion matrices, etc.)\nSpatial autocorrelation diagnostics (Moran’s I)\n\n\nDemonstrate Appropriate Application\n\nChoose the RIGHT technique for your problem (don’t use logistic regression for continuous outcomes!)\nChoose a spatial analytical unit (buildling, tract, grid, hexagon, etc.)\nShow you understand WHY each method is appropriate\nConsider and discuss model limitations\n\nDocument Transparently\n\nShow ALL data cleaning steps (don’t hide messy decisions!)\nExplain why you made each choice (variable selection, transformations, etc.)\nAcknowledge what you tried that didn’t work\nBe honest about limitations and uncertainties - you are given an unreasonably short amount of time for this. Limitations are expected, we just want you to be transparent. Be decisive & don’t try to do too much!\n\nServe a Clear Purpose\n\nMust address a real policy question or decision\nMust be useful to city officials, planners, or community stakeholders\nMust consider equity implications and potential for bias (but if this is just generic verbatim what AI says, the sharks will not be happy)\nMust include actionable recommendations\n\n\n\n\n\n\n\nYour presentation should answer:\n\n\n\nWhat policy problem are you solving?\nWho is affected and why does it matter?\nWhat decision would your model inform?\n\n\n\n\n\nWhat are you predicting and why?\nHow does prediction help solve the problem?\nWhat makes your approach better than current practice?\n\n\n\n\n\nHow accurate is your model?\nHow did you validate it?\nWhat are its limitations?\nCould it introduce or perpetuate bias?\n\n\n\n\n\nHow would the city actually use this?\nWhat data would they need to collect?\nWhat are the costs/benefits?\nWhat safeguards are needed?\n\n\n\n\n\n\n\n\n\nDuration: 7 minutes max to present + 3 minutes for Q&A from the sharks\nFormat: Slide deck with visualizations\nContent:\n\nProblem statement and policy context (What are you predicting and why?)\nData sources and integration approach\nMethods overview (What technique did you use and why is it appropriate?)\nResults and model performance (How accurate? How did you validate?)\nLimitations and bias considerations (What could go wrong? Who could be harmed?)\nRecommendations for implementation (How would the city actually use this?)\n\nAudience: Assume your judges are smart city officials who understand policy but may need technical concepts explained clearly\n\n\n\n\n\nDue: Posted to GitHub BEFORE class begins on Monday, December 8th\nSubmit: GitHub repository link via Canvas\nContent:\n\nExecutive summary (1-2 paragraphs for non-technical readers)\nFull reproducible analysis starting with EDA\nALL code, outputs, and visualizations\nTransparent documentation of all data cleaning and decision-making\nDiscussion of methods and why they’re appropriate\nModel validation and performance assessment\nLimitations and ethical considerations\nData sources and integration methods clearly documented\n\nStyle: Professional portfolio piece (remember, potential employers will see this!)\nRepository must include:\n\nWell-organized file structure\nClean, commented code\nREADME explaining project\nAll data sources documented (or links to where they can be downloaded)\n\n\nTHIS IS YOUR LAST ASSIGNMENT OF THE SEMESTER! 🎉\n\n\n\n\n\nYour project will be evaluated on:\n\n\n\nAppropriate method selection for the problem\nCorrect implementation of techniques\nThorough model validation and testing\nClear demonstration of technical understanding during Q&A\n\n\n\n\n\nAddresses a meaningful policy question\nProvides actionable insights\nConsiders implementation feasibility\nConnects predictions to decisions\n\n\n\n\n\nAcknowledges limitations honestly\nConsiders potential for bias and harm\nDiscusses ethical implications\nProposes appropriate safeguards\n\n\n\n\n\nClear, compelling presentation\nEffective visualizations\nProfessional documentation\nAbility to explain technical concepts to non-technical audience\n\n\n\n\n\n\nDuring your presentation, judges will ask probing questions such as:\nTechnical Understanding:\n\n“Why did you choose [method X] instead of [method Y]?”\n“What happens if [assumption Z] is violated?”\n“How do you know your model isn’t just overfitting?”\n\nPolicy Implications:\n\n“What happens to communities your model predicts poorly for?”\n“How would you explain this to city council?”\n“What could go wrong if the city implements this?”\n\nEthical Considerations:\n\n“Could this system perpetuate existing inequities?”\n“Who would be harmed if your model is wrong?”\n“What safeguards would you build in?”\n\nBe prepared to defend every decision you made.\n\n\n\n\n\n\nDue date: In-class presentations Monday, December 8th Repo must be posted BEFORE class begins on the 8th\nThis is a tight timeline, so strategy matters more than ambition:\n✅ DO THIS: Smaller scope, thorough execution, solid understanding (beautiful) ❌ DON’T DO THIS: Overly ambitious project, surface-level analysis, can’t defend your choices\n\n\n\n\nWeek 1 (Immediately):\n\nForm teams and select dataset\nSTART WITH EDA - load the data, explore it, understand what you have\nLet the EDA guide your problem framing\nIdentify what additional data sources you need\n\nWeek 2: (You’ll have all class period on 12/1 to work on this)\n\nDefine your specific predictive question based on EDA insights\nGather and integrate additional datasets (Census, spatial features, etc.)\nBuild baseline model(s)\nStart documenting your process\n\nWeek 3 (Final Sprint):\n\nRefine model, validate thoroughly\nConduct bias/equity review\nPrepare presentation and slides\nCRITICAL: Test your repo, make sure everything renders\nPost repo BEFORE Monday morning!\n\n\n\nConsider defining roles within your team:\nEveryone contributes to everything, but having point people prevents duplication and ensures nothing gets missed.\n\n\n\nMake a plan NOW for posting your repo before class on December 8th:\n\nTest that your Quarto document renders completely\nInclude clear README\nOrganized file structure\nAll data sources documented\nClean, commented code\n\nDon’t wait until very late Sunday night to discover your repo won’t render!"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#iron-chef-meets-shark-tank-meets-kaggle",
    "href": "assignments/final/final_project_instructions.html#iron-chef-meets-shark-tank-meets-kaggle",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Presentation Date: Monday, December 8, 2025\nTeam Size: 3-4 students per team\nFormat: In-class presentation with live Q&A (judges will grill you!)"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#overview",
    "href": "assignments/final/final_project_instructions.html#overview",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Welcome to the ultimate data science showdown! Your team will compete to build the most compelling predictive model that serves a real public policy need. Like Iron Chef, you’ll work with a “secret ingredient” (your chosen dataset). Like Shark Tank, you’ll pitch your model’s value to city decision-makers. And like Kaggle, you’ll be evaluated on both technical rigor and real-world applicability.\nThe Challenge: Build a predictive model that city officials would actually want to use, using only the techniques we’ve covered this semester.\nThe Stakes: Your presentation will be evaluated by a panel of judges (your professor + a surprise guest) who will grill you on your methods, assumptions, and recommendations. You need to demonstrate not just technical competence, but deep understanding of what you’ve built and why it matters."
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#the-secret-ingredient-dataset-options",
    "href": "assignments/final/final_project_instructions.html#the-secret-ingredient-dataset-options",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "For each dataset option:\n\nYour Task: Through exploratory data analysis, identify what predictive question would be most useful for policy and determine what outcome variable makes sense to predict\n\nConsider:\n\nWhat policy context does this address?\nWho are the stakeholders?\nWhat would you predict (counts? yes/no? risk levels?)?\nWhat other data would enrich this analysis?\nWhat method is appropriate for your chosen outcome?\nWhat could go wrong? Who could be harmed? Like Iron Chef’s mystery ingredient, you must choose ONE of the following as your primary dataset. However, you should (and are strongly encouraged to) incorporate additional data sources to enrich your analysis.\n\n\n\n\nPrimary Source: Eviction Lab Philadelphia Tracking\nDescription: Eviction filing data for Philadelphia\n\n\n\n\n\n\nPrimary Source: Real Estate Tax Balances\nDescription: Outstanding real estate tax balances for Philadelphia properties\n\n\n\n\n\n\nPrimary Source: SEPTA Ridership Statistics\nDescription: Ridership data by route, line, station, time period, and mode\n\n\n\n\n\n\nPrimary Source: L&I Building Certifications\nDescription: Required periodic inspections for certain buildings - certification status (pass/fail/deficient)"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#technical-requirements",
    "href": "assignments/final/final_project_instructions.html#technical-requirements",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Start with Exploratory Data Analysis\n\nBefore deciding what to predict, explore your data thoroughly\nUnderstand distributions, patterns, data quality issues\nLet the EDA guide your problem framing and use case selection\nDocument your EDA process - this is where you justify your decisions\n\nIncorporate Multiple Data Sources\n\nYour primary dataset alone is NOT enough for a strong model\nYou should integrate Census/ACS data, property characteristics, spatial features, and other relevant OpenDataPhilly datasets\nDocument where each data source comes from and why you chose it\nBe transparent about how you joined/merged datasets\n\nBe Predictive\n\nMust forecast or predict an outcome\nMust demonstrate clear out-of-sample prediction accuracy\nMust use appropriate train/test splits or cross-validation\n\nUse Course Techniques ONLY\nYour toolbox includes (and is limited to):\n\nRegression methods:\n\nLinear regression (OLS)\nLogistic regression\nCount models (Poisson/negative binomial)\nSpace Time Models\n\nFeature engineering:\n\nCategorical variables and interactions\nPolynomial terms\nSpatial features (buffers, kNN, distance to amenities)\nNeighborhood fixed effects\nLocal Hotspots & Cold Spots (+ Distance to them)\nk-means\n\nModel evaluation:\n\nTrain/test splits\nCross-validation (k-fold, LOOCV, spatial CV)\nAppropriate accuracy metrics (MAE, RMSE, confusion matrices, etc.)\nSpatial autocorrelation diagnostics (Moran’s I)\n\n\nDemonstrate Appropriate Application\n\nChoose the RIGHT technique for your problem (don’t use logistic regression for continuous outcomes!)\nChoose a spatial analytical unit (buildling, tract, grid, hexagon, etc.)\nShow you understand WHY each method is appropriate\nConsider and discuss model limitations\n\nDocument Transparently\n\nShow ALL data cleaning steps (don’t hide messy decisions!)\nExplain why you made each choice (variable selection, transformations, etc.)\nAcknowledge what you tried that didn’t work\nBe honest about limitations and uncertainties - you are given an unreasonably short amount of time for this. Limitations are expected, we just want you to be transparent. Be decisive & don’t try to do too much!\n\nServe a Clear Purpose\n\nMust address a real policy question or decision\nMust be useful to city officials, planners, or community stakeholders\nMust consider equity implications and potential for bias (but if this is just generic verbatim what AI says, the sharks will not be happy)\nMust include actionable recommendations"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#the-shark-tank-pitch-what-to-sell",
    "href": "assignments/final/final_project_instructions.html#the-shark-tank-pitch-what-to-sell",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Your presentation should answer:\n\n\n\nWhat policy problem are you solving?\nWho is affected and why does it matter?\nWhat decision would your model inform?\n\n\n\n\n\nWhat are you predicting and why?\nHow does prediction help solve the problem?\nWhat makes your approach better than current practice?\n\n\n\n\n\nHow accurate is your model?\nHow did you validate it?\nWhat are its limitations?\nCould it introduce or perpetuate bias?\n\n\n\n\n\nHow would the city actually use this?\nWhat data would they need to collect?\nWhat are the costs/benefits?\nWhat safeguards are needed?"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#deliverables",
    "href": "assignments/final/final_project_instructions.html#deliverables",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Duration: 7 minutes max to present + 3 minutes for Q&A from the sharks\nFormat: Slide deck with visualizations\nContent:\n\nProblem statement and policy context (What are you predicting and why?)\nData sources and integration approach\nMethods overview (What technique did you use and why is it appropriate?)\nResults and model performance (How accurate? How did you validate?)\nLimitations and bias considerations (What could go wrong? Who could be harmed?)\nRecommendations for implementation (How would the city actually use this?)\n\nAudience: Assume your judges are smart city officials who understand policy but may need technical concepts explained clearly\n\n\n\n\n\nDue: Posted to GitHub BEFORE class begins on Monday, December 8th\nSubmit: GitHub repository link via Canvas\nContent:\n\nExecutive summary (1-2 paragraphs for non-technical readers)\nFull reproducible analysis starting with EDA\nALL code, outputs, and visualizations\nTransparent documentation of all data cleaning and decision-making\nDiscussion of methods and why they’re appropriate\nModel validation and performance assessment\nLimitations and ethical considerations\nData sources and integration methods clearly documented\n\nStyle: Professional portfolio piece (remember, potential employers will see this!)\nRepository must include:\n\nWell-organized file structure\nClean, commented code\nREADME explaining project\nAll data sources documented (or links to where they can be downloaded)\n\n\nTHIS IS YOUR LAST ASSIGNMENT OF THE SEMESTER! 🎉"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#evaluation-criteria",
    "href": "assignments/final/final_project_instructions.html#evaluation-criteria",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Your project will be evaluated on:\n\n\n\nAppropriate method selection for the problem\nCorrect implementation of techniques\nThorough model validation and testing\nClear demonstration of technical understanding during Q&A\n\n\n\n\n\nAddresses a meaningful policy question\nProvides actionable insights\nConsiders implementation feasibility\nConnects predictions to decisions\n\n\n\n\n\nAcknowledges limitations honestly\nConsiders potential for bias and harm\nDiscusses ethical implications\nProposes appropriate safeguards\n\n\n\n\n\nClear, compelling presentation\nEffective visualizations\nProfessional documentation\nAbility to explain technical concepts to non-technical audience"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#the-grilling-what-to-expect",
    "href": "assignments/final/final_project_instructions.html#the-grilling-what-to-expect",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "During your presentation, judges will ask probing questions such as:\nTechnical Understanding:\n\n“Why did you choose [method X] instead of [method Y]?”\n“What happens if [assumption Z] is violated?”\n“How do you know your model isn’t just overfitting?”\n\nPolicy Implications:\n\n“What happens to communities your model predicts poorly for?”\n“How would you explain this to city council?”\n“What could go wrong if the city implements this?”\n\nEthical Considerations:\n\n“Could this system perpetuate existing inequities?”\n“Who would be harmed if your model is wrong?”\n“What safeguards would you build in?”\n\nBe prepared to defend every decision you made."
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#time-management-team-workflow",
    "href": "assignments/final/final_project_instructions.html#time-management-team-workflow",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Due date: In-class presentations Monday, December 8th Repo must be posted BEFORE class begins on the 8th\nThis is a tight timeline, so strategy matters more than ambition:\n✅ DO THIS: Smaller scope, thorough execution, solid understanding (beautiful) ❌ DON’T DO THIS: Overly ambitious project, surface-level analysis, can’t defend your choices"
  },
  {
    "objectID": "assignments/final/final_project_instructions.html#recommended-timeline-adapt-to-your-team",
    "href": "assignments/final/final_project_instructions.html#recommended-timeline-adapt-to-your-team",
    "title": "MUSA 5080 Final Project",
    "section": "",
    "text": "Week 1 (Immediately):\n\nForm teams and select dataset\nSTART WITH EDA - load the data, explore it, understand what you have\nLet the EDA guide your problem framing\nIdentify what additional data sources you need\n\nWeek 2: (You’ll have all class period on 12/1 to work on this)\n\nDefine your specific predictive question based on EDA insights\nGather and integrate additional datasets (Census, spatial features, etc.)\nBuild baseline model(s)\nStart documenting your process\n\nWeek 3 (Final Sprint):\n\nRefine model, validate thoroughly\nConduct bias/equity review\nPrepare presentation and slides\nCRITICAL: Test your repo, make sure everything renders\nPost repo BEFORE Monday morning!\n\n\n\nConsider defining roles within your team:\nEveryone contributes to everything, but having point people prevents duplication and ensures nothing gets missed.\n\n\n\nMake a plan NOW for posting your repo before class on December 8th:\n\nTest that your Quarto document renders completely\nInclude clear README\nOrganized file structure\nAll data sources documented\nClean, commented code\n\nDon’t wait until very late Sunday night to discover your repo won’t render!"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4: Spatial Data & GIS Operations",
    "section": "",
    "text": "Spatial data week. The big idea: geographic features are represented as points, lines, or polygons, each with geometry (where it is) and attributes (what it is). Schools are points, roads are lines, census tracts are polygons.\nThe sf package:\nsf treats spatial data as a regular data.frame with a special geometry column. This means you can use all the tidyverse stuff you already know - filter(), mutate(), group_by() - on spatial data. Before sf, spatial analysis in R was painful. Now it’s almost normal.\nSpatial predicates - this tripped me up:\n\nst_intersects() - any overlap at all\nst_touches() - share boundary but no interior overlap (like neighboring counties)\nst_within() - completely inside\nst_contains() - completely contains\n\nThe trap: st_intersects will match a polygon with itself. If you’re finding neighbors, you get the reference polygon back too. You need to filter it out.\nCoordinate reference systems:\nThis is where I messed up on the assignment. Geographic coordinates (lat/lon, EPSG:4326) are fine for mapping but terrible for measuring distances or areas. For calculations, you need a projected system like State Plane (PA is EPSG:3365). I was getting weird distance results until I figured this out."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes - Spatial Data & GIS Operations in R",
    "section": "",
    "text": "Vector data model represents real-world features as simplified geometric shapes: Points (locations like schools, hospitals), Lines (roads, transit routes), and Polygons (census tracts, neighborhoods). Each feature has both geometry (shape/location) and attributes (data about that feature)\n\nsf package (Simple Features for R) is the modern standard for spatial data — it treats spatial data as a regular data.frame plus a geometry column, integrating seamlessly with tidyverse workflows\n\nSpatial predicates define different types of geographic relationships: st_intersects() (any overlap), st_touches() (share boundary only, no interior overlap), st_within() (completely inside), st_contains() (completely contains), st_disjoint() (no relationship). Choosing the right predicate is critical — using st_intersects when you want neighbors will incorrectly include the reference feature itself\n\nGeographic vs. Projected Coordinate Systems: Geographic (lat/lon, EPSG:4326) is good for global data and web mapping but bad for distance/area calculations. Projected systems (like State Plane EPSG:3365 for Pennsylvania) use meters/feet and are essential for accurate local measurements\n\nSpatial analysis reveals geographic bias in algorithms: training data may under-represent certain areas, spatial autocorrelation violates independence assumptions, and service delivery algorithms can reinforce geographic inequities (e.g., rideshare avoiding neighborhoods, crime prediction concentrating enforcement)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4: Spatial Data & GIS Operations",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Load spatial data\nlibrary(sf)\ntracts &lt;- st_read(\"census_tracts.shp\")\n# Or from tigris package\ntracts &lt;- tigris::tracts(state = \"PA\", county = \"Philadelphia\")\n\n# Check and transform CRS\nst_crs(tracts)  # what is it?\ntracts_projected &lt;- st_transform(tracts, crs = 3365)  # transform to State Plane\n\n# Spatial filtering\nphilly_schools &lt;- st_filter(all_schools, philly_boundary)\n# Default predicate is st_intersects\n\n# Spatial join - attach attributes from one layer to another based on location\ntracts_with_crime &lt;- st_join(tracts, crime_points)\n\n# Distance calculations - NEED PROJECTED CRS\ndistances &lt;- st_distance(points1, points2)\n\n# Buffers\nschool_buffers &lt;- st_buffer(schools, dist = 1000)  # 1000 meters if projected\n\n# The dot placeholder in pipes - this got me\npa_counties %&gt;%\n  mutate(area = st_area(.))  # . refers to the data being piped"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4: Spatial Data & GIS Operations",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhen should I use State Plane vs UTM vs something else? The answer seems to be “depends on your study area” but I want a clearer decision rule.\nEdge effects: if I’m buffering near the edge of my study area, I’m missing things outside the boundary. How do I handle this?\nThe lecture mentioned spatial autocorrelation violates independence assumptions. Does this mean standard regression doesn’t work on spatial data? What do I use instead?"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4: Spatial Data & GIS Operations",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThe “where” question transforms analysis. “Which communities are poorest?” becomes more actionable when you can see spatial clusters, proximity to resources, transportation access. It’s the difference between a table and a map.\nEnvironmental justice is inherently spatial - are pollution sources near vulnerable communities? Facility location decisions (where to put a clinic, school, fire station) need spatial optimization. Rideshare algorithms avoiding certain neighborhoods is a spatial equity issue that planners need to understand."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4: Spatial Data & GIS Operations",
    "section": "Reflection",
    "text": "Reflection\nGetting projections right is boring but essential. I spent an hour debugging distance calculations before realizing I was using lat/lon coordinates. The numbers looked plausible but were wrong. Now I have a checklist: check CRS, transform if needed, verify, then analyze.\nThe st_filter() vs st_intersection() distinction matters: filter selects complete features, intersection clips and creates new geometries. I kept confusing these."
  },
  {
    "objectID": "weekly-notes/week-12-notes.html",
    "href": "weekly-notes/week-12-notes.html",
    "title": "Week 12: Neighborhood Change & Sequence Analysis",
    "section": "",
    "text": "Final topic: understanding neighborhood change as a process, not just comparing snapshots. The “DNA metaphor” captures it well - just like genetic sequences encode biological history, neighborhood sequences encode trajectories of change.\nThe problem with snapshots:\nLooking at a neighborhood in 2010 and labeling it “gentrifying” misses crucial context. Did it steadily improve since 1970? Decline then recover? Cycle through ups and downs? Same 2010 status, completely different histories, potentially different policy needs.\nThe methodological workflow (5 steps):\n\nK-means clustering: classify neighborhoods at each time point into types (e.g., White, Black, Hispanic, Mixed)\nCreate sequences: string together each tract’s cluster membership over time (White → White → Hispanic → Hispanic)\nMeasure dissimilarity: compare sequences using OMstrans (transition-aware optimal matching)\nCluster sequences: group tracts with similar trajectories\nMap results: visualize which trajectory types occur where\n\nK-means for neighborhood classification:\nThis is unsupervised learning - we don’t tell it what makes a “gentrifying” neighborhood, it finds patterns in the data. Input variables include socioeconomic (% college, unemployment, poverty), housing (home values, owner-occupied %), and demographic factors (recent movers, vacancy).\nKey insight: cluster ALL years together so a “Struggling” neighborhood in 1980 has the same meaning as “Struggling” in 2010. Ensures temporal consistency.\nChoosing k is “more art than science.” Mathematical optimum (elbow method, silhouette) might give k=3 but k=6 provides more nuance. The tutorial example found k=3 grouped basically by race alone - not very informative.\nSequence dissimilarity with OMstrans:\nStandard optimal matching counts “edits” to transform one sequence into another. OMstrans focuses on transitions specifically:\n\nStandard: [White, White, Hispanic, Hispanic]\nOMstrans: [White→White, White→Hispanic, Hispanic→Hispanic]\n\nThis captures WHEN the transition happened (early vs. late), not just that it happened.\nClassic urban theories tested:\nChicago School predicted regular spatial patterns - concentric rings of decline and renewal spreading from downtown. LA School predicted chaotic fragmentation with no regular patterns. The data shows both can be right depending on the city. Chicago shows persistent rings; LA is more fragmented."
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#key-concepts",
    "href": "weekly-notes/week-12-notes.html#key-concepts",
    "title": "Week 12: Neighborhood Change & Sequence Analysis",
    "section": "",
    "text": "Final topic: understanding neighborhood change as a process, not just comparing snapshots. The “DNA metaphor” captures it well - just like genetic sequences encode biological history, neighborhood sequences encode trajectories of change.\nThe problem with snapshots:\nLooking at a neighborhood in 2010 and labeling it “gentrifying” misses crucial context. Did it steadily improve since 1970? Decline then recover? Cycle through ups and downs? Same 2010 status, completely different histories, potentially different policy needs.\nThe methodological workflow (5 steps):\n\nK-means clustering: classify neighborhoods at each time point into types (e.g., White, Black, Hispanic, Mixed)\nCreate sequences: string together each tract’s cluster membership over time (White → White → Hispanic → Hispanic)\nMeasure dissimilarity: compare sequences using OMstrans (transition-aware optimal matching)\nCluster sequences: group tracts with similar trajectories\nMap results: visualize which trajectory types occur where\n\nK-means for neighborhood classification:\nThis is unsupervised learning - we don’t tell it what makes a “gentrifying” neighborhood, it finds patterns in the data. Input variables include socioeconomic (% college, unemployment, poverty), housing (home values, owner-occupied %), and demographic factors (recent movers, vacancy).\nKey insight: cluster ALL years together so a “Struggling” neighborhood in 1980 has the same meaning as “Struggling” in 2010. Ensures temporal consistency.\nChoosing k is “more art than science.” Mathematical optimum (elbow method, silhouette) might give k=3 but k=6 provides more nuance. The tutorial example found k=3 grouped basically by race alone - not very informative.\nSequence dissimilarity with OMstrans:\nStandard optimal matching counts “edits” to transform one sequence into another. OMstrans focuses on transitions specifically:\n\nStandard: [White, White, Hispanic, Hispanic]\nOMstrans: [White→White, White→Hispanic, Hispanic→Hispanic]\n\nThis captures WHEN the transition happened (early vs. late), not just that it happened.\nClassic urban theories tested:\nChicago School predicted regular spatial patterns - concentric rings of decline and renewal spreading from downtown. LA School predicted chaotic fragmentation with no regular patterns. The data shows both can be right depending on the city. Chicago shows persistent rings; LA is more fragmented."
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#coding-techniques",
    "href": "weekly-notes/week-12-notes.html#coding-techniques",
    "title": "Week 12: Neighborhood Change & Sequence Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Step 1: K-means clustering\nlibrary(cluster)\n\n# Standardize variables (z-scores within metro)\ncensus_scaled &lt;- census %&gt;%\n  group_by(year) %&gt;%\n  mutate(across(c(pct_college, unemployment, median_income), scale)) %&gt;%\n  ungroup()\n\n# Fit k-means on all years together\nset.seed(123)\nkmeans_result &lt;- kmeans(census_scaled[, vars_to_cluster], centers = 6, nstart = 25)\ncensus_scaled$cluster &lt;- kmeans_result$cluster\n\n# Evaluate with elbow plot\nwss &lt;- sapply(2:10, function(k) {\n  kmeans(census_scaled[, vars_to_cluster], k, nstart = 25)$tot.withinss\n})\nplot(2:10, wss, type = \"b\")\n\n# Step 2: Create sequences\nlibrary(TraMineR)\n\ncensus_wide &lt;- census_scaled %&gt;%\n  select(tract, year, cluster) %&gt;%\n  pivot_wider(names_from = year, values_from = cluster)\n\nseq_data &lt;- seqdef(\n  census_wide,\n  var = c(\"1980\", \"1990\", \"2000\", \"2010\", \"2020\"),\n  labels = c(\"White\", \"Black\", \"Hispanic\", \"Asian\", \"Mixed\", \"Diverse\")\n)\n\n# Visualize all sequences\nseqplot(seq_data, type = \"I\")  # Index plot\n\n# Step 3: OMstrans dissimilarity\nsubmat &lt;- seqsubm(seq_data, method = \"TRATE\", transition = \"both\")\ndist_omstrans &lt;- seqdist(seq_data, method = \"OMstrans\", indel = 1, \n                          sm = submat, otto = 0.1)\n\n# Step 4: Hierarchical clustering of sequences\nhc &lt;- hclust(as.dist(dist_omstrans), method = \"ward.D2\")\ncensus_wide$trajectory &lt;- cutree(hc, k = 14)\n\n# Step 5: Map it\nggplot(tracts_sf %&gt;% left_join(census_wide)) +\n  geom_sf(aes(fill = factor(trajectory))) +\n  theme_void()\nThe TraMineR package does the heavy lifting for sequence analysis. The seqsubm() with “TRATE” sets substitution costs based on transition frequencies - rare transitions cost more."
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#questions-challenges",
    "href": "weekly-notes/week-12-notes.html#questions-challenges",
    "title": "Week 12: Neighborhood Change & Sequence Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow sensitive are results to the choice of k in the initial clustering? Would different k produce different trajectory types?\nThe OMstrans parameters (indel, otto) seem important but I don’t fully understand how to tune them\nWith only 5 time points (census decades), sequences are pretty short. Does this limit what patterns we can detect?"
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#connections-to-policy",
    "href": "weekly-notes/week-12-notes.html#connections-to-policy",
    "title": "Week 12: Neighborhood Change & Sequence Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThe big payoff is targeted intervention:\n\nNeighborhood A: Stable → Declining → Struggling → Gentrifying\n\nPolicy: Anti-displacement, affordable housing preservation\n\nNeighborhood B: Struggling → Struggling → Struggling → Gentrifying\n\nPolicy: Infrastructure investment, longtime resident wealth-building\n\n\nSame current status, different histories, different policy responses.\nAlso tests urban theory empirically. The Chicago vs. LA comparison shows that classic urban models (Burgess concentric zones, Hoyt sectors) still describe some cities well, while others follow the fragmented LA School pattern. One-size-fits-all urban policy probably doesn’t work."
  },
  {
    "objectID": "weekly-notes/week-12-notes.html#reflection",
    "href": "weekly-notes/week-12-notes.html#reflection",
    "title": "Week 12: Neighborhood Change & Sequence Analysis",
    "section": "Reflection",
    "text": "Reflection\nThis felt like a capstone topic pulling together clustering, spatial analysis, and temporal thinking. The DNA metaphor is elegant - neighborhoods have trajectories that encode their history, and similar trajectories might reflect similar underlying processes.\nThe unsupervised approach is both liberating and concerning. We’re not imposing definitions of “gentrification” or “decline” - the algorithm finds natural groupings. But that also means the results depend heavily on what variables we include and how we cluster. Different choices, different stories.\nThe 14 NYC trajectory clusters in the tutorial showed just how varied neighborhood change can be. Some are stable forever, some flip once, some cycle. The spatial patterns of where these trajectories occur tell us something about urban structure that snapshot analysis misses."
  },
  {
    "objectID": "weekly-notes/week-10-notes.html",
    "href": "weekly-notes/week-10-notes.html",
    "title": "Week 10: Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "Switched gears from continuous outcomes (house prices) to binary outcomes (yes/no, 0/1). Linear regression doesn’t work here because predictions can go below 0 or above 1, which makes no sense for probabilities.\nWhy logistic regression:\n\nThe logistic function (sigmoid curve) constrains predictions between 0 and 1\nInstead of predicting Y directly, we predict the probability that Y = 1\nThe S-shaped curve means the effect of predictors isn’t constant - it’s steepest in the middle and flattens at extremes\n\nThe logit transformation:\nThis is the key conceptual piece. We work with log-odds, not raw probabilities:\n\nOdds = p / (1-p). If probability is 0.75, odds are 0.75/0.25 = 3:1\nLog-odds (logit) = ln(odds). This creates a linear relationship with predictors\nCoefficients are in log-odds units, which is hard to interpret directly\nExponentiate coefficients to get odds ratios: exp(β). OR &gt; 1 means predictor increases odds, OR &lt; 1 means it decreases odds\n\nPolicy applications:\nThe slides listed tons of examples: will someone reoffend (recidivism)? will they show up for court (flight risk)? will a building be demolished (blight prediction)? will a household participate in a program (uptake)? All binary outcomes that matter for resource allocation."
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#key-concepts",
    "href": "weekly-notes/week-10-notes.html#key-concepts",
    "title": "Week 10: Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "Switched gears from continuous outcomes (house prices) to binary outcomes (yes/no, 0/1). Linear regression doesn’t work here because predictions can go below 0 or above 1, which makes no sense for probabilities.\nWhy logistic regression:\n\nThe logistic function (sigmoid curve) constrains predictions between 0 and 1\nInstead of predicting Y directly, we predict the probability that Y = 1\nThe S-shaped curve means the effect of predictors isn’t constant - it’s steepest in the middle and flattens at extremes\n\nThe logit transformation:\nThis is the key conceptual piece. We work with log-odds, not raw probabilities:\n\nOdds = p / (1-p). If probability is 0.75, odds are 0.75/0.25 = 3:1\nLog-odds (logit) = ln(odds). This creates a linear relationship with predictors\nCoefficients are in log-odds units, which is hard to interpret directly\nExponentiate coefficients to get odds ratios: exp(β). OR &gt; 1 means predictor increases odds, OR &lt; 1 means it decreases odds\n\nPolicy applications:\nThe slides listed tons of examples: will someone reoffend (recidivism)? will they show up for court (flight risk)? will a building be demolished (blight prediction)? will a household participate in a program (uptake)? All binary outcomes that matter for resource allocation."
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#coding-techniques",
    "href": "weekly-notes/week-10-notes.html#coding-techniques",
    "title": "Week 10: Logistic Regression for Binary Outcomes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Fit logistic regression with glm()\nmodel &lt;- glm(\n  outcome ~ predictor1 + predictor2 + predictor3,\n  data = mydata,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\nsummary(model)\n\n# Get predicted probabilities\nmydata$predicted_prob &lt;- predict(model, type = \"response\")\n\n# Convert to binary predictions using threshold\nmydata$predicted_class &lt;- ifelse(mydata$predicted_prob &gt; 0.5, 1, 0)\n\n# Confusion matrix\ntable(Predicted = mydata$predicted_class, Actual = mydata$outcome)\n\n# Calculate odds ratios from coefficients\nexp(coef(model))\n\n# Confidence intervals for odds ratios\nexp(confint(model))\nThe spam email example was a nice toy case: predict spam (1) vs. legitimate (0) from exclamation marks, whether it contains “free”, and email length.\nImportant distinction from linear regression:\n\nUse glm() not lm()\nSet family = \"binomial\"\nFor predictions, use type = \"response\" to get probabilities (not log-odds)\nInterpretation shifts from “one unit increase in X changes Y by β” to “one unit increase in X multiplies odds by exp(β)”"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#questions-challenges",
    "href": "weekly-notes/week-10-notes.html#questions-challenges",
    "title": "Week 10: Logistic Regression for Binary Outcomes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow do you choose the probability threshold for classification? 0.5 is the default but might not be appropriate if classes are imbalanced or if false positives/negatives have different costs\nThe coefficients in the spam example had huge values and standard errors. Is that a sign of perfect separation or some other problem?\nFor policy applications, which metric matters more - accuracy, sensitivity, specificity, or something else?"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#connections-to-policy",
    "href": "weekly-notes/week-10-notes.html#connections-to-policy",
    "title": "Week 10: Logistic Regression for Binary Outcomes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThe choice of threshold is inherently a policy decision, not just a technical one. If you’re predicting flight risk for bail decisions:\n\nLow threshold: flag more people as high risk → more people detained → more false positives (people detained who would have shown up)\nHigh threshold: flag fewer people → more people released → more false negatives (people released who don’t show up)\n\nThe costs aren’t symmetric. Detaining someone who would have appeared has different consequences than releasing someone who flees. The “optimal” threshold depends on how you weigh these errors, which is a values question.\nAlso, the COMPAS recidivism case (from earlier weeks) is a logistic regression at heart. The debates about fairness and bias apply here too."
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#reflection",
    "href": "weekly-notes/week-10-notes.html#reflection",
    "title": "Week 10: Logistic Regression for Binary Outcomes",
    "section": "Reflection",
    "text": "Reflection\nLogistic regression feels like unlocking a whole new category of problems. So many policy-relevant questions are binary: eligible/ineligible, approved/denied, participated/didn’t participate. The math is a bit more abstract (working in log-odds space) but the intuition carries over from linear regression.\nThe odds ratio interpretation takes getting used to. “A one-unit increase in X multiplies the odds by 1.5” is harder to parse than “increases Y by 10 units.” But it’s the right tool for the job."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7: Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "This week focused on how to diagnose whether our regression models are missing important spatial information. The core question: if our errors cluster geographically, we’re probably missing something.\nModel errors and what they tell us:\n\nPrediction error is simply ŷ - y (predicted minus actual). In house price models, this shows where we over- or under-predict.\nRandom scatter of errors = good. The model captures the key relationships.\nClustered errors = bad. We’re systematically wrong in certain areas, which means location matters in ways we haven’t captured yet.\nIf you map your residuals and see blue clusters (under-predict) or red clusters (over-predict), that’s a signal to add more spatial features.\n\nMoran’s I - the key diagnostic:\n\nMeasures spatial autocorrelation on a scale from -1 to +1. Positive values mean clustering (similar values near each other), zero means random, negative means dispersion.\nThe formula looks intimidating but the intuition is simple: “when I’m above average, are my neighbors also above average?”\nFor each pair of neighbors, multiply their deviations from the mean. If both are above (or both below) average, the product is positive. Sum these up and normalize.\nApplied to model residuals: significant positive Moran’s I means our errors cluster spatially → we need more spatial features.\n\nPractical interpretation:\nThe worked example with 5 houses in a row made it click for me. Houses A and B both had positive errors (over-predicted), D and E both had negative errors (under-predicted). The products A×B and D×E are both positive because similar neighbors. The sum of products being large and positive indicates clustering."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts",
    "href": "weekly-notes/week-07-notes.html#key-concepts",
    "title": "Week 7: Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "This week focused on how to diagnose whether our regression models are missing important spatial information. The core question: if our errors cluster geographically, we’re probably missing something.\nModel errors and what they tell us:\n\nPrediction error is simply ŷ - y (predicted minus actual). In house price models, this shows where we over- or under-predict.\nRandom scatter of errors = good. The model captures the key relationships.\nClustered errors = bad. We’re systematically wrong in certain areas, which means location matters in ways we haven’t captured yet.\nIf you map your residuals and see blue clusters (under-predict) or red clusters (over-predict), that’s a signal to add more spatial features.\n\nMoran’s I - the key diagnostic:\n\nMeasures spatial autocorrelation on a scale from -1 to +1. Positive values mean clustering (similar values near each other), zero means random, negative means dispersion.\nThe formula looks intimidating but the intuition is simple: “when I’m above average, are my neighbors also above average?”\nFor each pair of neighbors, multiply their deviations from the mean. If both are above (or both below) average, the product is positive. Sum these up and normalize.\nApplied to model residuals: significant positive Moran’s I means our errors cluster spatially → we need more spatial features.\n\nPractical interpretation:\nThe worked example with 5 houses in a row made it click for me. Houses A and B both had positive errors (over-predicted), D and E both had negative errors (under-predicted). The products A×B and D×E are both positive because similar neighbors. The sum of products being large and positive indicates clustering."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07-notes.html#coding-techniques",
    "title": "Week 7: Model Diagnostics & Spatial Autocorrelation",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Suppress progress bars in rendered documents\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)\n# Or add progress = FALSE to each get_acs() call\n\n# Calculate prediction errors\ndata &lt;- data %&gt;%\n  mutate(\n    predicted = predict(model, .),\n    error = predicted - actual_value,\n    abs_error = abs(error),\n    pct_error = abs(error) / actual_value\n  )\n\n# Create spatial weights for Moran's I (k nearest neighbors)\nlibrary(spdep)\ncoords &lt;- st_coordinates(data_sf)\nneighbors &lt;- knn2nb(knearneigh(coords, k = 5))\nweights &lt;- nb2listw(neighbors, style = \"W\")\n\n# Calculate spatial lag of errors (average of neighbors' errors)\ndata$error_lag &lt;- lag.listw(weights, data$error)\n\n# Test for spatial autocorrelation\nmoran.test(data$error, weights)\nThe scatter plot of error vs. spatial lag of error is really useful. If there’s a strong positive relationship, your errors are spatially autocorrelated."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7: Model Diagnostics & Spatial Autocorrelation",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow do you decide on the number of neighbors (k) for the weights matrix? I used k=5 but is there a principled way to choose?\nThe assignment mentioned that clustered errors could indicate “non-stationarity” where relationships vary across space. How would we model that?\nWhen Moran’s I is significant, what’s the best strategy - add more spatial features, use neighborhood fixed effects, or something else?"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#connections-to-policy",
    "href": "weekly-notes/week-07-notes.html#connections-to-policy",
    "title": "Week 7: Model Diagnostics & Spatial Autocorrelation",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nSpatial autocorrelation in model errors has direct policy implications. If our housing price model systematically under-predicts in certain neighborhoods, property tax assessments based on that model would be unfair to residents there. The errors themselves might correlate with race or income, compounding existing inequities.\nThe bigger lesson: a model with good overall R² can still be deeply problematic if it’s wrong in systematic, spatially-patterned ways. Mapping residuals should be standard practice."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7: Model Diagnostics & Spatial Autocorrelation",
    "section": "Reflection",
    "text": "Reflection\nThe connection between Tobler’s First Law and model diagnostics finally made sense this week. We’ve been told “near things are more related” but now I see how to operationalize that as a diagnostic check. The Moran’s I calculation demystified something that seemed like a black box before.\nThe housekeeping note about progress = FALSE is worth remembering - those progress bars make rendered documents look unprofessional."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5: Introduction to Linear Regression",
    "section": "",
    "text": "First modeling week. The setup: we have outcome Y and predictors X, and we’re trying to estimate the function that connects them. Y = f(X) + ε, where ε is the stuff we can’t explain.\nPrediction vs. Inference:\nTwo different goals. Prediction just wants accurate forecasts - don’t care why, just want to know what. Inference wants to understand relationships - does X actually affect Y, and by how much? Sometimes you need both, but they can conflict. A complex model might predict well but be impossible to interpret.\nParametric vs. Non-parametric:\nParametric assumes a functional form (like linear). You’re betting that the relationship is roughly linear, then estimating the parameters. Non-parametric lets the data determine the shape. More flexible but needs more data and can overfit.\nOLS regression:\nThe workhorse. Minimizes sum of squared residuals to find the best-fit line. The coefficients tell you: a one-unit change in X is associated with a β-unit change in Y, holding other variables constant. R² tells you what proportion of variance you’ve explained.\nBut R² can mislead:\nA model can have decent R² and still be garbage for prediction. It might overfit the training data, violate assumptions, or miss important nonlinearities. Always check the actual predictions, not just summary statistics.\nTrain/test splits:\nThe key insight: evaluate your model on data it hasn’t seen. Split your data, train on one part, test on the other. If performance drops a lot on test data, you’re probably overfitting. Cross-validation takes this further - multiple train/test splits, average the results."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts",
    "href": "weekly-notes/week-05-notes.html#key-concepts",
    "title": "Week 5: Introduction to Linear Regression",
    "section": "",
    "text": "First modeling week. The setup: we have outcome Y and predictors X, and we’re trying to estimate the function that connects them. Y = f(X) + ε, where ε is the stuff we can’t explain.\nPrediction vs. Inference:\nTwo different goals. Prediction just wants accurate forecasts - don’t care why, just want to know what. Inference wants to understand relationships - does X actually affect Y, and by how much? Sometimes you need both, but they can conflict. A complex model might predict well but be impossible to interpret.\nParametric vs. Non-parametric:\nParametric assumes a functional form (like linear). You’re betting that the relationship is roughly linear, then estimating the parameters. Non-parametric lets the data determine the shape. More flexible but needs more data and can overfit.\nOLS regression:\nThe workhorse. Minimizes sum of squared residuals to find the best-fit line. The coefficients tell you: a one-unit change in X is associated with a β-unit change in Y, holding other variables constant. R² tells you what proportion of variance you’ve explained.\nBut R² can mislead:\nA model can have decent R² and still be garbage for prediction. It might overfit the training data, violate assumptions, or miss important nonlinearities. Always check the actual predictions, not just summary statistics.\nTrain/test splits:\nThe key insight: evaluate your model on data it hasn’t seen. Split your data, train on one part, test on the other. If performance drops a lot on test data, you’re probably overfitting. Cross-validation takes this further - multiple train/test splits, average the results."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5: Introduction to Linear Regression",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Fit a linear model\nmodel &lt;- lm(price ~ sqft + bedrooms + age, data = housing)\nsummary(model)\n\n# Train/test split\nset.seed(123)  # reproducibility\ntrain_idx &lt;- sample(1:nrow(housing), size = 0.7 * nrow(housing))\ntrain &lt;- housing[train_idx, ]\ntest &lt;- housing[-train_idx, ]\n\nmodel &lt;- lm(price ~ sqft + bedrooms, data = train)\n\n# Predict on test set\ntest$predicted &lt;- predict(model, newdata = test)\n\n# Calculate RMSE\nrmse &lt;- sqrt(mean((test$price - test$predicted)^2))\n\n# Cross-validation with caret\nlibrary(caret)\nctrl &lt;- trainControl(method = \"cv\", number = 10)\ncv_model &lt;- train(price ~ sqft + bedrooms, data = housing, \n                  method = \"lm\", trControl = ctrl)\ncv_model$results$RMSE\n\n# Diagnostic plots\nplot(model)  # gives you residuals vs fitted, Q-Q plot, etc.\n\n# Breusch-Pagan test for heteroscedasticity\nlibrary(lmtest)\nbptest(model)"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5: Introduction to Linear Regression",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow do I decide what variables to include? Just throw everything in and see what’s significant? That feels wrong.\nThe diagnostic plots show patterns in residuals. What do I actually do about it? Add polynomial terms? Transform variables?\nCross-validation gives different RMSE each time unless I set a seed. How much variation is acceptable?"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5: Introduction to Linear Regression",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nModel interpretation matters for policy. If your housing price model shows a $50/sqft coefficient, that’s actionable information for assessors. But if the model is misspecified, that number is misleading and could lead to unfair assessments.\nThe healthcare algorithm example from class was disturbing. A model can be statistically “good” (high R², low RMSE) but ethically terrible if it systematically disadvantages certain groups. Prediction accuracy isn’t the only thing that matters.\nAlso: outliers in policy data often represent the communities we should care most about. Dropping them for model fit might mean ignoring the people who need the most help."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5: Introduction to Linear Regression",
    "section": "Reflection",
    "text": "Reflection\nThe train/test split logic makes sense now. Of course you should evaluate on unseen data - otherwise you’re just measuring memorization. I’ve been doing this wrong in other classes, evaluating on the same data I trained on.\nThe “held constant” interpretation of regression coefficients is tricky. In observational data, you can’t actually hold things constant. The coefficient is really about the correlation structure in your data, which may or may not reflect causal relationships."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3: Data Visualization & Exploratory Analysis",
    "section": "",
    "text": "This week was about looking at your data before doing anything fancy with it. The Anscombe’s Quartet example drove this home - four datasets with identical means, variances, correlations, and regression lines that look completely different when you actually plot them. One is linear, one is curved, one has an outlier pulling the line, one is just vertical with one outlier. Summary statistics can lie.\nThe Grammar of Graphics:\nggplot2 builds plots in layers: data → aesthetics → geometries → extras. It sounds abstract but it clicks once you do it. You’re mapping data columns to visual properties (x position, y position, color, size) then choosing how to draw them (points, lines, bars).\nData quality and uncertainty:\nThe Jurjevich et al. paper was sobering. Only 27% of planners report margins of error when presenting census data. And in Portland, 72% of census tracts had unreliable child poverty estimates (CV &gt; 40%). We’re making resource allocation decisions based on shaky numbers and nobody’s saying so.\nThe Coefficient of Variation thresholds: under 12% is reliable, 12-40% is questionable, over 40% is basically guessing. I need to start flagging these."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts",
    "href": "weekly-notes/week-03-notes.html#key-concepts",
    "title": "Week 3: Data Visualization & Exploratory Analysis",
    "section": "",
    "text": "This week was about looking at your data before doing anything fancy with it. The Anscombe’s Quartet example drove this home - four datasets with identical means, variances, correlations, and regression lines that look completely different when you actually plot them. One is linear, one is curved, one has an outlier pulling the line, one is just vertical with one outlier. Summary statistics can lie.\nThe Grammar of Graphics:\nggplot2 builds plots in layers: data → aesthetics → geometries → extras. It sounds abstract but it clicks once you do it. You’re mapping data columns to visual properties (x position, y position, color, size) then choosing how to draw them (points, lines, bars).\nData quality and uncertainty:\nThe Jurjevich et al. paper was sobering. Only 27% of planners report margins of error when presenting census data. And in Portland, 72% of census tracts had unreliable child poverty estimates (CV &gt; 40%). We’re making resource allocation decisions based on shaky numbers and nobody’s saying so.\nThe Coefficient of Variation thresholds: under 12% is reliable, 12-40% is questionable, over 40% is basically guessing. I need to start flagging these."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3: Data Visualization & Exploratory Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Basic ggplot structure\nggplot(data, aes(x = var1, y = var2)) +\n  geom_point() +\n  labs(title = \"Title\", x = \"X Label\", y = \"Y Label\") +\n  theme_minimal()\n\n# Aesthetics that depend on data go INSIDE aes()\n# Constants go outside\nggplot(data, aes(x = income, y = life_expectancy, color = region)) +\n  geom_point(size = 3, alpha = 0.7)  # size and alpha are constants\n\n# Common geoms\ngeom_point()      # scatter plots\ngeom_histogram()  # distributions (one variable)\ngeom_boxplot()    # compare distributions across groups\ngeom_line()       # time series or trends\n\n# Combining data with left_join\ncombined &lt;- left_join(demographics, economic_data, by = \"tract_id\")\n# Always check: did the number of rows change? Do I have unexpected NAs?\nThe key mistake I keep making: putting constant values inside aes(). If you want all points to be blue, it’s geom_point(color = \"blue\"), not aes(color = \"blue\")."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3: Data Visualization & Exploratory Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWhen presenting to policymakers, how do you communicate uncertainty without undermining confidence in your analysis? “These numbers might be wrong” doesn’t play well.\nThe CV thresholds feel arbitrary. Who decided 40%? Is there research behind this or just convention?\nFor outliers that represent real communities (like a very poor county), removing them for visualization purposes feels wrong but keeping them makes everything else hard to see."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3: Data Visualization & Exploratory Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThe ethical dimension here is real. The AICP Code of Ethics requires honest communication, which includes uncertainty. But in practice, most planners don’t report margins of error. We’re failing at this basic professional obligation.\nBad visualizations cause real harm. Misleading scales, cherry-picked time periods, hidden uncertainty - these lead to misallocation of resources. Someone loses services they need because the data presentation made their community look better off than it actually is."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3: Data Visualization & Exploratory Analysis",
    "section": "Reflection",
    "text": "Reflection\nI’ll be honest - before this week I didn’t think much about reporting margins of error. I’d look at the ACS estimates and treat them as facts. The Portland child poverty example was a wake-up call. When 72% of your geographic units have unreliable estimates for a key variable, maybe you shouldn’t be making maps from that variable at all.\nThe visualization-as-detective-work framing is useful. What patterns exist? What’s unusual? What questions does this raise? It’s easy to jump to modeling before really understanding what you have."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts",
    "href": "weekly-notes/week-04-notes.html#key-concepts",
    "title": "Week 4: Spatial Data & GIS Operations",
    "section": "",
    "text": "Spatial data week. The big idea: geographic features are represented as points, lines, or polygons, each with geometry (where it is) and attributes (what it is). Schools are points, roads are lines, census tracts are polygons.\nThe sf package:\nsf treats spatial data as a regular data.frame with a special geometry column. This means you can use all the tidyverse stuff you already know - filter(), mutate(), group_by() - on spatial data. Before sf, spatial analysis in R was painful. Now it’s almost normal.\nSpatial predicates - this tripped me up:\n\nst_intersects() - any overlap at all\nst_touches() - share boundary but no interior overlap (like neighboring counties)\nst_within() - completely inside\nst_contains() - completely contains\n\nThe trap: st_intersects will match a polygon with itself. If you’re finding neighbors, you get the reference polygon back too. You need to filter it out.\nCoordinate reference systems:\nThis is where I messed up on the assignment. Geographic coordinates (lat/lon, EPSG:4326) are fine for mapping but terrible for measuring distances or areas. For calculations, you need a projected system like State Plane (PA is EPSG:3365). I was getting weird distance results until I figured this out."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6: Spatial Machine Learning & Fixed Effects",
    "section": "",
    "text": "Building on last week’s regression with spatial features. The motivating example: predicting house prices. A 1,000 sqft house in Back Bay is worth way more than the same house in Roxbury. Location matters, and we need to capture it.\nHedonic model framework:\nPrice = structural features + neighborhood features + locational features. Structural is bedrooms, bathrooms, sqft. Neighborhood is schools, crime, amenities. Locational is distance to downtown, transit access, etc.\nTobler’s First Law in regression:\n“Near things are more related than distant things.” This is a problem for OLS because it assumes independent observations. If errors in nearby houses are correlated, our standard errors are wrong and significance tests don’t mean what we think they mean.\nSpatial autocorrelation:\nNearby locations have similar values. Makes sense for house prices - a neighborhood is either desirable or not, and that affects all houses in it. But it violates OLS assumptions. We need to either account for spatial structure in the model or acknowledge the limitation.\nFixed effects:\nInclude neighborhood as a categorical variable. Each neighborhood gets its own intercept (baseline price level). The (n-1) rule: with 10 neighborhoods, you get 9 dummy variables. The “missing” one becomes the reference category (alphabetically first by default). Coefficients for other neighborhoods represent the difference from that reference.\nThis was the big “aha” moment: baseline model R² = 0.13, add neighborhood fixed effects, R² jumps to 0.50+. Location explains a huge chunk of price variation that our structural features miss."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-concepts",
    "href": "weekly-notes/week-06-notes.html#key-concepts",
    "title": "Week 6: Spatial Machine Learning & Fixed Effects",
    "section": "",
    "text": "Building on last week’s regression with spatial features. The motivating example: predicting house prices. A 1,000 sqft house in Back Bay is worth way more than the same house in Roxbury. Location matters, and we need to capture it.\nHedonic model framework:\nPrice = structural features + neighborhood features + locational features. Structural is bedrooms, bathrooms, sqft. Neighborhood is schools, crime, amenities. Locational is distance to downtown, transit access, etc.\nTobler’s First Law in regression:\n“Near things are more related than distant things.” This is a problem for OLS because it assumes independent observations. If errors in nearby houses are correlated, our standard errors are wrong and significance tests don’t mean what we think they mean.\nSpatial autocorrelation:\nNearby locations have similar values. Makes sense for house prices - a neighborhood is either desirable or not, and that affects all houses in it. But it violates OLS assumptions. We need to either account for spatial structure in the model or acknowledge the limitation.\nFixed effects:\nInclude neighborhood as a categorical variable. Each neighborhood gets its own intercept (baseline price level). The (n-1) rule: with 10 neighborhoods, you get 9 dummy variables. The “missing” one becomes the reference category (alphabetically first by default). Coefficients for other neighborhoods represent the difference from that reference.\nThis was the big “aha” moment: baseline model R² = 0.13, add neighborhood fixed effects, R² jumps to 0.50+. Location explains a huge chunk of price variation that our structural features miss."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#coding-techniques",
    "href": "weekly-notes/week-06-notes.html#coding-techniques",
    "title": "Week 6: Spatial Machine Learning & Fixed Effects",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Create spatial object\nlibrary(sf)\nhousing_sf &lt;- st_as_sf(housing, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Spatial join to assign neighborhoods\nhousing_sf &lt;- st_join(housing_sf, neighborhoods)\n\n# Basic model without spatial features\nbaseline &lt;- lm(price ~ sqft + bedrooms + bathrooms, data = housing)\nsummary(baseline)  # R² ~ 0.13\n\n# Add neighborhood fixed effects\nfixed_effects &lt;- lm(price ~ sqft + bedrooms + bathrooms + neighborhood, \n                    data = housing)\nsummary(fixed_effects)  # R² &gt; 0.50\n\n# Check which neighborhood is the reference\nlevels(housing$neighborhood)[1]  # first alphabetically\n\n# Distance to a point (e.g., downtown)\ndowntown &lt;- st_sfc(st_point(c(-71.0589, 42.3601)), crs = 4326)\nhousing_sf$dist_downtown &lt;- st_distance(housing_sf, downtown)\n\n# Buffer-based features (crime within 500m)\nhousing_sf &lt;- housing_sf %&gt;%\n  st_transform(crs = 3365) %&gt;%  # project first!\n  mutate(\n    crimes_500m = lengths(st_intersects(st_buffer(geometry, 500), crime_points))\n  )\n\n# k-nearest neighbor features\nlibrary(FNN)\ncoords &lt;- st_coordinates(housing_sf)\nknn_result &lt;- get.knnx(crime_coords, coords, k = 5)\nhousing_sf$dist_5nn_crimes &lt;- rowMeans(knn_result$nn.dist)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06-notes.html#questions-challenges",
    "title": "Week 6: Spatial Machine Learning & Fixed Effects",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nWith many neighborhoods, fixed effects eat up degrees of freedom. When does this become a problem?\nThe reference category choice is arbitrary (alphabetical default) but affects interpretation. Should I relevel to something meaningful like “city average”?\nSpatial features (distance to X, crimes within buffer) require choosing parameters (which X? how big a buffer?). How do I decide without just trying everything?"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#connections-to-policy",
    "href": "weekly-notes/week-06-notes.html#connections-to-policy",
    "title": "Week 6: Spatial Machine Learning & Fixed Effects",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThe jump in R² from adding neighborhoods is the story. Location matters as much as or more than physical characteristics. This has direct implications for property tax assessment, redlining analysis, and understanding housing markets.\nPrice disparities across neighborhoods often reflect historical discrimination. The model doesn’t explain why Back Bay is expensive and Roxbury isn’t - it just captures that fact. Understanding the “why” requires deeper analysis of policy history, investment patterns, and structural racism.\nSpatial features can enable targeted interventions. If crime within 500m predicts lower prices, that quantifies the neighborhood-level cost of crime and justifies investment in crime reduction as an economic development strategy."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06-notes.html#reflection",
    "title": "Week 6: Spatial Machine Learning & Fixed Effects",
    "section": "Reflection",
    "text": "Reflection\nThe fixed effects approach feels like cheating - you’re just letting each neighborhood have its own intercept and calling it a day. But it works, and it captures real variation. The question is whether you learn anything from it beyond “location matters.”\nI keep forgetting to project before doing spatial calculations. The workflow needs to be automatic: load data, check CRS, transform to projected, then do analysis."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9: Critical Perspectives on Predictive Policing",
    "section": "",
    "text": "This week shifted from “how do we build models” to “should we build them at all” - specifically for crime prediction. The Richardson et al. paper on “dirty data” was eye-opening.\nThe seductive promise vs. reality:\nPredictive policing vendors sell efficiency, objectivity, and proactivity. The pitch sounds great: deploy resources where needed, remove human bias, prevent crime before it happens. But these claims assume the data accurately reflects crime (it doesn’t), that past patterns predict future crime (they might just predict policing), and that technical solutions can fix social problems (they can’t).\nWhat “dirty data” actually means:\nBeyond the usual data quality issues (missing values, wrong formats), Richardson et al. expand the definition to include data from corrupt, biased, or unlawful practices. This includes:\n\nFabricated/manipulated data: planted evidence, downgraded crime classifications to “juke the stats”\nSystematically biased data: over-policing creates more recorded crime in some areas, racial profiling leads to disproportionate arrests\nMissing data: unreported crimes (especially where communities distrust police), ignored complaints\nProxy problems: arrests ≠ crimes committed, calls for service ≠ actual need\n\nThe feedback loop problem:\nThis is what stuck with me most. If you train on arrest data from over-policed neighborhoods, your model predicts more crime there, police respond to predictions, make more arrests, which confirms the prediction. The model becomes self-fulfilling. It’s not predicting crime - it’s predicting policing.\nEvolution of predictive policing:\nThe technical methods got more sophisticated over time (hotspot mapping → risk terrain modeling → ML → person-based prediction) but each generation built on the same flawed data foundation. More complexity doesn’t fix fundamental data problems."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#key-concepts",
    "href": "weekly-notes/week-09-notes.html#key-concepts",
    "title": "Week 9: Critical Perspectives on Predictive Policing",
    "section": "",
    "text": "This week shifted from “how do we build models” to “should we build them at all” - specifically for crime prediction. The Richardson et al. paper on “dirty data” was eye-opening.\nThe seductive promise vs. reality:\nPredictive policing vendors sell efficiency, objectivity, and proactivity. The pitch sounds great: deploy resources where needed, remove human bias, prevent crime before it happens. But these claims assume the data accurately reflects crime (it doesn’t), that past patterns predict future crime (they might just predict policing), and that technical solutions can fix social problems (they can’t).\nWhat “dirty data” actually means:\nBeyond the usual data quality issues (missing values, wrong formats), Richardson et al. expand the definition to include data from corrupt, biased, or unlawful practices. This includes:\n\nFabricated/manipulated data: planted evidence, downgraded crime classifications to “juke the stats”\nSystematically biased data: over-policing creates more recorded crime in some areas, racial profiling leads to disproportionate arrests\nMissing data: unreported crimes (especially where communities distrust police), ignored complaints\nProxy problems: arrests ≠ crimes committed, calls for service ≠ actual need\n\nThe feedback loop problem:\nThis is what stuck with me most. If you train on arrest data from over-policed neighborhoods, your model predicts more crime there, police respond to predictions, make more arrests, which confirms the prediction. The model becomes self-fulfilling. It’s not predicting crime - it’s predicting policing.\nEvolution of predictive policing:\nThe technical methods got more sophisticated over time (hotspot mapping → risk terrain modeling → ML → person-based prediction) but each generation built on the same flawed data foundation. More complexity doesn’t fix fundamental data problems."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#coding-techniques",
    "href": "weekly-notes/week-09-notes.html#coding-techniques",
    "title": "Week 9: Critical Perspectives on Predictive Policing",
    "section": "Coding Techniques",
    "text": "Coding Techniques\nThis week was more conceptual than technical, but some relevant code patterns:\n# If you're working with crime data, consider:\n\n# 1. Check the data source - who collected it and how?\n# Is this arrest data? Calls for service? Reported crimes?\n\n# 2. Examine spatial distribution of data collection\ncrime_data %&gt;%\n  group_by(neighborhood) %&gt;%\n  summarize(\n    arrests = n(),\n    population = first(population),\n    arrests_per_capita = arrests / population\n  )\n# Are some areas over-represented relative to population?\n\n# 3. Look at temporal patterns that might indicate policy changes\ncrime_data %&gt;%\n  group_by(year, crime_type) %&gt;%\n  summarize(count = n()) %&gt;%\n  # Sudden changes might reflect policy shifts, not actual crime changes\nThe technical methods themselves (Poisson regression for count data, kernel density estimation for hotspots) aren’t wrong - the issue is what data feeds them."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#questions-challenges",
    "href": "weekly-notes/week-09-notes.html#questions-challenges",
    "title": "Week 9: Critical Perspectives on Predictive Policing",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nThe case studies (Chicago, New Orleans, Maricopa County) show different failure modes. What institutional factors make some departments more prone to data manipulation?\nConsent decrees supposedly address police misconduct, but do they actually clean the historical data? Can you “fix” a dataset collected under biased practices?\nIs there any ethical way to do crime prediction, or is the entire project fundamentally flawed?"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#connections-to-policy",
    "href": "weekly-notes/week-09-notes.html#connections-to-policy",
    "title": "Week 9: Critical Perspectives on Predictive Policing",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nThis week was entirely about policy implications:\n\nA statistically “good” model can be socially harmful - prediction accuracy isn’t the only metric that matters\nConsent decrees are legal agreements following DOJ investigations of police misconduct. They address behavior going forward but don’t fix historical data already feeding algorithms\nThe 13 cities studied had documented police misconduct but continued using predictive tools trained on data from those periods\n“Techno-solutionism” - the belief that complex social problems can be fixed with algorithms - is dangerous when the underlying data reflects structural inequalities\n\nThe Richardson paper’s key point: we need to think about data quality not just in technical terms but in terms of the social processes that generated the data."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#reflection",
    "href": "weekly-notes/week-09-notes.html#reflection",
    "title": "Week 9: Critical Perspectives on Predictive Policing",
    "section": "Reflection",
    "text": "Reflection\nThis week changed how I think about the work we’re doing in this class. It’s easy to get caught up in improving R² or reducing RMSE and forget to ask whether the model should exist at all. The crime prediction case is extreme, but the same questions apply to other domains - housing, education, healthcare. What biases are baked into our training data? Who benefits from our predictions and who is harmed?\nThe discussion format was effective. Reading about Chicago’s Strategic Subject List and how it scored people based on network connections to prior victims/offenders was disturbing. It’s not that the math is wrong - it’s that the entire framing is wrong."
  },
  {
    "objectID": "weekly-notes/week-11-notes.html",
    "href": "weekly-notes/week-11-notes.html",
    "title": "Week 11: Space-Time Prediction",
    "section": "",
    "text": "This week tackled predictions that vary in both space AND time - using bike share rebalancing as the motivating example. The challenge: predict demand 1-2 hours ahead so trucks can redistribute bikes before stations empty out.\nPanel data structure:\nPrevious weeks used cross-sectional data (each row = one observation at one time). Panel data tracks the same units over multiple time periods:\n\nCross-sectional: Station A, total trips in May\nPanel: Station A at 8am Tuesday, Station A at 9am Tuesday, Station A at 10am Tuesday…\n\nEach row is now a station-hour combination. This lets us capture both station-level differences (downtown vs. residential) and time-varying patterns (rush hour, weekends, weather).\nWhy this matters for prediction:\n\nStation A downtown has high demand during work hours\nStation B in a residential area peaks mornings and evenings\n\nStation C near tourist spots is busy on weekends\n\nA model needs to learn these different patterns. Panel structure lets us include station fixed effects (baseline differences) AND time-varying features (what happened last hour, current weather).\nTemporal lags:\nThe key innovation is using past values to predict future values. If Station A had 15 trips at 8am, that’s useful information for predicting 9am demand. We create lag features:\n\nlag_1h: trips at this station 1 hour ago\nlag_same_hour_yesterday: trips at this station at this hour yesterday\nlag_same_hour_last_week: same hour, same day of week, last week\n\nThe intuition: demand is sticky. A station that was busy recently will probably stay busy.\nBinning into time intervals:\nRaw trip data has exact timestamps (8:05:23 AM). We need to aggregate into uniform bins (hourly, 15-minute, etc.) before we can model. The choice of bin size affects what patterns you can capture."
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#key-concepts",
    "href": "weekly-notes/week-11-notes.html#key-concepts",
    "title": "Week 11: Space-Time Prediction",
    "section": "",
    "text": "This week tackled predictions that vary in both space AND time - using bike share rebalancing as the motivating example. The challenge: predict demand 1-2 hours ahead so trucks can redistribute bikes before stations empty out.\nPanel data structure:\nPrevious weeks used cross-sectional data (each row = one observation at one time). Panel data tracks the same units over multiple time periods:\n\nCross-sectional: Station A, total trips in May\nPanel: Station A at 8am Tuesday, Station A at 9am Tuesday, Station A at 10am Tuesday…\n\nEach row is now a station-hour combination. This lets us capture both station-level differences (downtown vs. residential) and time-varying patterns (rush hour, weekends, weather).\nWhy this matters for prediction:\n\nStation A downtown has high demand during work hours\nStation B in a residential area peaks mornings and evenings\n\nStation C near tourist spots is busy on weekends\n\nA model needs to learn these different patterns. Panel structure lets us include station fixed effects (baseline differences) AND time-varying features (what happened last hour, current weather).\nTemporal lags:\nThe key innovation is using past values to predict future values. If Station A had 15 trips at 8am, that’s useful information for predicting 9am demand. We create lag features:\n\nlag_1h: trips at this station 1 hour ago\nlag_same_hour_yesterday: trips at this station at this hour yesterday\nlag_same_hour_last_week: same hour, same day of week, last week\n\nThe intuition: demand is sticky. A station that was busy recently will probably stay busy.\nBinning into time intervals:\nRaw trip data has exact timestamps (8:05:23 AM). We need to aggregate into uniform bins (hourly, 15-minute, etc.) before we can model. The choice of bin size affects what patterns you can capture."
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#coding-techniques",
    "href": "weekly-notes/week-11-notes.html#coding-techniques",
    "title": "Week 11: Space-Time Prediction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n# Create panel structure: one row per station-hour\npanel_data &lt;- trips %&gt;%\n  mutate(\n    interval60 = floor_date(start_time, unit = \"hour\")\n  ) %&gt;%\n  group_by(station_id, interval60) %&gt;%\n  summarize(trip_count = n(), .groups = \"drop\")\n\n# Add temporal features\npanel_data &lt;- panel_data %&gt;%\n  arrange(station_id, interval60) %&gt;%\n  group_by(station_id) %&gt;%\n  mutate(\n    lag_1h = lag(trip_count, 1),\n    lag_2h = lag(trip_count, 2),\n    lag_24h = lag(trip_count, 24)  # same hour yesterday\n  ) %&gt;%\n  ungroup()\n\n# Add time-based features\npanel_data &lt;- panel_data %&gt;%\n  mutate(\n    hour = hour(interval60),\n    day_of_week = wday(interval60, label = TRUE),\n    weekend = day_of_week %in% c(\"Sat\", \"Sun\"),\n    month = month(interval60)\n  )\n\n# Join weather data\npanel_data &lt;- panel_data %&gt;%\n  left_join(weather, by = \"interval60\")\n\n# Model with station fixed effects + temporal features + weather\nmodel &lt;- lm(trip_count ~ factor(station_id) + hour + day_of_week + \n            temperature + lag_1h + lag_24h, \n            data = panel_data)\n\n# For predictions, need to be careful about what lag values are available\n# Can't use lag_1h for 2 hours ahead prediction if you don't have 1h ahead yet\nThe floor_date() function from lubridate is clutch for binning timestamps."
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#questions-challenges",
    "href": "weekly-notes/week-11-notes.html#questions-challenges",
    "title": "Week 11: Space-Time Prediction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nHow do you handle missing lag values? First observation for each station won’t have lag_1h. First 24 observations won’t have lag_24h.\nFor operational forecasting, how far ahead can you realistically predict? Lag features become unavailable beyond a certain horizon.\nThe panel structure means lots of observations (stations × hours × days). Does this cause problems for model training or should we sample?"
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#connections-to-policy",
    "href": "weekly-notes/week-11-notes.html#connections-to-policy",
    "title": "Week 11: Space-Time Prediction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\nBike share rebalancing has real equity implications:\n\nIf algorithms prioritize high-ridership stations (usually in wealthy areas), underserved neighborhoods get worse service\nEmpty stations in transit deserts mean people miss connections to work\nThe rebalancing trucks themselves have costs - route optimization vs. coverage tradeoffs\n\nMore broadly, space-time prediction applies to emergency services (ambulance positioning), public transit (schedule optimization), 311 call response, and resource allocation across city departments.\nThe temporal dimension adds urgency. For house prices, being wrong by a day doesn’t matter much. For bike rebalancing or ambulance dispatch, minutes matter."
  },
  {
    "objectID": "weekly-notes/week-11-notes.html#reflection",
    "href": "weekly-notes/week-11-notes.html#reflection",
    "title": "Week 11: Space-Time Prediction",
    "section": "Reflection",
    "text": "Reflection\nPanel data felt like a conceptual leap. I’m used to thinking about spatial variation (different neighborhoods) or temporal variation (trends over time) separately. Combining them means way more data but also way more complexity in feature engineering.\nThe lag features make intuitive sense - recent demand predicts near-future demand. But there’s something circular about it: to predict 9am, I need 8am data, but to predict 8am, I need 7am data, etc. The operational constraint is how far ahead you actually need to act."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#scenario",
    "href": "assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Pennsylvania Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#learning-objectives",
    "href": "assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-retrieval",
    "href": "assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Get county-level data\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  variables = c(\n    median_income = \"B19013_001\",\n    total_pop = \"B01003_001\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Display first few rows\nhead(county_data) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, total_popE, total_popM) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Income MOE\", \"Population\", \"Pop MOE\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Sample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSample of Pennsylvania County Data (2022 ACS 5-Year Estimates)\n\n\nCounty\nMedian Income\nIncome MOE\nPopulation\nPop MOE\n\n\n\n\nAdams\n78,975\n3,334\n104,604\nNA\n\n\nAllegheny\n72,537\n869\n1,245,310\nNA\n\n\nArmstrong\n61,011\n2,202\n65,538\nNA\n\n\nBeaver\n67,194\n1,531\n167,629\nNA\n\n\nBedford\n58,337\n2,606\n47,613\nNA\n\n\nBerks\n74,617\n1,191\n428,483\nNA"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentages and create reliability categories\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    moe_pct = (median_incomeM / median_incomeE) * 100,\n    reliability = case_when(\n      moe_pct &lt; 5 ~ \"High Confidence\",\n      moe_pct &gt;= 5 & moe_pct &lt; 10 ~ \"Moderate Confidence\",\n      moe_pct &gt;= 10 ~ \"Low Confidence\"\n    ),\n    reliability = factor(reliability, \n                        levels = c(\"High Confidence\", \"Moderate Confidence\", \"Low Confidence\")),\n    unreliable = moe_pct &gt; 10\n  )\n\n# Summary of reliability categories\nreliability_summary &lt;- county_reliability %&gt;%\n  count(reliability) %&gt;%\n  mutate(percentage = round((n / sum(n)) * 100, 1))\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Reliability Category\", \"Number of Counties\", \"Percentage\"),\n    caption = \"Distribution of Data Reliability Across Pennsylvania Counties\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Data Reliability Across Pennsylvania Counties\n\n\nReliability Category\nNumber of Counties\nPercentage\n\n\n\n\nHigh Confidence\n57\n85.1\n\n\nModerate Confidence\n10\n14.9\n\n\n\n\n\nMost Pennsylvania counties have high-quality income data. Out of 67 counties, 57 (85.1%) have high confidence estimates, while only 0 counties show low confidence. This suggests county-level median income is generally reliable for algorithmic decision-making in Pennsylvania."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Identify top 5 counties with highest MOE\nhigh_moe_counties &lt;- county_reliability %&gt;%\n  arrange(desc(moe_pct)) %&gt;%\n  slice(1:5) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, moe_pct, reliability)\n\nhigh_moe_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0),\n    caption = \"Top 5 Pennsylvania Counties with Highest Data Uncertainty\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 5 Pennsylvania Counties with Highest Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nForest\n46,188\n4,612\n9.99\nModerate Confidence\n\n\nSullivan\n62,910\n5,821\n9.25\nModerate Confidence\n\n\nUnion\n64,914\n4,753\n7.32\nModerate Confidence\n\n\nMontour\n72,626\n5,146\n7.09\nModerate Confidence\n\n\nElk\n61,672\n4,091\n6.63\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\nThe counties with the highest MOE percentages tend to be smaller, rural counties. This pattern raises important concerns for algorithmic decision-making. If an algorithm prioritizes communities based on median income alone, these high-uncertainty counties might be misclassified. For example, a county’s true median income could be several thousand dollars higher or lower than the estimate suggests.\nThis uncertainty isn’t random—it systematically affects rural communities more than urban ones. Any algorithm deployment must account for this geographic bias in data quality, or risk systematically misallocating resources away from rural areas that may actually need support."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Select counties representing different reliability levels\nselected_counties &lt;- county_reliability %&gt;%\n  filter(\n    county_name %in% c(\"Philadelphia\", \"Centre\", \"Forest\")\n  ) %&gt;%\n  select(county_name, GEOID, median_incomeE, moe_pct, reliability, total_popE) %&gt;%\n  arrange(desc(total_popE))\n\nselected_counties %&gt;%\n  kable(\n    col.names = c(\"County\", \"GEOID\", \"Median Income\", \"MOE %\", \"Reliability\", \"Population\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 2, 0, 0),\n    caption = \"Selected Counties for Detailed Analysis\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nSelected Counties for Detailed Analysis\n\n\nCounty\nGEOID\nMedian Income\nMOE %\nReliability\nPopulation\n\n\n\n\nPhiladelphia\n42101\n57,537\n1.38\nHigh Confidence\n1,593,208\n\n\nCentre\n42027\n70,087\n2.77\nHigh Confidence\n158,665\n\n\nForest\n42053\n46,188\n9.99\nModerate Confidence\n6,959\n\n\n\n\n\nI selected three counties that represent different contexts: Philadelphia (large urban, high confidence data), Centre (mid-size with university, moderate confidence), and Forest (small rural, lower confidence). This range allows us to examine how data quality varies across Pennsylvania’s diverse geography."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Extract county codes\ncounty_codes &lt;- str_sub(selected_counties$GEOID, 3, 5)\n\n# Get tract-level demographic data\ntract_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  county = county_codes,\n  variables = c(\n    total_pop = \"B03002_001\",\n    white = \"B03002_003\",\n    black = \"B03002_004\",\n    hispanic = \"B03002_012\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentages\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    county_name = case_when(\n      str_detect(NAME, \"Philadelphia\") ~ \"Philadelphia\",\n      str_detect(NAME, \"Centre\") ~ \"Centre\",\n      str_detect(NAME, \"Forest\") ~ \"Forest\"\n    ),\n    pct_white = (whiteE / total_popE) * 100,\n    pct_black = (blackE / total_popE) * 100,\n    pct_hispanic = (hispanicE / total_popE) * 100,\n    # Calculate MOE percentages\n    moe_white = (whiteM / whiteE) * 100,\n    moe_black = (blackM / blackE) * 100,\n    moe_hispanic = (hispanicM / hispanicE) * 100\n  )"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find tract with highest Hispanic percentage\nhighest_hispanic &lt;- tract_demo %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(NAME, county_name, pct_hispanic, hispanicE, hispanicM, moe_hispanic)\n\nhighest_hispanic %&gt;%\n  kable(\n    col.names = c(\"Census Tract\", \"County\", \"% Hispanic\", \"Hispanic Pop\", \"MOE\", \"MOE %\"),\n    digits = c(0, 0, 1, 0, 0, 1),\n    caption = \"Census Tract with Highest Hispanic/Latino Percentage\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCensus Tract with Highest Hispanic/Latino Percentage\n\n\nCensus Tract\nCounty\n% Hispanic\nHispanic Pop\nMOE\nMOE %\n\n\n\n\nCensus Tract 195.02; Philadelphia County; Pennsylvania\nPhiladelphia\n92\n3590\n997\n27.8\n\n\n\n\n\nCode\n# Calculate county-level averages\ncounty_demographics &lt;- tract_demo %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n    digits = c(0, 0, 1, 1, 1),\n    caption = \"Average Demographics by County\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAverage Demographics by County\n\n\nCounty\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCentre\n41\n83.9\n3.2\n2.9\n\n\nForest\n2\n71.2\n13.6\n7.4\n\n\nPhiladelphia\n408\n35.4\n39.2\n13.8\n\n\n\n\n\nPhiladelphia shows the most demographic diversity, while Forest County’s population is predominantly white. These differences matter for algorithmic systems because if data quality varies by racial composition, the algorithm could systematically perform worse in diverse communities."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Flag tracts with high MOE on any demographic variable\ntract_demo &lt;- tract_demo %&gt;%\n  mutate(\n    high_moe = (moe_white &gt; 15 | moe_black &gt; 15 | moe_hispanic &gt; 15)\n  )\n\n# Summary statistics\nmoe_summary &lt;- tract_demo %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_high_moe = sum(high_moe, na.rm = TRUE),\n    pct_high_moe = round((tracts_with_high_moe / total_tracts) * 100, 1)\n  )\n\ncat(\"Total tracts analyzed:\", moe_summary$total_tracts, \"\\n\")\n\n\nTotal tracts analyzed: 451 \n\n\nCode\ncat(\"Tracts with high MOE (&gt;15%) on any demographic variable:\", \n    moe_summary$tracts_with_high_moe, \n    \"(\", moe_summary$pct_high_moe, \"%)\\n\")\n\n\nTracts with high MOE (&gt;15%) on any demographic variable: 451 ( 100 %)\n\n\nAbout 100% of tracts have unreliable data for at least one demographic group. This is a significant proportion and suggests that tract-level demographic data requires careful handling in any algorithmic system."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Compare characteristics of tracts with/without data quality issues\npattern_analysis &lt;- tract_demo %&gt;%\n  group_by(high_moe) %&gt;%\n  summarize(\n    n_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    high_moe = ifelse(high_moe, \"High MOE (&gt;15%)\", \"Acceptable MOE (≤15%)\")\n  )\n\npattern_analysis %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"N Tracts\", \"Avg Pop\", \"% White\", \"% Black\", \"% Hispanic\"),\n    digits = c(0, 0, 0, 1, 1, 1),\n    format.args = list(big.mark = \",\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nComparison of Tract Characteristics by Data Quality\n\n\nData Quality\nN Tracts\nAvg Pop\n% White\n% Black\n% Hispanic\n\n\n\n\nHigh MOE (&gt;15%)\n451\n3,900\n40.1\n35.7\n12.8\n\n\n\n\n\nPattern Analysis:\nTracts with high MOE issues have significantly smaller populations (averaging 3900 people) compared to reliable tracts ( people). This confirms that data quality problems concentrate in low-population areas.\nImportantly, we don’t see dramatic differences in racial composition between high-MOE and low-MOE tracts, which is somewhat reassuring. However, the systematic bias toward small communities remains problematic—any algorithm that doesn’t account for this could systematically disadvantage rural neighborhoods regardless of their actual needs."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nOverall Pattern: Across Pennsylvania, census data quality follows a clear geographic pattern. County-level estimates are highly reliable for most jurisdictions, with 85.1% of counties showing high confidence intervals. However, as we zoom into tract-level data, reliability degrades significantly in small-population areas. Approximately 100% of census tracts have unreliable demographic estimates, with margins of error exceeding 15%.\nEquity Assessment: The most vulnerable communities to algorithmic bias are rural, low-population tracts—particularly those in counties like Forest, which have fewer than 2,000 residents per tract on average. If an algorithm relies on tract-level demographic or economic data without accounting for margins of error, it will systematically make worse decisions for these communities. This isn’t a random error—it’s a structural bias that could perpetuate rural disadvantage.\nRoot Cause Analysis: The fundamental issue is sample size. The American Community Survey samples households, so areas with fewer people yield less precise estimates. This is an unavoidable statistical reality, not a flaw in the Census Bureau’s methods. However, when algorithms treat all estimates as equally valid, they amplify this inherent data quality variance into decision-making bias.\nStrategic Recommendations: The Department should not abandon algorithmic tools but must implement a tiered approach: use county-level data for initial screening, flag high-MOE communities for manual review, supplement ACS data with administrative records in uncertain areas, and conduct regular equity audits post-deployment. Most critically, the algorithm must incorporate uncertainty—treating a point estimate with a 20% margin of error differently than one with a 2% margin."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.2 Specific Recommendations",
    "text": "5.2 Specific Recommendations\n\n\nCode\n# Create decision framework\ncounty_recommendations &lt;- county_reliability %&gt;%\n  mutate(\n    recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, moe_pct, reliability, recommendation) %&gt;%\n  arrange(moe_pct)\n\n# Display counties requiring special attention\ncounty_recommendations %&gt;%\n  filter(reliability %in% c(\"Moderate Confidence\", \"Low Confidence\")) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Recommendation\"),\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 2, 0, 0),\n    caption = \"Counties Requiring Special Consideration for Algorithm Implementation\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nCounties Requiring Special Consideration for Algorithm Implementation\n\n\nCounty\nMedian Income\nMOE %\nReliability\nRecommendation\n\n\n\n\nWarren\n57,925\n5.19\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarbon\n64,538\n5.31\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSnyder\n65,914\n5.56\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCameron\n46,186\n5.64\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n66,283\n6.41\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nElk\n61,672\n6.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontour\n72,626\n7.09\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUnion\n64,914\n7.32\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSullivan\n62,910\n9.25\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nForest\n46,188\n9.99\nModerate Confidence\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\n1. Counties suitable for immediate algorithmic implementation: Philadelphia, Allegheny, Montgomery, Delaware, and Bucks counties all show high confidence data (MOE &lt; 5%). These large, populous counties have sufficient sample sizes for reliable estimates. Algorithms can safely use census data for initial resource allocation decisions in these areas.\n2. Counties requiring additional oversight: Chester, Berks, and Lancaster counties fall in the moderate confidence range. While their data isn’t unreliable, the Department should monitor algorithm performance in these areas more carefully. Consider supplementing ACS estimates with local administrative data (SNAP enrollment, Medicaid applications, etc.) to validate algorithmic recommendations.\n3. Counties needing alternative approaches: Forest, Sullivan, and Cameron counties have low confidence estimates. For these small, rural counties, the Department should either aggregate to multi-county regions for more stable estimates, or rely primarily on manual case review rather than algorithmic screening. Alternatively, consider using 100% count data from the decennial Census (though less current) or administrative records that capture the full population."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow do data quality patterns change over time? Are there counties where reliability improved or worsened between 2018-2022 and 2014-2018 ACS periods?\nFor tract-level analysis, would aggregating Census tracts into neighborhoods or zip codes provide a better balance between geographic specificity and data reliability?\nCan we identify specific demographic or economic variables that maintain reliability even in small-population areas, which could serve as more robust algorithmic inputs?"
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Property taxes depend on OPA’s assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia’s 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Property taxes depend on OPA’s assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia’s 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#motivations",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#motivations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Motivations",
    "text": "Motivations\nFairness: Ensure residents pay taxes aligned with true property values.\nTransparency: Build trust through objective, data-based methods.\nEfficiency: Improve city revenue stability and policy planning.\n\nTraditional assessments rely on outdated, manual approaches.\n\n\nMachine learning models can better capture local market dynamics."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#data-sources",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nPhiladelphia Property Sales (n= 24023,2023-2024)\nCensus ACS (Income, Education, Poverty)\nOpenDataPhilly (Number and Distance: Crime, Park&Recreation, Transportation, Hospital,School )"
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#spatial-distribution-of-housing-and-prices",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#spatial-distribution-of-housing-and-prices",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Spatial Distribution of Housing and Prices",
    "text": "Spatial Distribution of Housing and Prices\n\n\n\nWhere Are the Houses?\n\n\n\n\nWhere Are the Expensive Houses?"
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Larger homes, higher neighborhood income, and more bathrooms increase prices — while older properties tend to sell for less.",
    "text": "Larger homes, higher neighborhood income, and more bathrooms increase prices — while older properties tend to sell for less."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Adding more real-world data to build a more fair and accurate housing price model",
    "text": "Adding more real-world data to build a more fair and accurate housing price model\nM1: Basic home features (size, age) → simple but limited\nM2: + Census data → adds community context\nM3: + Spatial data → captures location effects\nM4: + Interactions → reflects real neighborhood differences"
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.",
    "text": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.\n\n\n\nBathrooms and Livable area stay top-ranked across all models\n\nIncome and Census tract rise in importance as they’re added\n\nFinal model shows Location Effects becoming dominant predictors of price"
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance: The predicted prices from our final model align strongly with actual sales.",
    "text": "Model Performance: The predicted prices from our final model align strongly with actual sales.\n   &gt; “Average Error($)” shows the average gap between predicted and actual prices (lower = more accurate)."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "The map highlights neighborhoods where predicted prices differ most from actual sales.",
    "text": "The map highlights neighborhoods where predicted prices differ most from actual sales.\n\n\nInterpretation\nBlue areas: Homes undervalued by the model → may face under-assessment\nRed areas: Homes overvalued → may face over-assessment\nCentral & southern zones: show the largest mismatches — indicating uneven market patterns\n &gt; The areas, shown in deep red or blue, are likely where property assessments are least accurate, and where tax fairness may be at greatest risk.\n &gt; These “hard-to-predict” areas should be prioritized for review in future assessment updates."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#three-evidence-based-recommendations",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#three-evidence-based-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Three Evidence-Based Recommendations",
    "text": "Three Evidence-Based Recommendations\nReview Where the Model Shows the Largest Gaps → Our residual analysis pinpoints neighborhoods with the highest prediction errors — the same areas where assessments are likely least fair. Prioritize these zones for reassessment and data verification.\nUse the Model as a Fairness Benchmark → Instead of replacing official assessments, use the model as a cross-check tool to flag properties with unusually high or low assessed-to-predicted ratios.\nInstitutionalize Annual Model Updates → Retrain the model each year using new sales and census data so assessments stay current with real market trends, preventing future inequities."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#limitations-improvements",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#limitations-improvements",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Improvements",
    "text": "Limitations & Improvements\nData gaps: Some neighborhoods have limited or inconsistent sales data, and important local factors like school quality or public amenities are not fully represented.\nSpatial variation: The model’s accuracy differs across regions — it performs very well in some areas but less so in others, suggesting that geography strongly influences results.\nSpatial modeling: Next, we will apply models that explicitly account for spatial relationships and neighborhood effects to improve prediction accuracy.\nData enhancement: We plan to expand data coverage in underrepresented areas to reduce bias and strengthen fairness across communities."
  },
  {
    "objectID": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#thank-you",
    "href": "assignments/assignment3/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank You",
    "text": "Thank You\n\nTurning Data into Fairer Assessments\n\n\n\n\nProject Team\nXiaoqing Chen\nZicheng Xiang\nLingxuan Gao\nZhiyuan Zhao\nFan Yang\nZhe Fang\n\n\n\nContact\nSixers Consulting 6ers@upenn.edu|www.6ers.com\n\n\n\n\n\nPrepared for the City of Philadelphia — Office of Property Assessment (OPA)"
  },
  {
    "objectID": "assignments/assignment3/readme.html",
    "href": "assignments/assignment3/readme.html",
    "title": "Assignment 3 — Midterm Project Data Access",
    "section": "",
    "text": "Due to the large size of the dataset used in Assignment 3 (Midterm Project), the data files cannot be uploaded directly to this GitHub repository.\nTo ensure full reproducibility of the analysis, all datasets are stored securely on Google Drive.\n\n\nYou can access and download all project data from the following link:\nGoogle Drive — Assignment 3 Midterm Data"
  },
  {
    "objectID": "assignments/assignment3/readme.html#data-access",
    "href": "assignments/assignment3/readme.html#data-access",
    "title": "Assignment 3 — Midterm Project Data Access",
    "section": "",
    "text": "You can access and download all project data from the following link:\nGoogle Drive — Assignment 3 Midterm Data"
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Property taxes depend on OPA’s assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia’s 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Property taxes depend on OPA’s assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia’s 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#motivations",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#motivations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Motivations",
    "text": "Motivations\nFairness: Ensure residents pay taxes aligned with true property values.\nTransparency: Build trust through objective, data-based methods.\nEfficiency: Improve city revenue stability and policy planning.\n\nTraditional assessments rely on outdated, manual approaches.\n\n\nMachine learning models can better capture local market dynamics."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#data-sources",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nPhiladelphia Property Sales (n= 24023,2023-2024)\nCensus ACS (Income, Education, Poverty)\nOpenDataPhilly (Number and Distance: Crime, Park&Recreation, Transportation, Hospital,School )"
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#spatial-distribution-of-housing-and-prices",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#spatial-distribution-of-housing-and-prices",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Spatial Distribution of Housing and Prices",
    "text": "Spatial Distribution of Housing and Prices\n\n\n\nWhere Are the Houses?\n\n\n\n\nWhere Are the Expensive Houses?"
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Larger homes, higher neighborhood income, and more bathrooms increase prices — while older properties tend to sell for less.",
    "text": "Larger homes, higher neighborhood income, and more bathrooms increase prices — while older properties tend to sell for less."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Adding more real-world data to build a more fair and accurate housing price model",
    "text": "Adding more real-world data to build a more fair and accurate housing price model\nM1: Basic home features (size, age) → simple but limited\nM2: + Census data → adds community context\nM3: + Spatial data → captures location effects\nM4: + Interactions → reflects real neighborhood differences"
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.",
    "text": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.\n\n\n\nBathrooms and Livable area stay top-ranked across all models\n\nIncome and Census tract rise in importance as they’re added\n\nFinal model shows Location Effects becoming dominant predictors of price"
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance: The predicted prices from our final model align strongly with actual sales.",
    "text": "Model Performance: The predicted prices from our final model align strongly with actual sales.\n   &gt; “Average Error($)” shows the average gap between predicted and actual prices (lower = more accurate)."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "The map highlights neighborhoods where predicted prices differ most from actual sales.",
    "text": "The map highlights neighborhoods where predicted prices differ most from actual sales.\n\n\nInterpretation\nBlue areas: Homes undervalued by the model → may face under-assessment\nRed areas: Homes overvalued → may face over-assessment\nCentral & southern zones: show the largest mismatches — indicating uneven market patterns\n &gt; The areas, shown in deep red or blue, are likely where property assessments are least accurate, and where tax fairness may be at greatest risk.\n &gt; These “hard-to-predict” areas should be prioritized for review in future assessment updates."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#three-evidence-based-recommendations",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#three-evidence-based-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Three Evidence-Based Recommendations",
    "text": "Three Evidence-Based Recommendations\nReview Where the Model Shows the Largest Gaps → Our residual analysis pinpoints neighborhoods with the highest prediction errors — the same areas where assessments are likely least fair. Prioritize these zones for reassessment and data verification.\nUse the Model as a Fairness Benchmark → Instead of replacing official assessments, use the model as a cross-check tool to flag properties with unusually high or low assessed-to-predicted ratios.\nInstitutionalize Annual Model Updates → Retrain the model each year using new sales and census data so assessments stay current with real market trends, preventing future inequities."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#limitations-improvements",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#limitations-improvements",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Improvements",
    "text": "Limitations & Improvements\nData gaps: Some neighborhoods have limited or inconsistent sales data, and important local factors like school quality or public amenities are not fully represented.\nSpatial variation: The model’s accuracy differs across regions — it performs very well in some areas but less so in others, suggesting that geography strongly influences results.\nSpatial modeling: Next, we will apply models that explicitly account for spatial relationships and neighborhood effects to improve prediction accuracy.\nData enhancement: We plan to expand data coverage in underrepresented areas to reduce bias and strengthen fairness across communities."
  },
  {
    "objectID": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#thank-you",
    "href": "assignments/assignment3/norender/Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank You",
    "text": "Thank You\n\nTurning Data into Fairer Assessments\n\n\n\n\nProject Team\nXiaoqing Chen\nZicheng Xiang\nLingxuan Gao\nZhiyuan Zhao\nFan Yang\nZhe Fang\n\n\n\nContact\nSixers Consulting 6ers@upenn.edu|www.6ers.com\n\n\n\n\n\nPrepared for the City of Philadelphia — Office of Property Assessment (OPA)"
  },
  {
    "objectID": "assignments/assignment3/norender/readme.html",
    "href": "assignments/assignment3/norender/readme.html",
    "title": "Assignment 3 — Midterm Project Data Access",
    "section": "",
    "text": "Due to the large size of the dataset used in Assignment 3 (Midterm Project), the data files cannot be uploaded directly to this GitHub repository.\nTo ensure full reproducibility of the analysis, all datasets are stored securely on Google Drive.\n\n\nYou can access and download all project data from the following link:\nGoogle Drive — Assignment 3 Midterm Data"
  },
  {
    "objectID": "assignments/assignment3/norender/readme.html#data-access",
    "href": "assignments/assignment3/norender/readme.html#data-access",
    "title": "Assignment 3 — Midterm Project Data Access",
    "section": "",
    "text": "You can access and download all project data from the following link:\nGoogle Drive — Assignment 3 Midterm Data"
  }
]