set.seed(5080)
# Create visualization theme
theme_map <- function() {
theme_void() +
theme(
plot.title = element_text(face = "bold", size = 12),
plot.subtitle = element_text(color = "gray30", size = 10),
legend.position = "right"
)
}
theme_set(theme_map())
cat("✓ Packages loaded successfully\n")
#| message: false
#| output: false
# Load police districts for cross-validation
policeDistricts <-
st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
st_transform('ESRI:102271') %>%
dplyr::select(District = dist_num)
# Load Chicago boundary
chicagoBoundary <-
st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") %>%
st_transform('ESRI:102271')
# Load the downloaded data
violations <- read_csv("data/311_Service_Requests_-_Sanitation_Code_Complaints_-_Historical_20251114.csv") %>%
# Convert to sf object
filter(!is.na(Latitude), !is.na(Longitude)) %>%
st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
st_transform('ESRI:102271') %>%
# Parse date
mutate(
creation_date = mdy(`Creation Date`),
year = year(creation_date)
) %>%
# Keep only necessary columns
dplyr::select(
creation_date,
year,
status = Status,
violation_type = `What is the Nature of this Code Violation?`
)
cat("✓ Loaded sanitation violations\n")
cat("  - Total violations:", nrow(violations), "\n")
cat("  - Date range:", min(violations$creation_date), "to",
max(violations$creation_date), "\n")
#| fig-width: 10
#| fig-height: 5
# Point map
p1 <- ggplot() +
geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
geom_sf(data = violations, color = "#d62828", size = 0.1, alpha = 0.3) +
labs(
title = "Sanitation Code Violations",
subtitle = paste0("Chicago 2017, n = ", nrow(violations))
) +
theme_map()
# Density surface
p2 <- ggplot() +
geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
geom_density_2d_filled(
data = data.frame(st_coordinates(violations)),
aes(X, Y),
alpha = 0.7,
bins = 8
) +
scale_fill_viridis_d(
option = "plasma",
direction = -1,
guide = "none"
) +
labs(
title = "Density Surface",
subtitle = "Higher concentrations in certain areas"
) +
theme_map()
p1 + p2
# Create fishnet grid
fishnet <- st_make_grid(
chicagoBoundary,
cellsize = 500,
square = TRUE
) %>%
st_sf() %>%
mutate(uniqueID = row_number())
# Keep only cells intersecting Chicago
fishnet <- fishnet[chicagoBoundary, ]
cat("✓ Created fishnet grid\n")
cat("  - Number of cells:", nrow(fishnet), "\n")
cat("  - Cell size: 500 x 500 meters\n")
# Count violations per cell
violations_fishnet <- st_join(violations, fishnet, join = st_within) %>%
st_drop_geometry() %>%
group_by(uniqueID) %>%
summarize(countViolations = n())
# Join back to fishnet
fishnet <- fishnet %>%
left_join(violations_fishnet, by = "uniqueID") %>%
mutate(countViolations = replace_na(countViolations, 0))
# Summary statistics
cat("\nViolation count distribution:\n")
summary(fishnet$countViolations)
cat("\nCells with zero violations:",
sum(fishnet$countViolations == 0), "/", nrow(fishnet),
"(", round(100 * sum(fishnet$countViolations == 0) / nrow(fishnet), 1), "%)\n")
#| fig-width: 8
#| fig-height: 6
ggplot() +
geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +
geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
scale_fill_viridis_c(
name = "Violations",
option = "plasma",
trans = "sqrt"
) +
labs(
title = "Sanitation Code Violations by Grid Cell",
subtitle = "500m x 500m cells, Chicago 2017"
) +
theme_map()
# Convert to point pattern
violations_ppp <- as.ppp(
st_coordinates(violations),
W = as.owin(st_bbox(chicagoBoundary))
)
# Calculate KDE
kde_violations <- density.ppp(
violations_ppp,
sigma = 1000,
edge = TRUE
)
# Convert to raster and extract to fishnet
kde_raster <- rast(kde_violations)
fishnet <- fishnet %>%
mutate(
kde_value = terra::extract(
kde_raster,
vect(fishnet),
fun = mean,
na.rm = TRUE
)[, 2]
)
#| fig-width: 8
#| fig-height: 6
ggplot() +
geom_sf(data = fishnet, aes(fill = kde_value), color = NA) +
geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
scale_fill_viridis_c(
name = "KDE Value",
option = "plasma"
) +
labs(
title = "Kernel Density Estimation Baseline",
subtitle = "Simple spatial smoothing - our benchmark"
) +
theme_map()
#| message: false
# Try to load from local file first (recommended)
if (file.exists("data/abandoned_cars_2017.csv")) {
abandoned_cars <- read_csv("data/abandoned_cars_2017.csv") %>%
filter(!is.na(Latitude), !is.na(Longitude)) %>%
st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
st_transform('ESRI:102271')
cat("✓ Loaded from local file\n")
} else {
# Fallback: Try API (may be slow or fail)
abandoned_cars <- read_csv("https://data.cityofchicago.org/resource/3c9v-pnva.csv?$limit=50000&$where=creation_date between '2017-01-01T00:00:00' and '2017-12-31T23:59:59'") %>%
filter(!is.na(latitude), !is.na(longitude)) %>%
st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
st_transform('ESRI:102271')
cat("✓ Loaded from API\n")
}
cat("  - Number of abandoned vehicle calls:", nrow(abandoned_cars), "\n")
# Aggregate to fishnet
abandoned_fishnet <- st_join(abandoned_cars, fishnet, join = st_within) %>%
st_drop_geometry() %>%
group_by(uniqueID) %>%
summarize(abandoned_cars = n())
fishnet <- fishnet %>%
left_join(abandoned_fishnet, by = "uniqueID") %>%
mutate(abandoned_cars = replace_na(abandoned_cars, 0))
summary(fishnet$abandoned_cars)
#| fig-width: 10
#| fig-height: 4
p1 <- ggplot() +
geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +
scale_fill_viridis_c(name = "Count", option = "magma") +
labs(title = "Abandoned Vehicle Calls") +
theme_map()
p2 <- ggplot() +
geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +
scale_fill_viridis_c(name = "Count", option = "plasma") +
labs(title = "Sanitation Violations") +
theme_map()
p1 + p2 +
plot_annotation(
title = "Comparing Spatial Patterns",
subtitle = "Do these two phenomena co-occur?"
)
# Calculate mean distance to 3 nearest abandoned cars
fishnet_coords <- st_coordinates(st_centroid(fishnet))
abandoned_coords <- st_coordinates(abandoned_cars)
nn_result <- get.knnx(abandoned_coords, fishnet_coords, k = 3)
fishnet <- fishnet %>%
mutate(abandoned_cars.nn = rowMeans(nn_result$nn.dist))
summary(fishnet$abandoned_cars.nn)
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {
coords <- st_coordinates(st_centroid(data))
neighbors <- knn2nb(knearneigh(coords, k = k))
weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)
local_moran <- localmoran(data[[variable]], weights)
mean_val <- mean(data[[variable]], na.rm = TRUE)
data %>%
mutate(
local_i = local_moran[, 1],
p_value = local_moran[, 5],
is_significant = p_value < 0.05,
moran_class = case_when(
!is_significant ~ "Not Significant",
local_i > 0 & .data[[variable]] > mean_val ~ "High-High",
local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
local_i < 0 & .data[[variable]] > mean_val ~ "High-Low",
local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
TRUE ~ "Not Significant"
)
)
}
# Apply to abandoned cars
fishnet <- calculate_local_morans(fishnet, "abandoned_cars", k = 5)
#| fig-width: 8
#| fig-height: 6
ggplot() +
geom_sf(data = fishnet, aes(fill = moran_class), color = NA) +
scale_fill_manual(
values = c(
"High-High" = "#d7191c",
"High-Low" = "#fdae61",
"Low-High" = "#abd9e9",
"Low-Low" = "#2c7bb6",
"Not Significant" = "gray90"
),
name = "Cluster Type"
) +
labs(
title = "Local Moran's I: Abandoned Car Clusters",
subtitle = "High-High clusters = Hot spots of disorder"
) +
theme_map()
# Get hot spot centroids
hotspots <- fishnet %>%
filter(moran_class == "High-High") %>%
st_centroid()
# Calculate distance to nearest hot spot
if (nrow(hotspots) > 0) {
fishnet <- fishnet %>%
mutate(
dist_to_hotspot = as.numeric(
st_distance(st_centroid(fishnet), hotspots %>% st_union())
)
)
cat("✓ Calculated distance to hot spots\n")
cat("  - Number of hot spot cells:", nrow(hotspots), "\n")
} else {
fishnet <- fishnet %>% mutate(dist_to_hotspot = 0)
cat("⚠ No significant hot spots found\n")
}
# Join districts for cross-validation
fishnet <- st_join(
fishnet,
policeDistricts,
join = st_within,
left = TRUE
) %>%
filter(!is.na(District))
cat("✓ Joined police districts\n")
cat("  - Districts:", length(unique(fishnet$District)), "\n")
# Create clean dataset
fishnet_model <- fishnet %>%
st_drop_geometry() %>%
dplyr::select(
uniqueID,
District,
countViolations,
abandoned_cars,
abandoned_cars.nn,
dist_to_hotspot
) %>%
na.omit()
cat("✓ Prepared modeling data\n")
cat("  - Observations:", nrow(fishnet_model), "\n")
# Fit Poisson model
model_poisson <- glm(
countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,
data = fishnet_model,
family = "poisson"
)
summary(model_poisson)
# Calculate dispersion
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) /
model_poisson$df.residual
cat("Dispersion parameter:", round(dispersion, 2), "\n")
if (dispersion > 1.5) {
cat("⚠ Overdispersion detected! Negative Binomial is more appropriate.\n")
} else {
cat("✓ Dispersion acceptable for Poisson.\n")
}
# Fit Negative Binomial
model_nb <- glm.nb(
countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,
data = fishnet_model
)
summary(model_nb)
# Compare models
cat("\nModel Comparison:\n")
cat("Poisson AIC:", round(AIC(model_poisson), 1), "\n")
cat("Negative Binomial AIC:", round(AIC(model_nb), 1), "\n")
# LOGO CV
districts <- unique(fishnet_model$District)
cv_results <- tibble()
cat("Running spatial cross-validation...\n")
for (i in seq_along(districts)) {
test_district <- districts[i]
# Split data
train_data <- fishnet_model %>% filter(District != test_district)
test_data <- fishnet_model %>% filter(District == test_district)
# Fit model
model_cv <- glm.nb(
countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,
data = train_data
)
# Predict
test_data <- test_data %>%
mutate(prediction = predict(model_cv, test_data, type = "response"))
# Metrics
mae <- mean(abs(test_data$countViolations - test_data$prediction))
rmse <- sqrt(mean((test_data$countViolations - test_data$prediction)^2))
cv_results <- bind_rows(
cv_results,
tibble(
fold = i,
test_district = test_district,
n_test = nrow(test_data),
mae = mae,
rmse = rmse
)
)
cat("  Fold", i, "- District", test_district, "- MAE:", round(mae, 2), "\n")
}
# Overall metrics
cat("\n✓ Cross-Validation Complete\n")
cat("Mean MAE:", round(mean(cv_results$mae), 2), "\n")
cat("Mean RMSE:", round(mean(cv_results$rmse), 2), "\n")
cv_results %>%
arrange(desc(mae)) %>%
kable(
digits = 2,
caption = "Spatial Cross-Validation Results by District",
col.names = c("Fold", "District", "N Test", "MAE", "RMSE")
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Fit final model on all data
final_model <- glm.nb(
countViolations ~ abandoned_cars + abandoned_cars.nn + dist_to_hotspot,
data = fishnet_model
)
# Add predictions to fishnet
fishnet <- fishnet %>%
mutate(
prediction_nb = predict(final_model, fishnet_model, type = "response")[match(uniqueID, fishnet_model$uniqueID)]
)
# Normalize KDE to same scale
kde_sum <- sum(fishnet$kde_value, na.rm = TRUE)
count_sum <- sum(fishnet$countViolations, na.rm = TRUE)
fishnet <- fishnet %>%
mutate(prediction_kde = (kde_value / kde_sum) * count_sum)
#| fig-width: 12
#| fig-height: 4
p1 <- ggplot() +
geom_sf(data = fishnet, aes(fill = countViolations), color = NA) +
scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
labs(title = "Actual Violations") +
theme_map()
p2 <- ggplot() +
geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
labs(title = "Model Predictions") +
theme_map()
p3 <- ggplot() +
geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
labs(title = "KDE Baseline") +
theme_map()
p1 + p2 + p3 +
plot_annotation(
title = "Model Performance Comparison"
)
# Calculate metrics
comparison <- fishnet %>%
st_drop_geometry() %>%
filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
summarize(
model_mae = mean(abs(countViolations - prediction_nb)),
model_rmse = sqrt(mean((countViolations - prediction_nb)^2)),
kde_mae = mean(abs(countViolations - prediction_kde)),
kde_rmse = sqrt(mean((countViolations - prediction_kde)^2))
)
comparison %>%
pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
separate(metric, into = c("approach", "metric"), sep = "_") %>%
pivot_wider(names_from = metric, values_from = value) %>%
kable(
digits = 2,
caption = "Model vs. KDE Baseline Performance",
col.names = c("Approach", "MAE", "RMSE")
) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
#| fig-width: 10
#| fig-height: 5
# Calculate errors
fishnet <- fishnet %>%
mutate(
error_nb = countViolations - prediction_nb,
abs_error_nb = abs(error_nb)
)
p1 <- ggplot() +
geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
scale_fill_gradient2(
name = "Error",
low = "#2166ac", mid = "white", high = "#b2182b",
midpoint = 0
) +
labs(title = "Prediction Errors",
subtitle = "Red = underpredicted, Blue = overpredicted") +
theme_map()
p2 <- ggplot() +
geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
scale_fill_viridis_c(name = "Abs Error", option = "magma") +
labs(title = "Absolute Errors",
subtitle = "Where are predictions least accurate?") +
theme_map()
p1 + p2
# Create coefficient table
model_summary <- broom::tidy(final_model, exponentiate = TRUE) %>%
mutate(across(where(is.numeric), ~round(., 3)))
model_summary %>%
kable(
caption = "Final Model Coefficients (Rate Ratios)",
col.names = c("Variable", "Rate Ratio", "Std. Error", "Z", "P-Value")
) %>%
kable_styling(bootstrap_options = c("striped", "hover")) %>%
footnote(general = "Rate ratios > 1 indicate positive association with violation counts")
sessionInfo()
knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE,
cache = TRUE
)
# Core tidyverse
library(tidyverse)
library(lubridate)
# Spatial data
library(sf)
library(tigris)
# Census data
library(tidycensus)
# Weather data
library(riem)  # For Philadelphia weather from ASOS stations
# Visualization
library(viridis)
library(gridExtra)
library(knitr)
library(kableExtra)
# here!
library(here)
# Get rid of scientific notation. We gotta look good!
options(scipen = 999)
plotTheme <- theme(
plot.title = element_text(size = 14, face = "bold"),
plot.subtitle = element_text(size = 10),
plot.caption = element_text(size = 8),
axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 11, face = "bold"),
panel.background = element_blank(),
panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
panel.grid.minor = element_blank(),
axis.ticks = element_blank(),
legend.position = "right"
)
mapTheme <- theme(
plot.title = element_text(size = 14, face = "bold"),
plot.subtitle = element_text(size = 10),
plot.caption = element_text(size = 8),
axis.line = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank(),
panel.background = element_blank(),
panel.border = element_blank(),
panel.grid.major = element_line(colour = 'transparent'),
panel.grid.minor = element_blank(),
legend.position = "right",
plot.margin = margin(1, 1, 1, 1, 'cm'),
legend.key.height = unit(1, "cm"),
legend.key.width = unit(0.2, "cm")
)
palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
# Hidden key for rendering
census_api_key("YOUR_Key_Here", overwrite = TRUE)
# Read Q1 2025 data
indego <- read_csv(here("data/indego-trips-2025-q1.csv"))
