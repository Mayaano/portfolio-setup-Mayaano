[
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "href": "Xiaoqing_Chen_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Are current property tax assessments in Philadelphia fair and accurate?",
    "text": "Are current property tax assessments in Philadelphia fair and accurate?\n\n\nProperty taxes depend on OPA‚Äôs assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia‚Äôs 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#motivations",
    "href": "Xiaoqing_Chen_Presentation.html#motivations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Motivations",
    "text": "Motivations\nFairness: Ensure residents pay taxes aligned with true property values.\nTransparency: Build trust through objective, data-based methods.\nEfficiency: Improve city revenue stability and policy planning.\nTraditional assessments rely on outdated, manual approaches.\nMachine learning models can better capture local market dynamics."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#data-sources",
    "href": "Xiaoqing_Chen_Presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\nPhiladelphia Property Sales (n= 24023,2023-2024)\nCensus ACS (income, education, poverty)\nOpenDataPhilly (number and distance: crime, park&recreation, transportation, hospital,school )"
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#spatial-distribution-of-housing-and-prices",
    "href": "Xiaoqing_Chen_Presentation.html#spatial-distribution-of-housing-and-prices",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Spatial Distribution of Housing and Prices",
    "text": "Spatial Distribution of Housing and Prices\n\n\nWhere Are the Houses?\n\n\nWhere Are the Expensive Houses?"
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "href": "Xiaoqing_Chen_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Larger homes, higher neighborhood income, and more bathrooms increase prices ‚Äî while older properties tend to sell for less.",
    "text": "Larger homes, higher neighborhood income, and more bathrooms increase prices ‚Äî while older properties tend to sell for less."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "href": "Xiaoqing_Chen_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Adding more real-world data to build a more fair and accurate housing price model",
    "text": "Adding more real-world data to build a more fair and accurate housing price model\n\nM1: Basic home features (size, age) ‚Üí simple but limited\n\nM2: + Census data ‚Üí adds community context\n\nM3: + Spatial data ‚Üí captures location effects\n\nM4: + Interactions ‚Üí reflects real neighborhood differences"
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "href": "Xiaoqing_Chen_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.",
    "text": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.\nObservations - Structural features (bathrooms, livable area) stay top-ranked across all models\n- Socioeconomic and spatial factors (income, census tract) rise in importance as they‚Äôre added\n- Final model shows location effects becoming dominant predictors of price"
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "href": "Xiaoqing_Chen_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance: The predicted prices from our final model align strongly with actual sales.",
    "text": "Model Performance: The predicted prices from our final model align strongly with actual sales.\nOur approach can reliably estimate market values across neighborhoods.\n  &gt;‚ÄúAverage Error ($)‚Äù shows the average gap between predicted and actual prices (lower = more accurate)."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "href": "Xiaoqing_Chen_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "The map highlights neighborhoods where predicted prices differ most from actual sales.",
    "text": "The map highlights neighborhoods where predicted prices differ most from actual sales.\nThese areas, shown in deep red or blue, are likely where property assessments are least accurate, and where tax fairness may be at greatest risk.\n Interpretation - Blue areas: Homes undervalued by the model ‚Üí may face under-assessment\n- Red areas: Homes overvalued ‚Üí may face over-assessment\n- Central & southern zones show the largest mismatches ‚Äî indicating uneven market patterns\n\nThese ‚Äúhard-to-predict‚Äù areas should be prioritized for review in future assessment updates."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#three-evidence-based-recommendations",
    "href": "Xiaoqing_Chen_Presentation.html#three-evidence-based-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Three Evidence-Based Recommendations",
    "text": "Three Evidence-Based Recommendations\nReview Where the Model Shows the Largest Gaps ‚Üí Our residual analysis pinpoints neighborhoods with the highest prediction errors ‚Äî the same areas where assessments are likely least fair. Prioritize these zones for reassessment and data verification.\nUse the Model as a Fairness Benchmark ‚Üí Instead of replacing official assessments, use the model as a cross-check tool to flag properties with unusually high or low assessed-to-predicted ratios.\nInstitutionalize Annual Model Updates ‚Üí Retrain the model each year using new sales and census data so assessments stay current with real market trends, preventing future inequities."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#limitations-improvements",
    "href": "Xiaoqing_Chen_Presentation.html#limitations-improvements",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Improvements",
    "text": "Limitations & Improvements\nData gaps: Some neighborhoods have limited or inconsistent sales data, and important local factors like school quality or public amenities are not fully represented.\nSpatial variation: The model‚Äôs accuracy differs across regions ‚Äî it performs very well in some areas but less so in others, suggesting that geography strongly influences results.\nSpatial modeling: Next, we will apply models that explicitly account for spatial relationships and neighborhood effects to improve prediction accuracy.\nData enhancement: We plan to expand data coverage in underrepresented areas to reduce bias and strengthen fairness across communities."
  },
  {
    "objectID": "Xiaoqing_Chen_Presentation.html#thank-you",
    "href": "Xiaoqing_Chen_Presentation.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank You",
    "text": "Thank You\nTurning Data into Fairer Assessments\n\n\nProject Team\nXiaoqing Chen\nZicheng Xiang\nLingxuan Gao\nZhiyuan Zhao\nFan Yang\nZhe Fang\n\nContact\nSixers Consulting 6ers@upenn.edu www.6ers.com\n\n\nPrepared for the City of Philadelphia ‚Äî Office of Property Assessment (OPA)"
  },
  {
    "objectID": "Xiaoqing_Chen_Appendix.html",
    "href": "Xiaoqing_Chen_Appendix.html",
    "title": "Philadelphia Housing Price Prediction ‚Äì Technical Appendix",
    "section": "",
    "text": "Step0\n\n# =========================================================\n# Step 1: Load libraries and data\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(stringr)\n\nopa_raw &lt;- read_csv(\"data/opa_properties_public.csv\",\n                    na = c(\"\", \"NA\", \"NaN\", \"NULL\"),\n                    guess_max = 1e6)\n\ncat(\"Rows (loaded):\", nrow(opa_raw), \"\\n\")\n\nRows (loaded): 583754 \n\n\n\n# =========================================================\n# Step 2: Data cleaning and filtering\n# =========================================================\nopa_res &lt;- opa_raw %&gt;%\n  mutate(\n    sale_date = as_date(sale_date),\n    # Prices are standardized as numeric values: regardless of whether they were originally numbers or strings (including $ or commas).\n    sale_price_num = suppressWarnings(\n      coalesce(as.numeric(sale_price), readr::parse_number(as.character(sale_price)))\n    ),\n    # Categories are standardized to a consistent data type.\n    cat_chr = as.character(category_code)\n  ) %&gt;%\n  filter(\n    cat_chr %in% c(\"1\"),  \n    sale_date &gt;= as_date(\"2023-01-01\"),\n    sale_date &lt;= as_date(\"2024-12-31\"),\n    sale_price_num &gt;= 10000,\n    total_livable_area &gt; 0,\n    year_built &gt; 0,\n    number_of_bedrooms &gt; 0,\n    number_of_bathrooms &gt; 0,\n    !is.na(census_tract),\n    census_tract != 0,\n    census_tract != \"\",\n    !is.na(zip_code),\n    zip_code != \"\",\n    !is.na(exterior_condition),\n    exterior_condition != \"\",\n    !is.na(interior_condition),\n    interior_condition != \"\",\n    !is.na(shape),\n    shape != \"\"\n  ) %&gt;%\n  select(\n    parcel_number, sale_date,\n    sale_price = sale_price_num,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, shape\n  ) %&gt;%\n  distinct() %&gt;%\n  drop_na()\n\ncat(\"Rows (after cleaning and filtering):\", nrow(opa_res), \"\\n\")\n\nRows (after cleaning and filtering): 24123 \n\n\n\n# =========================================================\n# Step 3: Extract coordinates from shape field (using sf)\n# =========================================================\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Remove the prefix SRID=2272\nwkt &lt;- sub(\"^SRID=\\\\d+;\\\\s*\", \"\", opa_res$shape)\n\n# Convert to sf format.\ngeom &lt;- st_as_sfc(wkt, crs = 2272)\nopa_sf &lt;- st_sf(opa_res, geometry = geom)\n\n# Extract coordinates (for scatter plot).\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf$X &lt;- coords[, 1]\nopa_sf$Y &lt;- coords[, 2]\n\n# Debug output.\ncat(\"Rows with valid coordinates:\", nrow(opa_sf), \"\\n\")\n\nRows with valid coordinates: 24123 \n\ncat(\"Sample X coordinates:\", head(opa_sf$X, 3), \"\\n\")\n\nSample X coordinates: 2726356 2709355 2718833 \n\ncat(\"Sample Y coordinates:\", head(opa_sf$Y, 3), \"\\n\")\n\nSample Y coordinates: 290862.2 257213.9 269581 \n\n\n\n# =========================================================\n# Step 4: Save cleaned data\n# =========================================================\n# Create output directory\ndir.create(here::here(\"data\"), recursive = TRUE, showWarnings = FALSE)\n\n# Export data (remove the geometry column, keeping only coordinates)\nopa_export &lt;- opa_sf %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, x_coord = X, y_coord = Y\n  ) %&gt;%\n  st_drop_geometry()  \n\nwrite_csv(opa_export, \"./data/opa_sales_2023_2024_residential_clean.csv\")\n\ncat(\"Cleaned data saved to: opa_sales_2023_2024_residential_clean.csv\\n\")\n\nCleaned data saved to: opa_sales_2023_2024_residential_clean.csv\n\ncat(\"Final dataset contains\", nrow(opa_export), \"rows with\", ncol(opa_export), \"columns\\n\")\n\nFinal dataset contains 24123 rows with 14 columns\n\ncat(\"Columns:\", paste(names(opa_export), collapse = \", \"), \"\\n\")\n\nColumns: parcel_number, sale_date, sale_price, number_of_bedrooms, number_of_bathrooms, total_livable_area, year_built, zip_code, category_code, exterior_condition, interior_condition, census_tract, x_coord, y_coord \n\n\n\n# =========================================================\n# Step 5: Load, clean crime data and calculate crime count for properties\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(sf)\n\n# Load crime data for 2023 and 2024\ncrime_2023 &lt;- read_csv(here(\"./data/crime_2023.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\ncrime_2024 &lt;- read_csv(here(\"./data/crime_2024.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\n\n# Merge, clean, and process crime data\ncrime_clean &lt;- bind_rows(\n  crime_2023 %&gt;% mutate(year = 2023),\n  crime_2024 %&gt;% mutate(year = 2024)\n) %&gt;%\n  mutate(\n    dispatch_date_time = as_datetime(dispatch_date_time),\n    dispatch_date = as_date(dispatch_date),\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    point_x = as.numeric(point_x),\n    point_y = as.numeric(point_y),\n    ucr_general = as.numeric(ucr_general),\n    text_general_code = as.character(text_general_code)\n  ) %&gt;%\n  filter(\n    !is.na(lat) & !is.na(lng),\n    lat != 0 & lng != 0,\n    dispatch_date &gt;= as_date(\"2023-01-01\"),\n    dispatch_date &lt;= as_date(\"2024-12-31\")\n  ) %&gt;%\n  select(\n    objectid, dc_dist, psa, dispatch_date_time, dispatch_date, \n    dispatch_time, hour, dc_key, location_block, \n    ucr_general, text_general_code, \n    lat, lng, point_x, point_y, year\n  ) %&gt;%\n  distinct() %&gt;%\n  drop_na()\n\n# Coordinate transformation: convert from WGS84 (EPSG:4326) to EPSG:2272 coordinate system\ncrime_sf &lt;- crime_clean %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(crs = 2272) %&gt;%\n  mutate(\n    x_coord_2272 = st_coordinates(.)[, 1],\n    y_coord_2272 = st_coordinates(.)[, 2]\n  ) %&gt;%\n  st_drop_geometry()\n\n# Read the cleaned real estate data\nopa_clean &lt;- read_csv(here(\"./data/opa_sales_2023_2024_residential_clean.csv\"))\n\n# Convert the real estate data to an sf object (EPSG:2272)\nopa_sf &lt;- opa_clean %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n\n# Convert the crime data to an sf object (EPSG:2272)\ncrime_sf_spatial &lt;- crime_sf %&gt;%\n  st_as_sf(coords = c(\"x_coord_2272\", \"y_coord_2272\"), crs = 2272)\n\n# Create a 0.75-mile buffer around each home and count crimes (15-minute walkshed)\nopa_buffers &lt;- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\ncrime_count &lt;- st_intersects(opa_buffers, crime_sf_spatial)\n\n# Add the crime counts to the real estate dataset\nopa_with_crime &lt;- opa_clean %&gt;%\n  mutate(\n    crime_count_15min_walk = sapply(crime_count, length)\n  )\n\ncat(\"Crime data processing completed:\\n\")\n\nCrime data processing completed:\n\ncat(\"Number of 2023 crime records:\", nrow(crime_2023), \"\\n\")\n\nNumber of 2023 crime records: 169017 \n\ncat(\"Number of 2024 crime records:\", nrow(crime_2024), \"\\n\")\n\nNumber of 2024 crime records: 160388 \n\ncat(\"Number of valid crime records after cleaning:\", nrow(crime_clean), \"\\n\")\n\nNumber of valid crime records after cleaning: 218725 \n\ncat(\"Number of records after coordinate transformation:\", nrow(crime_sf), \"\\n\")\n\nNumber of records after coordinate transformation: 218725 \n\ncat(\"Number of real estate records:\", nrow(opa_with_crime), \"\\n\")\n\nNumber of real estate records: 24123 \n\ncat(\"Average number of crimes within a 15-minute walkshed per property:\", round(mean(opa_with_crime$crime_count_15min_walk), 2), \"\\n\")\n\nAverage number of crimes within a 15-minute walkshed per property: 4466.34 \n\ncat(\"Crime count range:\", min(opa_with_crime$crime_count_15min_walk), \"-\", max(opa_with_crime$crime_count_15min_walk), \"\\n\")\n\nCrime count range: 35 - 13238 \n\n\n\n# =========================================================\n# Step 6: Add park accessibility metrics (distance in feet, keep coordinate columns)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# Use the crime data processing results from the previous step\nopa_with_crime &lt;- opa_with_crime\n\n# Convert to an sf object (coordinate system: EPSG:2272, unit: feet)\nopa_sf &lt;- st_as_sf(\n  opa_with_crime,\n  coords = c(\"x_coord\", \"y_coord\"),\n  crs = 2272\n)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of real estate records read: 24123 \n\n# Read and project the park data.\nparks &lt;- st_read(here(\"./data/PPR_Program_Sites.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(2272)\n\n# Calculate park accessibility metrics (unit: feet)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    dist_to_park_ft = apply(st_distance(opa_sf, parks), 1, min),   \n    # Nearest park distance (feet).\n    park_within_15min_walk = lengths(st_within(opa_sf, st_buffer(parks, 3960)))  # 0.75 mile = 3960 feet\n  )\n\n# Extract coordinates to prevent st_drop_geometry from removing them\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export data (retain coordinates and newly added indicators)\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,              # keep coordinates\n    crime_count_15min_walk,        # number of crimes within 15-minute walkshed\n    dist_to_park_ft,               # nearest park distance (feet)\n    park_within_15min_walk         # number of parks within 15-minute walkshed\n  )\n\n# Output summary information\n# Save the complete dataset with park accessibility indicators\nwrite_csv(opa_export, here(\"./data/opa_sales_with_parks.csv\"))\n\ncat(\" Park accessibility indicators added (unit: feet)\\n\")\n\n Park accessibility indicators added (unit: feet)\n\ncat(\"  Average nearest park distance:\", round(mean(opa_export$dist_to_park_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average nearest park distance: 1768.6 ft\n\ncat(\"  Average number of parks within 15-minute walkshed:\", round(mean(opa_export$park_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of parks within 15-minute walkshed: 3.53 \n\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\n\n  Number of fields: 17 columns (including coordinate columns)\n\ncat(\"üéâ Data saved to: ./data/opa_sales_with_parks.csv\\n\")\n\nüéâ Data saved to: ./data/opa_sales_with_parks.csv\n\n\n\n# =========================================================\n# Step 7: Add public transit accessibility metrics\n# (distance to nearest stop & number of stops within 1,000 ft; also 15-min walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# Use the same CRS (EPSG:2272, units = feet)\nanalysis_crs &lt;- 2272\n\n# Read the dataset saved in the previous step (with price, crime, and park metrics)\nopa_data &lt;- read_csv(here(\"./data/opa_sales_with_parks.csv\"))\nopa_sf &lt;- opa_data %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of real estate records read: 24123 \n\n# Read and project transit stop data\ntransit_path &lt;- here(\"./data/Transit_Stops_(Spring_2025).geojson\")\nstopifnot(file.exists(transit_path))\n\ntransit_stops &lt;- st_read(transit_path, quiet = TRUE) %&gt;%\n  st_transform(analysis_crs) %&gt;%\n  suppressWarnings(st_collection_extract(\"POINT\")) %&gt;%\n  filter(!st_is_empty(geometry)) %&gt;%\n  distinct(geometry, .keep_all = TRUE)\n\ncat(\"Number of transit stops:\", nrow(transit_stops), \"\\n\")\n\nNumber of transit stops: 13839 \n\n# Compute accessibility metrics (units: feet)\n# Distance to nearest transit stop\nnearest_idx &lt;- st_nearest_feature(opa_sf, transit_stops)\ndist_ft &lt;- st_distance(opa_sf, transit_stops[nearest_idx, ], by_element = TRUE)\nopa_sf$dist_transit_ft &lt;- as.numeric(set_units(dist_ft, \"ft\"))\n\n# Count of transit stops within a 15-minute walkshed (0.75 mile = 3960 ft)\nbuffer_3960ft &lt;- st_buffer(opa_sf, dist = 3960)\nopa_sf$transit_15min_walk &lt;- lengths(st_intersects(buffer_3960ft, transit_stops))\n\n# (Optional) Count of transit stops within 1,000 ft\nbuffer_1000ft &lt;- st_buffer(opa_sf, dist = 1000)\nopa_sf$transit_within_1000ft &lt;- lengths(st_intersects(buffer_1000ft, transit_stops))\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and newly added indicators)\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                 # keep coordinates\n    crime_count_15min_walk,           # crime metric\n    dist_to_park_ft, park_within_15min_walk, # park metrics\n    dist_transit_ft, transit_15min_walk,     # transit metrics (walkshed)\n    transit_within_1000ft                    # transit stops within 1,000 ft\n  )\n\n# Output summary information\n# Save the complete dataset with transit metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_transit.csv\"))\n\ncat(\"  Public transit metrics added\\n\")\n\n  Public transit metrics added\n\ncat(\"  Average distance to nearest transit stop:\", round(mean(opa_export$dist_transit_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average distance to nearest transit stop: 428.8 ft\n\ncat(\"  Average number of transit stops within 15-minute walkshed:\", round(mean(opa_export$transit_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of transit stops within 15-minute walkshed: 151.79 \n\ncat(\"  Average number of transit stops within 1,000 ft:\", round(mean(opa_export$transit_within_1000ft, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of transit stops within 1,000 ft: 10.73 \n\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\n\n  Number of fields: 20 columns (including coordinate columns)\n\ncat(\"üéâ Data saved to: ./data/opa_sales_with_transit.csv\\n\")\n\nüéâ Data saved to: ./data/opa_sales_with_transit.csv\n\n\n\n# =========================================================\n# Step 8: Add hospital accessibility metrics\n# (distance to nearest hospital & count within 0.75-mile walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# CRS: EPSG:2272 (units: feet)\nanalysis_crs &lt;- 2272\n\n# Read the dataset saved in the previous step (with price, crime, park, and transit metrics)\nopa_data &lt;- read_csv(here(\"./data/opa_sales_with_transit.csv\"))\nopa_sf &lt;- opa_data %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of real estate records read: 24123 \n\n# Read hospital data and project to the same CRS\nhospitals &lt;- st_read(here(\"./data/Hospitals.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(analysis_crs)\n\ncat(\"Number of hospitals:\", nrow(hospitals), \"\\n\")\n\nNumber of hospitals: 36 \n\n# Compute hospital accessibility metrics (units: feet)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    # Distance to nearest hospital (feet)\n    dist_to_hospital_ft = as.numeric(apply(st_distance(opa_sf, hospitals), 1, min)),\n    # Number of hospitals within a 15-minute walkshed (0.75 mile = 3960 ft)\n    hospitals_15min_walk = lengths(st_within(opa_sf, st_buffer(hospitals, 3960)))\n  )\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and all features)\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                         # keep coordinates\n    crime_count_15min_walk,                   # crime metrics\n    dist_to_park_ft, park_within_15min_walk,  # park metrics\n    dist_transit_ft, transit_15min_walk,      # transit metrics\n    dist_to_hospital_ft, hospitals_15min_walk # hospital metrics (new)\n  )\n\n# Output summary information\n# Save the complete dataset with hospital metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_hospitals.csv\"))\n\ncat(\"  Hospital accessibility metrics added\\n\")\n\n  Hospital accessibility metrics added\n\ncat(\"  Average distance to nearest hospital:\", round(mean(opa_export$dist_to_hospital_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average distance to nearest hospital: 5169 ft\n\ncat(\"  Average number of hospitals within 15-minute walkshed:\", round(mean(opa_export$hospitals_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of hospitals within 15-minute walkshed: 0.74 \n\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\n\n  Number of fields: 21 columns (including coordinate columns)\n\ncat(\"  Includes all features: real estate info + crime + park + transit + hospital\\n\")\n\n  Includes all features: real estate info + crime + park + transit + hospital\n\ncat(\"  Data saved to: ./data/opa_sales_with_hospitals.csv\\n\")\n\n  Data saved to: ./data/opa_sales_with_hospitals.csv\n\n\n\n# =========================================================\n# Step 9: Enrich with ACS (Census) Socioeconomic Indicators\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(readr)\nlibrary(here)\n\n# ---------------------------------------------------------\n# Read the final property data (includes all accessibility metrics)\n# ---------------------------------------------------------\nopa_final &lt;- read_csv(here(\"./data/opa_sales_with_hospitals.csv\"))\n\n# Convert to sf object (ensure CRS is EPSG:2272)\nopa_sf &lt;- opa_final %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of property records read: 24123 \n\n# ---------------------------------------------------------\n# Download ACS data for Philadelphia (year can be changed)\n# ---------------------------------------------------------\nyear_acs &lt;- 2022\ncensus_api_key(\"86993dedbe98d77b9d79db6b8ba21a7fde55cb91\", install = FALSE)\n\nacs_vars &lt;- c(\n  total_pop      = \"B01003_001\",\n  median_income  = \"B19013_001\",\n  per_cap_income = \"B19301_001\",\n  below_pov      = \"B17001_002\",\n  edu_total25    = \"B15003_001\",\n  edu_bach       = \"B15003_022\",\n  edu_mast       = \"B15003_023\",\n  edu_prof       = \"B15003_024\",\n  edu_phd        = \"B15003_025\"\n)\n\nphl_acs &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = year_acs,\n  geometry = TRUE,\n  output = \"wide\",\n  variables = acs_vars\n) %&gt;%\n  mutate(\n    PCBACHMORE = 100 * ((edu_bachE + edu_mastE + edu_profE + edu_phdE) / edu_total25E),\n    PCTPOVERTY = 100 * (below_povE / total_popE)\n  ) %&gt;%\n  select(geometry, GEOID, total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# ---------------------------------------------------------\n# Reproject to align (EPSG:2272)\n# ---------------------------------------------------------\nphl_acs &lt;- st_transform(phl_acs, 2272)\n\n# ---------------------------------------------------------\n# Spatial join: assign each home to the tract it falls within\n# ---------------------------------------------------------\nopa_joined &lt;- st_join(\n  opa_sf,\n  phl_acs,\n  join = st_within,\n  left = TRUE\n)\n\n# ---------------------------------------------------------\n# For out-of-bound samples (occasional NAs), fill with nearest tract\n# ---------------------------------------------------------\nmissing &lt;- is.na(opa_joined$median_incomeE)\nif (any(missing)) {\n  idx &lt;- st_nearest_feature(opa_joined[missing, ], phl_acs)\n  repl &lt;- phl_acs[idx, ] %&gt;% st_drop_geometry()\n  cols &lt;- names(repl)\n  opa_joined[missing, cols] &lt;- repl\n}\n\n# ---------------------------------------------------------\n# Export results\n# ---------------------------------------------------------\nopa_export &lt;- opa_joined %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  # crime\n    dist_to_park_ft, park_within_15min_walk, # parks\n    dist_transit_ft, transit_15min_walk,     # transit\n    dist_to_hospital_ft, hospitals_15min_walk, # hospitals\n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY  # ACS\n  )\n\n# Save the complete dataset with Census indicators\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  ACS socioeconomic indicators added\\n\")\n\n  ACS socioeconomic indicators added\n\ncat(\"  Mean household income (USD):\", round(mean(opa_export$median_incomeE, na.rm = TRUE), 0), \"\\n\")\n\n  Mean household income (USD): 66446 \n\ncat(\"  Mean poverty rate (%):\", round(mean(opa_export$PCTPOVERTY, na.rm = TRUE), 2), \"\\n\")\n\n  Mean poverty rate (%): 20.85 \n\ncat(\"  Mean share with bachelor's degree or higher (%):\", round(mean(opa_export$PCBACHMORE, na.rm = TRUE), 2), \"\\n\")\n\n  Mean share with bachelor's degree or higher (%): 34.62 \n\ncat(\"  Includes all features: price + crime + parks + transit + hospitals + socioeconomic\\n\")\n\n  Includes all features: price + crime + parks + transit + hospitals + socioeconomic\n\ncat(\"  Data saved to: opa_sales_final_complete.csv\\n\")\n\n  Data saved to: opa_sales_final_complete.csv\n\n\n\n# =========================================================\n# Step 10: Add Education Accessibility Indicators (Schools)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# ---------------------------------------------------------\n# 1.Read the complete dataset saved from the previous step (including Census and accessibility features)\n# ---------------------------------------------------------\nopa_data &lt;- read_csv(here(\"opa_sales_final_complete.csv\"))\nopa_sf &lt;- opa_data %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of property records read: 24123 \n\ncat(\"Data column names:\", paste(names(opa_sf), collapse = \", \"), \"\\n\")\n\nData column names: parcel_number, sale_date, sale_price, number_of_bedrooms, number_of_bathrooms, total_livable_area, year_built, zip_code, category_code, exterior_condition, interior_condition, census_tract, x_coord, y_coord, crime_count_15min_walk, dist_to_park_ft, park_within_15min_walk, dist_transit_ft, transit_15min_walk, dist_to_hospital_ft, hospitals_15min_walk, total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY, geometry \n\n# ---------------------------------------------------------\n# 2.Read the school data (only one file)Ôºâ\n# ---------------------------------------------------------\nschools &lt;- st_read(here(\"./data/Schools_Parcels.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(2272) %&gt;%\n  filter(!st_is_empty(geometry))  \n\ncat(\"School data loaded:\", nrow(schools), \"records\\n\")\n\nSchool data loaded: 495 records\n\ncat(\"School data coordinate system:\", st_crs(schools)$input, \"\\n\")\n\nSchool data coordinate system: EPSG:2272 \n\ncat(\"Property data coordinate system:\", st_crs(opa_sf)$input, \"\\n\")\n\nProperty data coordinate system: EPSG:2272 \n\n# ---------------------------------------------------------\n# 3.Calculate education accessibility indicators\n# ---------------------------------------------------------\n# Distance to the nearest school (in feet)\ncat(\"Starting to calculate the nearest school distance...\\n\")\n\nStarting to calculate the nearest school distance...\n\ndist_matrix &lt;- st_distance(opa_sf, schools)\ncat(\"Distance matrix dimensions:\", dim(dist_matrix), \"\\n\")\n\nDistance matrix dimensions: 24123 495 \n\nopa_sf$dist_to_nearest_school_ft &lt;- as.numeric(apply(dist_matrix, 1, min))\ncat(\"Nearest school distance calculation completed, range:\",\n    min(opa_sf$dist_to_nearest_school_ft), \"-\", \n    max(opa_sf$dist_to_nearest_school_ft), \"feet\\n\")\n\nNearest school distance calculation completed, range: 0 - 5286.279 feet\n\n# Number of schools within a 15-minute walking distance\ncat(\"Starting to calculate the number of schools within a 15-minute walking distance...\\n\")\n\nStarting to calculate the number of schools within a 15-minute walking distance...\n\nbuffer_3960ft &lt;- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\nopa_sf$schools_within_15min_walk &lt;- lengths(st_intersects(buffer_3960ft, schools))\ncat(\"Calculation of schools within a 15-minute walking distance completed, range:\",\n    min(opa_sf$schools_within_15min_walk), \"-\", \n    max(opa_sf$schools_within_15min_walk), \"\\n\")\n\nCalculation of schools within a 15-minute walking distance completed, range: 0 - 28 \n\n# ---------------------------------------------------------\n# 4.Extract coordinates and retain all columns\n# ---------------------------------------------------------\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# ---------------------------------------------------------\n# 5.Export the complete table with education accessibility indicators\n# ---------------------------------------------------------\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  \n    dist_to_park_ft, park_within_15min_walk, \n    dist_transit_ft, transit_15min_walk,     \n    dist_to_hospital_ft, hospitals_15min_walk, \n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY,  # Census\n    dist_to_nearest_school_ft, schools_within_15min_walk                  \n  )\n\n# ---------------------------------------------------------\n# 6.Clean the data: remove all rows containing empty or NA values\n# ---------------------------------------------------------\nrows_before &lt;- nrow(opa_export)\n\nopa_export &lt;- opa_export %&gt;%\n  mutate(across(everything(), ~ {\n    if (is.character(.x)) {\n      clean_val &lt;- trimws(tolower(as.character(.x)))\n      ifelse(clean_val == \"\" | clean_val == \"na\" | clean_val == \"n/a\" | clean_val == \"null\", \n             NA, .x)\n    } else {\n      .x\n    }\n  })) %&gt;%\n  drop_na()\n\nrows_after &lt;- nrow(opa_export)\nrows_removed &lt;- rows_before - rows_after\n\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  Education accessibility indicators have been added\\n\")\n\n  Education accessibility indicators have been added\n\ncat(\"  Average distance to the nearest school:\", \n    round(mean(opa_export$dist_to_nearest_school_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average distance to the nearest school: 949.6 ft\n\ncat(\"  Average number of schools within a 15-minute walking distance:\", \n    round(mean(opa_export$schools_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of schools within a 15-minute walking distance: 10.56 \n\ncat(\"  Data cleaning completed: removed\", rows_removed, \"rows containing empty or NA values\\n\")\n\n  Data cleaning completed: removed 100 rows containing empty or NA values\n\ncat(\"  Before cleaning:\", rows_before, \"rows ‚Üí After cleaning:\", rows_after, \"rows\\n\")\n\n  Before cleaning: 24123 rows ‚Üí After cleaning: 24023 rows\n\ncat(\"  Final complete dataset saved to: opa_sales_final_complete.csv\\n\")\n\n  Final complete dataset saved to: opa_sales_final_complete.csv\n\ncat(\"  Includes all features: housing price + crime + parks + transit + hospitals + Census + schools\\n\")\n\n  Includes all features: housing price + crime + parks + transit + hospitals + Census + schools\n\ncat(\"  Number of columns:\", ncol(opa_export), \"(including coordinate columns)\\n\")\n\n  Number of columns: 28 (including coordinate columns)\n\n\nStep1\n\n# =========================================================\n# Step 1: Skewness Detection + Log Transformation + Descriptive Statistics\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(e1071)\nlibrary(patchwork)\nlibrary(readr)\nlibrary(tidyr)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"üìä Step 1: Data Cleaning, Transformation, and Descriptive Statistics\\n\")\n\nüìä Step 1: Data Cleaning, Transformation, and Descriptive Statistics\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n# === Read Data ===\ndf &lt;- read_csv(\"opa_sales_final_complete.csv\", show_col_types = FALSE)\ncat(sprintf(\"Original sample size: %d\\n\", nrow(df)))\n\nOriginal sample size: 24023\n\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\nNumber of variables: 28\n\n# === Basic Processing ===\nif (\"year_built\" %in% names(df)) {\n  df &lt;- df %&gt;%\n    mutate(age = 2025 - year_built,\n           age2 = age^2) %&gt;%\n    dplyr::select(-year_built)\n}\n\ncat_vars &lt;- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncoord_vars &lt;- c(\"x_coord\", \"y_coord\")\ndf[cat_vars] &lt;- lapply(df[cat_vars], factor)\n\n# === Skewness Detection ===\ncat(\"=== Skewness Detection ===\\n\")\n\n=== Skewness Detection ===\n\nnum_vars &lt;- df %&gt;% dplyr::select(where(is.numeric))\nskews &lt;- sapply(num_vars, function(x) {\n  if (all(is.na(x)) || sd(x, na.rm = TRUE) == 0) return(0)\n  skewness(x, na.rm = TRUE)\n})\nlogged_vars &lt;- names(skews[abs(skews) &gt; 1])\ncat(sprintf(\"Variables with |skew| &gt; 1: %d\\n\", length(logged_vars)))\n\nVariables with |skew| &gt; 1: 10\n\n# === Apply log1p Transformation ===\ndf_trans &lt;- df\nfor (v in logged_vars) {\n  df_trans[[v]] &lt;- log1p(pmax(df[[v]], 0))\n}\n\n\n# =========================================================\n# 1. Descriptive Statistics Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating Descriptive Statistics Table ===\\n\\n\")\n\n=== Generating Descriptive Statistics Table ===\n\n# Identify target variable\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df_trans) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df_trans) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df_trans) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n# Select key numeric variables\nkey_vars &lt;- c(\n  y_var,\n  \"total_livable_area\", \"number_of_bedrooms\", \"number_of_bathrooms\", \"age\",\n  \"median_incomeE\", \"per_cap_incomeE\", \"PCTPOVERTY\", \"PCBACHMORE\"\n)\nkey_vars &lt;- intersect(key_vars, names(df_trans))\n\nif (length(key_vars) &gt; 0) {\n  # Compute descriptive statistics (more meaningful on pre-transformation data)\n  desc_stats &lt;- df %&gt;%\n    dplyr::select(all_of(key_vars)) %&gt;%\n    summarise(across(everything(), list(\n      N = ~sum(!is.na(.)),\n      Mean = ~mean(., na.rm = TRUE),\n      SD = ~sd(., na.rm = TRUE),\n      Min = ~min(., na.rm = TRUE),\n      Q25 = ~quantile(., 0.25, na.rm = TRUE),\n      Median = ~median(., na.rm = TRUE),\n      Q75 = ~quantile(., 0.75, na.rm = TRUE),\n      Max = ~max(., na.rm = TRUE)\n    ), .names = \"{.col}_{.fn}\")) %&gt;%\n    pivot_longer(everything(), names_to = \"stat\", values_to = \"value\") %&gt;%\n    separate(stat, into = c(\"Variable\", \"Statistic\"), sep = \"_(?=[^_]+$)\") %&gt;%\n    pivot_wider(names_from = Statistic, values_from = value) %&gt;%\n    dplyr::select(Variable, N, Mean, SD, Min, Q25, Median, Q75, Max) %&gt;%\n    mutate(across(c(Mean, SD, Min, Q25, Median, Q75, Max), ~round(., 3)))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\")\n  write_csv(desc_stats, \"file/descriptive_statistics.csv\")\n  cat(\"  ‚úì file/descriptive_statistics.csv\\n\")\n  print(as.data.frame(desc_stats), row.names = FALSE)\n}\n\n  ‚úì file/descriptive_statistics.csv\n            Variable     N       Mean         SD       Min        Q25\n          sale_price 24023 340590.503 466810.992 10000.000 151970.000\n  total_livable_area 24023   1359.120    576.839   308.000   1050.000\n  number_of_bedrooms 24023      2.972      0.789     1.000      3.000\n number_of_bathrooms 24023      1.444      0.686     1.000      1.000\n                 age 24023     85.799     32.549     0.000     72.000\n      median_incomeE 24023  66445.992  31346.114 14983.000  41325.000\n     per_cap_incomeE 24023  39700.301  25344.230  7802.000  21372.000\n          PCTPOVERTY 24023     20.826     13.396     0.719     10.596\n          PCBACHMORE 24023     34.706     24.837     0.530     15.362\n     Median        Q75          Max\n 248000.000 370000.000 15428633.000\n   1208.000   1492.000    14150.000\n      3.000      3.000       12.000\n      1.000      2.000       12.000\n    100.000    105.000      275.000\n  59837.000  86989.000   181066.000\n  31406.000  48558.000   173777.000\n     17.939     29.449       77.833\n     27.435     53.441       95.598\n\n\n\n# =========================================================\n# 2. Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating distribution comparison plots ===\\n\\n\")\n\n=== Generating distribution comparison plots ===\n\nplot_list &lt;- lapply(names(num_vars), function(v) {\n  is_transformed &lt;- v %in% logged_vars\n  \n  # Original distribution\n  p1 &lt;- ggplot(df, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = \"#053061\", color = \"white\", alpha = 0.85) +\n    labs(title = paste0(v, \" (Original)\"), x = NULL, y = \"Count\") +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Transformed distribution\n  fill_color &lt;- if (is_transformed) \"#67001F\" else \"#4393C3\"\n  \n  p2 &lt;- ggplot(df_trans, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = fill_color, color = \"white\", alpha = 0.85) +\n    labs(\n      title = paste0(v, if (is_transformed) \" (Log-transformed)\" else \" (No transformation)\"),\n      x = NULL, y = \"Count\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Add skewness annotation\n  skew_val &lt;- round(skews[v], 2)\n  p1 &lt;- p1 + annotate(\"text\", x = Inf, y = Inf, \n                      label = paste0(\"Skewness: \", skew_val),\n                      hjust = 1.1, vjust = 1.5, size = 2.5, color = \"gray30\")\n  \n  pair_plot &lt;- (p1 | p2) + plot_layout(widths = c(1, 1)) &\n    theme(plot.margin = margin(3, 3, 3, 3))\n  \n  wrap_elements(pair_plot) + \n    theme(\n      plot.background = element_rect(fill = \"white\", color = \"#B0B0B0\", linewidth = 1.2),\n      plot.margin = margin(6, 6, 6, 6)\n    )\n})\n\nall_plot &lt;- wrap_plots(plot_list, ncol = 2) + \n  plot_annotation(\n    title = \"Variable Distributions: Original vs. Transformed\",\n    subtitle = paste0(\n      \"Dark Blue = Original | Dark Red = Log-transformed (|skew| &gt; 1) | Medium Blue = No transformation\"\n    ),\n    theme = theme(\n      plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray30\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\n\n# Skewness Summary Plot\nskew_df &lt;- data.frame(\n  variable = names(skews),\n  skewness = skews,\n  transformed = names(skews) %in% logged_vars\n) %&gt;% arrange(desc(abs(skewness)))\n\np_skew &lt;- ggplot(skew_df, aes(x = reorder(variable, abs(skewness)), y = skewness, fill = transformed)) +\n  geom_col(alpha = 0.85) +\n  geom_hline(yintercept = c(-1, 1), linetype = \"dashed\", color = \"#67001F\", linewidth = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  scale_fill_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"No transformation\", \"TRUE\" = \"Log-transformed\"),\n    name = NULL\n  ) +\n  coord_flip() +\n  labs(\n    title = \"Skewness of All Variables\",\n    subtitle = \"Variables with |skewness| &gt; 1 are log-transformed\",\n    x = \"Variable\", y = \"Skewness\",\n    caption = \"Dashed lines indicate skewness thresholds at ¬±1\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\n\n# =========================================================\n# 3. Categorical Variable Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating categorical variable distribution plots ===\\n\\n\")\n\n=== Generating categorical variable distribution plots ===\n\ncat_vars_viz &lt;- c(\"interior_condition\", \"exterior_condition\")\ncat_vars_viz &lt;- intersect(cat_vars_viz, names(df_trans))\n\nif (length(cat_vars_viz) &gt; 0 && !is.na(y_var)) {\n  plot_list_cat &lt;- list()\n  \n  for (var in cat_vars_viz) {\n    cat_summary &lt;- df_trans %&gt;%\n      group_by(!!sym(var)) %&gt;%\n      summarise(\n        count = n(),\n        mean_price = mean(.data[[y_var]], na.rm = TRUE),\n        .groups = \"drop\"\n      ) %&gt;%\n      filter(!is.na(!!sym(var))) %&gt;%\n      arrange(desc(mean_price))\n    \n    if (nrow(cat_summary) &gt; 0) {\n      p &lt;- ggplot(cat_summary, aes(reorder(!!sym(var), mean_price), mean_price, fill = mean_price)) +\n        geom_col(alpha = 0.9) +\n        geom_text(aes(label = count), vjust = -0.5, size = 3) +\n        scale_fill_gradient2(\n          low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n          midpoint = median(cat_summary$mean_price, na.rm = TRUE),\n          guide = \"none\"\n        ) +\n        coord_flip() +\n        theme_minimal(base_size = 10) +\n        theme(\n          panel.grid.major.y = element_blank(),\n          plot.title = element_text(hjust = 0.5, face = \"bold\", size = 12),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA)\n        ) +\n        labs(\n          title = gsub(\"_\", \" \", toupper(var)),\n          x = NULL,\n          y = \"Average Sale Price (log)\",\n          caption = \"Numbers indicate the count in each category\"\n        )\n      \n      plot_list_cat[[var]] &lt;- p\n    }\n  }\n  \n  if (length(plot_list_cat) &gt; 0) {\n    p_cat_combined &lt;- wrap_plots(plot_list_cat, ncol = 2) +\n      plot_annotation(\n        title = \"Average Sale Price by Property Condition\",\n        subtitle = \"Log-transformed sale prices\",\n        theme = theme(\n          plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n          plot.subtitle = element_text(size = 11, hjust = 0.5),\n          plot.background = element_rect(fill = \"white\", color = NA)\n        )\n      )\n    \n    if (!dir.exists(\"plot\")) dir.create(\"plot\")\n    ggsave(\"plot/categorical_price_comparison.png\", p_cat_combined,\n           width = 12, height = 6, dpi = 300, bg = \"white\")\n    cat(\"  ‚úì plot/categorical_price_comparison.png\\n\")\n  }\n}\n\n  ‚úì plot/categorical_price_comparison.png\n\n\n\n# =========================================================\n# Output Files\n# =========================================================\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nif (!dir.exists(\"file\")) dir.create(\"file\")\n\nggsave(\"plot/all_variables_distribution.png\", plot = all_plot, width = 20, height = 12, dpi = 300, bg = \"white\")\nggsave(\"plot/skewness_summary.png\", plot = p_skew, width = 10, height = 8, dpi = 300, bg = \"white\")\nwriteLines(logged_vars, \"file/logged_variables.txt\")\nwrite_csv(df_trans, \"file/opa_sales_step1_clean.csv\")\n\ntransform_summary &lt;- data.frame(\n  Variable = names(skews),\n  Original_Skewness = round(skews, 3),\n  Transformed = names(skews) %in% logged_vars\n) %&gt;% arrange(desc(abs(Original_Skewness)))\n\nwrite_csv(transform_summary, \"file/transformation_summary.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 1 Completed\\n\")\n\n Step 1 Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Output Files:\\n\\n\")\n\n Output Files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\ncat(\"  ‚Ä¢ file/descriptive_statistics.csv - Descriptive statistics\\n\")\n\n  ‚Ä¢ file/descriptive_statistics.csv - Descriptive statistics\n\ncat(\"  ‚Ä¢ file/transformation_summary.csv - Transformation summary\\n\")\n\n  ‚Ä¢ file/transformation_summary.csv - Transformation summary\n\ncat(\"  ‚Ä¢ file/logged_variables.txt - List of log-transformed variables\\n\")\n\n  ‚Ä¢ file/logged_variables.txt - List of log-transformed variables\n\ncat(\"  ‚Ä¢ file/opa_sales_step1_clean.csv - Cleaned dataset\\n\\n\")\n\n  ‚Ä¢ file/opa_sales_step1_clean.csv - Cleaned dataset\n\ncat(\"Figures:\\n\")\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/all_variables_distribution.png - All variables: original vs. transformed\\n\")\n\n  ‚Ä¢ plot/all_variables_distribution.png - All variables: original vs. transformed\n\ncat(\"  ‚Ä¢ plot/skewness_summary.png - Skewness summary\\n\")\n\n  ‚Ä¢ plot/skewness_summary.png - Skewness summary\n\ncat(\"  ‚Ä¢ plot/categorical_price_comparison.png - Categorical variables vs. sale price\\n\\n\")\n\n  ‚Ä¢ plot/categorical_price_comparison.png - Categorical variables vs. sale price\n\ncat(\"üìà Stats:\\n\")\n\nüìà Stats:\n\ncat(sprintf(\"  ‚Ä¢ Variables with |skew| &gt; 1: %d\\n\", length(logged_vars)))\n\n  ‚Ä¢ Variables with |skew| &gt; 1: 10\n\ncat(sprintf(\"  ‚Ä¢ Final number of variables: %d\\n\", ncol(df_trans)))\n\n  ‚Ä¢ Final number of variables: 29\n\ncat(sprintf(\"  ‚Ä¢ Final sample size: %d\\n\\n\", nrow(df_trans)))\n\n  ‚Ä¢ Final sample size: 24023\n\n\nStep2\n\n# =========================================================\n# Step 2 Enhanced: Correlation Matrix + VIF + Spatial Visualization + Scatterplots\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(reshape2)\nlibrary(readr)\nlibrary(patchwork)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"  Step 2: Correlation Analysis, Multicollinearity, and Spatial Exploration\\n\")\n\n  Step 2: Correlation Analysis, Multicollinearity, and Spatial Exploration\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n# === Read Data ===\ndf &lt;- read_csv(\"file/opa_sales_step1_clean.csv\", show_col_types = FALSE)\n\ncat(sprintf(\"Sample size: %d\\n\", nrow(df)))\n\nSample size: 24023\n\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\nNumber of variables: 29\n\n# === Auto-detect target variable ===\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat_vars &lt;- intersect(c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\"), names(df))\ncoord_vars &lt;- intersect(c(\"x_coord\", \"y_coord\"), names(df))\ndf[cat_vars] &lt;- lapply(df[cat_vars], factor)\n\n\n# =========================================================\n# 1. Correlation Matrix\n# =========================================================\n\ncat(\"=== Generating correlation matrix ===\\n\")\n\n=== Generating correlation matrix ===\n\nnum_df &lt;- df %&gt;% dplyr::select(where(is.numeric))\nnum_df &lt;- num_df[, sapply(num_df, function(x) sd(x, na.rm = TRUE) &gt; 0), drop = FALSE]\n\nif (ncol(num_df) &gt; 1) {\n  corr_mat &lt;- cor(num_df, use = \"pairwise.complete.obs\")\n  corr_mat[upper.tri(corr_mat)] &lt;- NA\n  corr_melt &lt;- melt(corr_mat, na.rm = TRUE)\n  \n  # If there are too many variables, keep only the first 50\n  if (ncol(num_df) &gt; 50) {\n    vars_top &lt;- names(num_df)[1:50]\n    corr_melt &lt;- corr_melt %&gt;% filter(Var1 %in% vars_top & Var2 %in% vars_top)\n  }\n  \n  p_corr &lt;- ggplot(corr_melt, aes(Var2, Var1, fill = value, size = abs(value))) +\n    geom_point(shape = 21, color = \"white\", stroke = 0.5) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0, limits = c(-1, 1), name = NULL,\n      breaks = seq(-1, 1, 0.2)\n    ) +\n    scale_size_continuous(range = c(0.5, 10), guide = \"none\") +\n    scale_x_discrete(position = \"top\") +\n    scale_y_discrete(limits = rev) +\n    theme_minimal(base_size = 11) +\n    theme(\n      axis.text.x.top = element_text(angle = 45, hjust = 0, vjust = 0, size = 9, color = \"black\"),\n      axis.text.y = element_text(size = 9, color = \"black\"),\n      axis.title = element_blank(),\n      panel.grid.major = element_line(color = \"#E0E0E0\", linewidth = 0.5),\n      panel.grid.minor = element_blank(),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"bottom\",\n      legend.direction = \"horizontal\",\n      legend.key.width = unit(3, \"cm\"),\n      legend.key.height = unit(0.4, \"cm\"),\n      plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n      plot.margin = margin(10, 10, 10, 10)\n    ) +\n    labs(title = \"Correlation Matrix\") +\n    coord_fixed()\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n  ggsave(\"plot/corr_matrix_enhanced.png\", p_corr, width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/corr_matrix_enhanced.png\\n\")\n}\n\n   plot/corr_matrix_enhanced.png\n\n\n\n# =========================================================\n# 2. VIF Analysis\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== VIF Analysis ===\\n\\n\")\n\n=== VIF Analysis ===\n\nvif_vars &lt;- setdiff(names(df), c(y_var, cat_vars, coord_vars))\nnum_vif &lt;- df %&gt;% dplyr::select(all_of(vif_vars)) %&gt;% dplyr::select(where(is.numeric))\nnum_vif &lt;- num_vif[, sapply(num_vif, function(x) sd(x, na.rm = TRUE) &gt; 0), drop = FALSE]\n\nif (ncol(num_vif) &gt;= 2) {\n  df_vif &lt;- df %&gt;% dplyr::select(all_of(c(y_var, names(num_vif)))) %&gt;% na.omit()\n  f_vif &lt;- as.formula(paste(y_var, \"~ .\"))\n  vif_model &lt;- lm(f_vif, data = df_vif)\n  vif_vals &lt;- car::vif(vif_model)\n  vif_tbl &lt;- data.frame(variable = names(vif_vals), VIF = round(vif_vals, 3))\n  vif_tbl &lt;- vif_tbl %&gt;% arrange(desc(VIF))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\n  write_csv(vif_tbl, \"file/vif_values.csv\")\n  cat(\"  ‚úì file/vif_values.csv\\n\")\n  \n  # High VIF warning\n  high_vif &lt;- vif_tbl %&gt;% filter(VIF &gt; 10)\n  if (nrow(high_vif) &gt; 0) {\n    cat(\"\\n  Variables with high multicollinearity (VIF &gt; 10):\\n\")\n    print(as.data.frame(high_vif), row.names = FALSE)\n  }\n  \n  # VIF visualization\n  vif_top20 &lt;- vif_tbl %&gt;% top_n(20, VIF)\n  \n  p_vif &lt;- ggplot(vif_top20, aes(x = reorder(variable, VIF), y = VIF, fill = VIF)) +\n    geom_col(alpha = 0.9) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 5, name = \"VIF Value\"\n    ) +\n    geom_hline(yintercept = 10, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n    geom_hline(yintercept = 5, linetype = \"dashed\", color = \"gray50\", linewidth = 0.7) +\n    coord_flip() +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.major.y = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Variance Inflation Factor (VIF) - Top 20\",\n      subtitle = \"Color gradient: Blue (low VIF) ‚Üí Red (high VIF)\",\n      x = \"Variable\", y = \"VIF Value\",\n      caption = \"Dark red line: VIF=10 (High multicollinearity) | Gray line: VIF=5 (Moderate)\"\n    )\n  \n  ggsave(\"plot/vif_analysis.png\", p_vif, width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/vif_analysis.png\\n\")\n}\n\n  ‚úì file/vif_values.csv\n\n  Variables with high multicollinearity (VIF &gt; 10):\n        variable    VIF\n per_cap_incomeE 10.608\n\n\n   plot/vif_analysis.png\n\n\n\n# =========================================================\n# 3. Spatial Visualization\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Spatial Distribution Visualization ===\\n\\n\")\n  \n  # 3.1 Spatial distribution of sale prices\n  p_spatial_price &lt;- ggplot(df, aes(x_coord, y_coord, color = .data[[y_var]])) +\n    geom_point(alpha = 0.6, size = 0.8) +\n    scale_color_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(df[[y_var]], na.rm = TRUE),\n      name = \"Sale Price\\n(log)\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"right\"\n    ) +\n    labs(\n      title = \"Spatial Distribution of Sale Prices\",\n      subtitle = \"Philadelphia housing market\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_price_distribution.png\", p_spatial_price,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_price_distribution.png\\n\")\n  \n  # 3.2 Hexbin density map\n  p_hex &lt;- ggplot(df, aes(x_coord, y_coord)) +\n    geom_hex(aes(fill = after_stat(count)), bins = 40) +\n    scale_fill_gradient(\n      low = \"#F7F7F7\", high = \"#67001F\",\n      name = \"Count\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Housing Density Heatmap\",\n      subtitle = \"Hexagonal binning of property locations\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_density_hexbin.png\", p_hex,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_density_hexbin.png\\n\")\n}\n\n\n================================================================================\n=== Spatial Distribution Visualization ===\n\n\n   plot/spatial_price_distribution.png\n\n\n   plot/spatial_density_hexbin.png\n\n\n\n# =========================================================\n# 4. Key Variable Scatterplots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Scatterplots: Key Variables vs. Sale Price ===\\n\\n\")\n\n=== Scatterplots: Key Variables vs. Sale Price ===\n\nkey_numeric &lt;- c(\"total_livable_area\", \"median_incomeE\", \"age\", \"number_of_bathrooms\")\nkey_numeric &lt;- intersect(key_numeric, names(df))\n\nif (length(key_numeric) &gt;= 4) {\n  scatter_plots &lt;- list()\n  \n  for (var in key_numeric[1:4]) {\n    # Correlation coefficient\n    corr_val &lt;- cor(df[[var]], df[[y_var]], use = \"pairwise.complete.obs\")\n    \n    p &lt;- ggplot(df, aes(.data[[var]], .data[[y_var]])) +\n      geom_point(alpha = 0.3, size = 0.8, color = \"#053061\") +\n      geom_smooth(method = \"lm\", se = TRUE, color = \"#67001F\", linewidth = 1.2) +\n      annotate(\"text\", x = Inf, y = -Inf, \n               label = sprintf(\"r = %.3f\", corr_val),\n               hjust = 1.1, vjust = -0.5, size = 4, color = \"#67001F\", fontface = \"bold\") +\n      theme_minimal(base_size = 10) +\n      theme(\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 11),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      ) +\n      labs(\n        title = gsub(\"_\", \" \", var),\n        x = gsub(\"_\", \" \", var),\n        y = \"Sale Price (log)\"\n      )\n    \n    scatter_plots[[var]] &lt;- p\n  }\n  \n  p_scatter &lt;- wrap_plots(scatter_plots, ncol = 2) +\n    plot_annotation(\n      title = \"Key Variables vs. Sale Price\",\n      subtitle = \"Linear fit (OLS) with correlation coefficients\",\n      theme = theme(\n        plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 11, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/key_variables_scatter.png\", p_scatter,\n         width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/key_variables_scatter.png\\n\")\n}\n\n   plot/key_variables_scatter.png\n\n\n\n# =========================================================\n# Save cleaned data\n# =========================================================\n\nwrite_csv(df, \"file/opa_sales_step2_clean.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"  Step 2 Completed\\n\")\n\n  Step 2 Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\"  Output files:\\n\\n\")\n\n  Output files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\ncat(\"  ‚Ä¢ file/vif_values.csv - VIF analysis results\\n\")\n\n  ‚Ä¢ file/vif_values.csv - VIF analysis results\n\ncat(\"  ‚Ä¢ file/opa_sales_step2_clean.csv - Cleaned dataset\\n\\n\")\n\n  ‚Ä¢ file/opa_sales_step2_clean.csv - Cleaned dataset\n\ncat(\"Figures:\\n\")\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/corr_matrix_enhanced.png - Correlation matrix\\n\")\n\n  ‚Ä¢ plot/corr_matrix_enhanced.png - Correlation matrix\n\ncat(\"  ‚Ä¢ plot/vif_analysis.png - VIF analysis\\n\")\n\n  ‚Ä¢ plot/vif_analysis.png - VIF analysis\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  ‚Ä¢ plot/spatial_price_distribution.png - Spatial distribution of sale prices\\n\")\n  cat(\"  ‚Ä¢ plot/spatial_density_hexbin.png - Housing density heatmap\\n\")\n}\n\n  ‚Ä¢ plot/spatial_price_distribution.png - Spatial distribution of sale prices\n  ‚Ä¢ plot/spatial_density_hexbin.png - Housing density heatmap\n\nif (length(key_numeric) &gt;= 4) {\n  cat(\"  ‚Ä¢ plot/key_variables_scatter.png - Key variables vs. sale price (scatterplots)\\n\")\n}\n\n  ‚Ä¢ plot/key_variables_scatter.png - Key variables vs. sale price (scatterplots)\n\ncat(\"\\n\")\n\nStep3\n\n# =========================================================\n# Step 3: VIF-based Filtering + LASSO Feature Selection (Streamlined)\n# =========================================================\n\nlibrary(glmnet)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\nset.seed(2025)\n\n# === Read data and VIF results ===\ndf &lt;- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nvif_path &lt;- \"file/vif_values.csv\"\nif (!file.exists(vif_path)) stop(\"  file/vif_values.csv not found. Please run step2.R first.\")\nvif_table &lt;- read_csv(vif_path, show_col_types = FALSE)\n\n# === Determine target variable ===\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\"  Target variable sale_price_log or sale_price not found\")\n\n# === Variable filtering (VIF) ===\nvif_threshold &lt;- 5\nvars_keep &lt;- vif_table %&gt;%\n  filter(VIF &lt;= vif_threshold) %&gt;%\n  pull(variable)\nvars_keep &lt;- intersect(vars_keep, names(df))\n\ncat(\"  Number of variables passing VIF ‚â§\", vif_threshold, \":\", length(vars_keep), \"\\n\")\n\n  Number of variables passing VIF ‚â§ 5 : 13 \n\n\n\n# === Prepare data matrix ===\ndf_lasso &lt;- df %&gt;%\n  dplyr::select(all_of(c(y_var, vars_keep))) %&gt;%\n  na.omit()\n\nx &lt;- as.matrix(df_lasso %&gt;% dplyr::select(-all_of(y_var)))\ny &lt;- df_lasso[[y_var]]\n\n# === Run LASSO regression ===\ncvfit &lt;- cv.glmnet(\n  x, y,\n  alpha = 1,            # LASSO\n  nfolds = 10,\n  standardize = TRUE\n)\n\nlambda_min &lt;- cvfit$lambda.min\nlambda_1se &lt;- cvfit$lambda.1se\ncat(\"Œª_min =\", signif(lambda_min, 5), \"\\n\")\n\nŒª_min = 0.00085507 \n\ncat(\"Œª_1se =\", signif(lambda_1se, 5), \"\\n\")\n\nŒª_1se = 0.026727 \n\n# === Extract non-zero coefficients ===\ncoef_min &lt;- coef(cvfit, s = \"lambda.min\")\nselected &lt;- rownames(coef_min)[coef_min[, 1] != 0]\nselected &lt;- selected[selected != \"(Intercept)\"]\n\nselected_tbl &lt;- data.frame(\n  variable = selected,\n  coefficient = as.numeric(coef_min[selected, 1])\n)\n\n# === Remove variables with near-zero coefficients ===\nselected_tbl &lt;- selected_tbl %&gt;%\n  filter(abs(coefficient) &gt;= 1e-5) %&gt;%\n  arrange(desc(abs(coefficient)))\n\ncat(\"  Number of variables retained after LASSO:\", nrow(selected_tbl), \"\\n\")\n\n  Number of variables retained after LASSO: 12 \n\n\n\n# === Output files ===\nif (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\nif (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n\nwrite_csv(selected_tbl, \"file/lasso_selected_variables.csv\")\n\n\n# === Visualization 1: LASSO Coefficient Importance (Top 20) ===\ntop_n_vars &lt;- min(20, nrow(selected_tbl))\nselected_top &lt;- selected_tbl %&gt;% \n  top_n(top_n_vars, abs(coefficient)) %&gt;%\n  arrange(coefficient)\n\np_coef &lt;- ggplot(selected_top, aes(x = reorder(variable, coefficient), y = coefficient, fill = coefficient)) +\n  geom_col(alpha = 0.9) +\n  scale_fill_gradient2(\n    low = \"#053061\",      \n    mid = \"#F7F7F7\",      \n    high = \"#67001F\",     \n    midpoint = 0,\n    name = \"Coefficient\"\n  ) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  coord_flip() +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Selected Variables - Coefficient Importance\",\n    subtitle = paste0(\"Top \", top_n_vars, \" variables by absolute coefficient value\"),\n    x = \"Variable\",\n    y = \"Coefficient\",\n    caption = paste0(\"Œª_min = \", signif(lambda_min, 4), \" | Total selected: \", nrow(selected_tbl), \" variables\")\n  )\n\nggsave(\"plot/lasso_coefficients.png\", p_coef, width = 10, height = 8, dpi = 300, bg = \"white\")\n\n\n# === Visualization 2: Cross-Validation Curve (Œª Selection) ===\ncv_df &lt;- data.frame(\n  lambda = cvfit$lambda,\n  cvm = cvfit$cvm,\n  cvsd = cvfit$cvsd,\n  cvlo = cvfit$cvm - cvfit$cvsd,\n  cvup = cvfit$cvm + cvfit$cvsd\n)\n\np_cv &lt;- ggplot(cv_df, aes(x = log(lambda), y = cvm)) +\n  geom_ribbon(aes(ymin = cvlo, ymax = cvup), fill = \"#4393C3\", alpha = 0.3) +\n  geom_line(color = \"#053061\", linewidth = 1) +\n  geom_point(color = \"#053061\", size = 2, alpha = 0.6) +\n  geom_vline(xintercept = log(lambda_min), linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  geom_vline(xintercept = log(lambda_1se), linetype = \"dashed\", color = \"#D73027\", linewidth = 0.7) +\n  annotate(\"text\", x = log(lambda_min), y = max(cv_df$cvm) * 0.95, \n           label = paste0(\"Œª_min = \", signif(lambda_min, 3)), \n           hjust = -0.1, size = 3.5, color = \"#67001F\") +\n  annotate(\"text\", x = log(lambda_1se), y = max(cv_df$cvm) * 0.90, \n           label = paste0(\"Œª_1se = \", signif(lambda_1se, 3)), \n           hjust = -0.1, size = 3.5, color = \"#D73027\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Cross-Validation Curve\",\n    subtitle = \"Mean Squared Error vs. log(Œª)\",\n    x = \"log(Œª)\",\n    y = \"Mean Squared Error\",\n    caption = \"Shaded area represents ¬±1 standard error\"\n  )\n\nggsave(\"plot/lasso_cv_curve.png\", p_cv, width = 10, height = 7, dpi = 300, bg = \"white\")\n\n\n# === Selection Summary ===\ncat(\"\\n\", rep(\"=\", 60), \"\\n\", sep = \"\")\n\n\n============================================================\n\ncat(\" Summary of Feature Selection Results\\n\")\n\n Summary of Feature Selection Results\n\ncat(rep(\"=\", 60), \"\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\"Initial number of predictors:\", ncol(x), \"\\n\")\n\nInitial number of predictors: 13 \n\ncat(\"After VIF ‚â§\", vif_threshold, \":\", length(vars_keep), \"\\n\")\n\nAfter VIF ‚â§ 5 : 13 \n\ncat(\"Retained after LASSO:\", nrow(selected_tbl), \"\\n\")\n\nRetained after LASSO: 12 \n\ncat(\"Selection rate:\", round((1 - nrow(selected_tbl) / ncol(x)) * 100, 1), \"%\\n\")\n\nSelection rate: 7.7 %\n\ncat(rep(\"=\", 60), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Step 3 Completed\\n\")\n\n Step 3 Completed\n\ncat(\" LASSO results: file/lasso_selected_variables.csv\\n\")\n\n LASSO results: file/lasso_selected_variables.csv\n\ncat(\" Coefficient importance: plot/lasso_coeff_importance_top20.png\\n\")\n\n Coefficient importance: plot/lasso_coeff_importance_top20.png\n\ncat(\" CV curve: plot/lasso_cv_curve.png\\n\")\n\n CV curve: plot/lasso_cv_curve.png\n\n\nStep4\n\n# =========================================================\n# Step 4 Enhanced: Four Progressive Models + Coefficient Tables + Feature Importance\n# =========================================================\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(car)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"  Step 4: OLS Progressive Modeling and Full Analysis\\n\")\n\n  Step 4: OLS Progressive Modeling and Full Analysis\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n\n\n# === Read Data ===\ndf &lt;- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\n\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat(sprintf(\"Target variable: %s\\n\", y_var))\n\nTarget variable: sale_price\n\ncat(sprintf(\"Sample size: %d\\n\\n\", nrow(df)))\n\nSample size: 24023\n\n\n\n# === Data preprocessing ===\ncat(\"=== Data preprocessing ===\\n\")\n\n=== Data preprocessing ===\n\ncat_vars &lt;- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncat_vars &lt;- intersect(cat_vars, names(df))\n\nfor (cat_var in cat_vars) {\n  if (cat_var %in% c(\"zip_code\", \"census_tract\")) {\n    freq_table &lt;- table(df[[cat_var]])\n    sparse_levels &lt;- names(freq_table[freq_table &lt; 100])\n    df[[cat_var]] &lt;- as.character(df[[cat_var]])\n    df[[cat_var]][df[[cat_var]] %in% sparse_levels] &lt;- \"Other\"\n    df[[cat_var]] &lt;- factor(df[[cat_var]])\n    cat(sprintf(\"  %s: %d ‚Üí %d categories\\n\", cat_var, length(freq_table), length(levels(df[[cat_var]]))))\n  } else {\n    df[[cat_var]] &lt;- factor(df[[cat_var]])\n  }\n}\n\n  zip_code: 46 ‚Üí 46 categories\n  census_tract: 310 ‚Üí 86 categories\n\ncoord_vars &lt;- c(\"x_coord\", \"y_coord\")\nif (all(coord_vars %in% names(df))) {\n  center_x &lt;- median(df$x_coord, na.rm = TRUE)\n  center_y &lt;- median(df$y_coord, na.rm = TRUE)\n  df$dist_to_center &lt;- sqrt((df$x_coord - center_x)^2 + (df$y_coord - center_y)^2)\n  cat(\"  Added: dist_to_center\\n\")\n} else {\n  coord_vars &lt;- c()\n}\n\n  Added: dist_to_center\n\n# Identify variable groups\nstructural_vars &lt;- c()\nfor (pattern in c(\"livable_area\", \"bedroom\", \"bathroom\", \"stories\", \"garage\", \"age\")) {\n  matched &lt;- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  structural_vars &lt;- c(structural_vars, matched)\n}\nstructural_vars &lt;- unique(structural_vars)\nstructural_vars &lt;- structural_vars[sapply(df[structural_vars], is.numeric)]\n\ncensus_vars &lt;- c()\nfor (pattern in c(\"income\", \"poverty\", \"education\", \"bachelor\", \"population\", \"household\")) {\n  matched &lt;- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  census_vars &lt;- c(census_vars, matched)\n}\ncensus_vars &lt;- unique(census_vars)\ncensus_vars &lt;- census_vars[sapply(df[census_vars], is.numeric)]\n\nspatial_vars &lt;- c(coord_vars, \"dist_to_center\")\nspatial_vars &lt;- intersect(spatial_vars, names(df))\n\nfixed_effects &lt;- c(\"zip_code\", \"census_tract\")\nfixed_effects &lt;- intersect(fixed_effects, names(df))\n\ncat(sprintf(\"\\n  Structural features: %d\\n\", length(structural_vars)))\n\n\n  Structural features: 5\n\ncat(sprintf(\"  Census features: %d\\n\", length(census_vars)))\n\n  Census features: 3\n\ncat(sprintf(\"  Spatial features: %d\\n\", length(spatial_vars)))\n\n  Spatial features: 3\n\ncat(sprintf(\"  Fixed effects: %d\\n\", length(fixed_effects)))\n\n  Fixed effects: 2\n\n\n\n# =========================================================\n# Build 4 Progressive Models\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Building 4 Progressive Models ===\\n\\n\")\n\n=== Building 4 Progressive Models ===\n\nmake_formula &lt;- function(y, main_vars, interact_vars = NULL, fe_vars = NULL) {\n  rhs &lt;- main_vars\n  if (!is.null(interact_vars) && length(interact_vars) == 2) {\n    rhs &lt;- c(rhs, paste(interact_vars, collapse = \":\"))\n  }\n  if (!is.null(fe_vars)) {\n    rhs &lt;- c(rhs, fe_vars)\n  }\n  as.formula(paste(y, \"~\", paste(rhs, collapse = \" + \")))\n}\n\nf1 &lt;- make_formula(y_var, structural_vars)\nf2 &lt;- make_formula(y_var, c(structural_vars, census_vars))\nf3 &lt;- make_formula(y_var, c(structural_vars, census_vars, spatial_vars))\n\ninteract_pairs &lt;- NULL\nif (\"total_livable_area\" %in% structural_vars && \"median_incomeE\" %in% census_vars) {\n  interact_pairs &lt;- c(\"total_livable_area\", \"median_incomeE\")\n}\n\nf4 &lt;- make_formula(y_var, c(structural_vars, census_vars, spatial_vars),\n                   interact_vars = interact_pairs, fe_vars = fixed_effects)\n\nmodels &lt;- list()\ncat(\"  Fitting Model 1...\"); models[[\"M1: Structural\"]] &lt;- lm(f1, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 1...\n\n\n ‚úì\n\ncat(\"  Fitting Model 2...\"); models[[\"M2: +Census\"]] &lt;- lm(f2, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 2...\n\n\n ‚úì\n\ncat(\"  Fitting Model 3...\"); models[[\"M3: +Spatial\"]] &lt;- lm(f3, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 3...\n\n\n ‚úì\n\ncat(\"  Fitting Model 4...\"); models[[\"M4: +Interact+FE\"]] &lt;- lm(f4, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 4...\n\n\n ‚úì\n\n\n\n# =========================================================\n# Model Performance Evaluation\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Model Performance Evaluation ===\\n\\n\")\n\n=== Model Performance Evaluation ===\n\ntrain_perf &lt;- lapply(models, function(m) {\n  pred &lt;- fitted(m)\n  actual &lt;- df[[y_var]][!is.na(fitted(m))]\n  \n  # Log-scale metrics\n  rmse_log &lt;- sqrt(mean((pred - actual)^2, na.rm = TRUE))\n  mae_log &lt;- mean(abs(pred - actual), na.rm = TRUE)\n  \n  # Original-scale metrics ‚Äî use expm1 to invert log1p\n  pred_original &lt;- expm1(pred)\n  actual_original &lt;- expm1(actual)\n  \n  # Method 3: remove outliers based on residuals (|residual| &gt; 3 standard deviations)\n  residuals_original &lt;- pred_original - actual_original\n  res_sd &lt;- sd(residuals_original, na.rm = TRUE)\n  valid_idx &lt;- abs(residuals_original) &lt;= 3 * res_sd\n  \n  # Compute RMSE and MAE after removing outliers\n  rmse_original &lt;- sqrt(mean((pred_original[valid_idx] - actual_original[valid_idx])^2, na.rm = TRUE))\n  mae_original &lt;- mean(abs(pred_original[valid_idx] - actual_original[valid_idx]), na.rm = TRUE)\n  \n  data.frame(\n    RMSE_log = rmse_log,\n    RMSE_original = rmse_original,\n    MAE_log = mae_log,\n    MAE_original = mae_original,\n    R2 = summary(m)$r.squared,\n    Adj_R2 = summary(m)$adj.r.squared,\n    N_vars = length(coef(m)) - 1,\n    N = length(pred),\n    N_valid = sum(valid_idx, na.rm = TRUE),      # retained sample count\n    Pct_valid = mean(valid_idx, na.rm = TRUE) * 100  # percentage of retained samples\n  )\n}) %&gt;% bind_rows(.id = \"Model\")\n\n# Print removal stats\ncat(\"\\nOutlier removal statistics (|residual| &gt; 3œÉ):\\n\")\n\n\nOutlier removal statistics (|residual| &gt; 3œÉ):\n\nprint(train_perf %&gt;% select(Model, N, N_valid, Pct_valid, RMSE_original, MAE_original))\n\n             Model     N N_valid Pct_valid RMSE_original MAE_original\n1   M1: Structural 24023   23693  98.62632      172980.9    113702.55\n2      M2: +Census 24023   23713  98.70957      137239.1     84251.14\n3     M3: +Spatial 24023   23714  98.71373      135990.7     82773.39\n4 M4: +Interact+FE 24023   23726  98.76368      121478.7     73179.58\n\ncat(\"Performing 10-fold cross-validation...\\n\")\n\nPerforming 10-fold cross-validation...\n\ncv_ctrl &lt;- trainControl(method = \"cv\", number = 10, verboseIter = FALSE)\nformulas &lt;- list(f1, f2, f3, f4)\nnames(formulas) &lt;- names(models)\n\ncv_results &lt;- lapply(formulas, function(fm) {\n  tryCatch(train(fm, data = df, method = \"lm\", trControl = cv_ctrl, na.action = na.omit),\n           error = function(e) NULL)\n})\ncv_results &lt;- Filter(Negate(is.null), cv_results)\n\ncv_perf &lt;- lapply(cv_results, function(m) {\n  data.frame(CV_RMSE_log = m$results$RMSE[1], CV_R2 = m$results$Rsquared[1])\n}) %&gt;% bind_rows(.id = \"Model\")\n\nperf_table &lt;- train_perf %&gt;%\n  left_join(cv_perf, by = \"Model\") %&gt;%\n  mutate(Overfit_RMSE_log = CV_RMSE_log - RMSE_log, Overfit_R2 = R2 - CV_R2)\n\ncat(\"\\n\")\nprint(perf_table, digits = 4, row.names = FALSE)\n\n            Model RMSE_log RMSE_original MAE_log MAE_original     R2 Adj_R2\n   M1: Structural   0.6863        172981  0.4860       113703 0.3442 0.3441\n      M2: +Census   0.5917        137239  0.3776        84251 0.5126 0.5124\n     M3: +Spatial   0.5870        135991  0.3714        82773 0.5202 0.5200\n M4: +Interact+FE   0.5561        121479  0.3380        73180 0.5695 0.5669\n N_vars     N N_valid Pct_valid CV_RMSE_log  CV_R2 Overfit_RMSE_log Overfit_R2\n      5 24023   23693     98.63      0.6862 0.3445       -7.634e-05 -0.0003115\n      8 24023   23713     98.71      0.5916 0.5128       -8.878e-05 -0.0001838\n     11 24023   23714     98.71      0.5869 0.5204       -1.000e-04 -0.0001902\n    142 24023   23726     98.76      0.5593 0.5644        3.269e-03  0.0050628\n\n\n\n# =========================================================\n# Heteroskedasticity Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Heteroskedasticity Test (Model 4) ===\\n\\n\")\n\n=== Heteroskedasticity Test (Model 4) ===\n\nbp_test &lt;- bptest(models[[\"M4: +Interact+FE\"]])\ncat(sprintf(\"Breusch‚ÄìPagan Test:\\n  Statistic = %.2f\\n  p-value = %.4f\\n\", \n            bp_test$statistic, bp_test$p.value))\n\nBreusch‚ÄìPagan Test:\n  Statistic = 2134.62\n  p-value = 0.0000\n\nif (bp_test$p.value &lt; 0.05) {\n  cat(\"\\nÔ∏è Significant heteroskedasticity detected (p &lt; 0.05)\\n   Recommendation: use robust standard errors\\n\")\n} else {\n  cat(\"\\n No significant heteroskedasticity detected\\n\")\n}\n\n\nÔ∏è Significant heteroskedasticity detected (p &lt; 0.05)\n   Recommendation: use robust standard errors\n\n\n\n# =========================================================\n# Model 4 Full Coefficient Table (Robust Standard Errors)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Model 4: Full Coefficient Table (Robust SE) ===\\n\\n\")\n\n=== Model 4: Full Coefficient Table (Robust SE) ===\n\nmodel4 &lt;- models[[\"M4: +Interact+FE\"]]\nrobust_coef &lt;- coeftest(model4, vcov = vcovHC(model4, type = \"HC1\"))\n\ncoef_table &lt;- tidy(robust_coef) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    sig = case_when(\n      p.value &lt; 0.001 ~ \"***\",\n      p.value &lt; 0.01 ~ \"**\",\n      p.value &lt; 0.05 ~ \"*\",\n      p.value &lt; 0.10 ~ \".\",\n      TRUE ~ \"\"\n    )\n  ) %&gt;%\n  arrange(p.value) %&gt;%\n  dplyr::select(term, estimate, std.error, statistic, p.value, sig)\n\nif (!dir.exists(\"file\")) dir.create(\"file\")\nif (!dir.exists(\"table\")) dir.create(\"table\")\n\nwrite_csv(coef_table, \"table/model4_full_coefficients.csv\")\ncat(\"  ‚úì table/model4_full_coefficients.csv (All coefficients)\\n\")\n\n  ‚úì table/model4_full_coefficients.csv (All coefficients)\n\ncoef_sig &lt;- coef_table %&gt;% filter(p.value &lt; 0.05)\nwrite_csv(coef_sig, \"table/model4_significant_coefficients.csv\")\ncat(\"  ‚úì table/model4_significant_coefficients.csv (Significant coefficients)\\n\")\n\n  ‚úì table/model4_significant_coefficients.csv (Significant coefficients)\n\ncat(sprintf(\"\\nNumber of significant variables (p &lt; 0.05): %d\\n\", nrow(coef_sig)))\n\n\nNumber of significant variables (p &lt; 0.05): 74\n\ncat(\"\\nTop 20 significant variables:\\n\")\n\n\nTop 20 significant variables:\n\nprint(as.data.frame(head(coef_sig, 20)), row.names = FALSE)\n\n                term    estimate   std.error  statistic       p.value sig\n number_of_bathrooms  0.57683321 0.015420414  37.407116 1.142354e-297 ***\n     per_cap_incomeE  0.36773913 0.025683742  14.317973  2.628332e-46 ***\n  total_livable_area  0.48199968 0.038119301  12.644505  1.571285e-36 ***\n       zip_code19103  0.63540760 0.054794635  11.596165  5.218699e-31 ***\n                age2 -0.04775122 0.004555385 -10.482367  1.183564e-25 ***\n       census_tract7  0.87985167 0.088002818   9.997994  1.729730e-23 ***\n      census_tract13  0.54303063 0.068221853   7.959775  1.799726e-15 ***\n      census_tract14  0.54498041 0.068646953   7.938887  2.129287e-15 ***\n      census_tract12  0.54252167 0.068824818   7.882646  3.341484e-15 ***\n       zip_code19106  0.63471600 0.085682176   7.407795  1.326740e-13 ***\n       zip_code19147  0.49363371 0.066777930   7.392169  1.492021e-13 ***\n     census_tract180  0.72410180 0.099219613   7.297970  3.012660e-13 ***\n       zip_code19130  0.49781714 0.072690260   6.848471  7.645435e-12 ***\n       zip_code19107  0.49815251 0.073630825   6.765543  1.358805e-11 ***\n       zip_code19148  0.44251467 0.073274428   6.039142  1.572250e-09 ***\n      census_tract11  0.39171657 0.066613539   5.880435  4.146363e-09 ***\n     census_tract379  0.54364745 0.102377266   5.310236  1.104621e-07 ***\n     census_tract179  0.61289816 0.115555239   5.303941  1.143375e-07 ***\n       zip_code19146  0.34323604 0.065585163   5.233440  1.677892e-07 ***\n       zip_code19123  0.37828484 0.072407204   5.224409  1.761784e-07 ***\n\n\n\n# =========================================================\n# Stargazer Regression Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating Stargazer Regression Table ===\\n\\n\")\n\n=== Generating Stargazer Regression Table ===\n\nif (requireNamespace(\"stargazer\", quietly = TRUE)) {\n  sink(\"table/regression_table.txt\")\n  stargazer::stargazer(\n    models[[1]], models[[2]], models[[3]], models[[4]],\n    type = \"text\",\n    title = \"Progressive OLS Regression Results\",\n    dep.var.labels = \"Log Sale Price\",\n    column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"),\n    omit.stat = c(\"ser\", \"f\"),\n    digits = 3,\n    star.cutoffs = c(0.05, 0.01, 0.001),\n    notes = \"Robust standard errors in parentheses\"\n  )\n  sink()\n  cat(\"   table/regression_table.txt\\n\")\n}\n\n\n# =========================================================\n# Cross-Model Feature Importance\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Cross-Model Feature Importance Comparison ===\\n\\n\")\n\n=== Cross-Model Feature Importance Comparison ===\n\nimportance_list &lt;- list()\nfor (i in 1:4) {\n  model_name &lt;- names(models)[i]\n  model &lt;- models[[i]]\n  \n  coefs &lt;- tidy(model) %&gt;%\n    filter(term != \"(Intercept)\") %&gt;%\n    mutate(abs_estimate = abs(estimate)) %&gt;%\n    arrange(desc(abs_estimate)) %&gt;%\n    head(10) %&gt;%\n    mutate(Model = model_name) %&gt;%\n    dplyr::select(Model, term, estimate, abs_estimate)\n  \n  importance_list[[i]] &lt;- coefs\n}\n\nimportance_all &lt;- bind_rows(importance_list)\nwrite_csv(importance_all, \"table/feature_importance_all_models.csv\")\ncat(\"  ‚úì table/feature_importance_all_models.csv\\n\")\n\n  ‚úì table/feature_importance_all_models.csv\n\n# Identify variables that are important across multiple models\ntop_vars &lt;- importance_all %&gt;%\n  group_by(term) %&gt;%\n  summarise(appearances = n(), .groups = \"drop\") %&gt;%\n  filter(appearances &gt;= 3) %&gt;%\n  pull(term)\n\nif (length(top_vars) &gt; 0) {\n  importance_key &lt;- importance_all %&gt;% filter(term %in% top_vars)\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\")\n  \n  p_importance &lt;- ggplot(importance_key, aes(Model, abs_estimate, fill = Model)) +\n    geom_col(alpha = 0.8) +\n    facet_wrap(~term, scales = \"free_y\", ncol = 4) +\n    scale_fill_manual(\n      values = c(\"#053061\", \"#2166AC\", \"#4393C3\", \"#D6604D\"),\n      guide = \"none\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1, size = 7),\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.major.x = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      plot.subtitle = element_text(hjust = 0.5, size = 10),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Feature Importance Across Models\",\n      subtitle = \"Variables appearing in top 10 of at least 3 models\",\n      x = NULL, y = \"Absolute Coefficient\"\n    )\n  \n  ggsave(\"plot/feature_importance_across_models.png\", p_importance,\n         width = 14, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/feature_importance_across_models.png\\n\")\n}\n\n   plot/feature_importance_across_models.png\n\n\n\n# =========================================================\n# Visualization (Original 3 Figures)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generate Performance Visualizations ===\\n\\n\")\n\n=== Generate Performance Visualizations ===\n\nperf_plot_data &lt;- perf_table %&gt;% mutate(Model_Num = 1:n())\n\np_r2 &lt;- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = R2, color = \"Training R¬≤\"), linewidth = 1.2) +\n  geom_line(aes(y = CV_R2, color = \"CV R¬≤\"), linewidth = 1.2) +\n  geom_point(aes(y = R2, color = \"Training R¬≤\"), size = 3) +\n  geom_point(aes(y = CV_R2, color = \"CV R¬≤\"), size = 3) +\n  scale_color_manual(values = c(\"Training R¬≤\" = \"#67001F\", \"CV R¬≤\" = \"#053061\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"R¬≤ Improvement: Progressive Model Building\", x = NULL, y = \"R¬≤ Value\")\n\np_rmse &lt;- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), linewidth = 1.2) +\n  geom_point(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), size = 3) +\n  scale_color_manual(values = c(\"RMSE ($1000s)\" = \"#67001F\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"RMSE Reduction (Original Scale)\", x = NULL, y = \"RMSE ($1000s)\")\n\np_combined &lt;- (p_r2 | p_rmse) +\n  plot_annotation(\n    title = \"4-Model Progressive OLS: Performance Evolution\",\n    subtitle = \"From structural features to full specification with interactions and fixed effects\",\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 11, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/ols_4model_performance.png\", p_combined, width = 14, height = 6, dpi = 300, bg = \"white\")\ncat(\"  ‚úì plot/ols_4model_performance.png\\n\")\n\n  ‚úì plot/ols_4model_performance.png\n\n\n\n# Prediction Scatter Plot\npred_m4 &lt;- fitted(models[[\"M4: +Interact+FE\"]])\nactual_m4 &lt;- df[[y_var]][!is.na(pred_m4)]\npred_data &lt;- data.frame(\n  actual = actual_m4,\n  predicted = pred_m4[!is.na(pred_m4)],\n  residual = pred_m4[!is.na(pred_m4)] - actual_m4\n)\n\np_pred &lt;- ggplot(pred_data, aes(actual, predicted)) +\n  geom_point(aes(color = residual), alpha = 0.4, size = 0.8) +\n  scale_color_gradient2(low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\", midpoint = 0, name = \"Residual\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = min(pred_data$actual) + 0.5, y = max(pred_data$predicted) - 0.5,\n           label = sprintf(\"R¬≤ = %.4f\\nRMSE = %.4f\", perf_table$R2[4], perf_table$RMSE[4]),\n           hjust = 0, size = 4.5, fontface = \"bold\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"Model 4: Prediction vs. Actual\",\n       subtitle = \"Full specification with interactions and fixed effects\",\n       x = \"Actual Sale Price (log)\", y = \"Predicted Sale Price (log)\")\n\nggsave(\"plot/ols_4model_prediction.png\", p_pred, width = 10, height = 8, dpi = 300, bg = \"white\")\ncat(\"   plot/ols_4model_prediction.png\\n\")\n\n   plot/ols_4model_prediction.png\n\n\n\n# Residual Diagnostics\nresid_data &lt;- data.frame(\n  fitted = pred_m4[!is.na(pred_m4)],\n  residual = pred_data$residual,\n  std_resid = rstandard(models[[\"M4: +Interact+FE\"]])[!is.na(pred_m4)]\n)\n\np_r1 &lt;- ggplot(resid_data, aes(fitted, residual)) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residuals vs. Fitted\", x = \"Fitted\", y = \"Residuals\")\n\np_r2 &lt;- ggplot(resid_data, aes(sample = std_resid)) +\n  stat_qq(alpha = 0.4, size = 0.6, color = \"#053061\") +\n  stat_qq_line(color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical\", y = \"Std. Residuals\")\n\np_r3 &lt;- ggplot(resid_data, aes(std_resid)) +\n  geom_histogram(bins = 50, fill = \"#053061\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residual Distribution\", x = \"Std. Residuals\", y = \"Count\")\n\np_r4 &lt;- ggplot(resid_data, aes(fitted, sqrt(abs(std_resid)))) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Scale-Location\", x = \"Fitted\", y = \"‚àö|Std. Residuals|\")\n\np_diag &lt;- (p_r1 | p_r2) / (p_r3 | p_r4) +\n  plot_annotation(\n    title = sprintf(\"Model 4 Diagnostics (BP test p = %.4f)\", bp_test$p.value),\n    theme = theme(plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n                  plot.background = element_rect(fill = \"white\", color = NA))\n  )\n\nggsave(\"plot/ols_4model_diagnostics.png\", p_diag, width = 12, height = 10, dpi = 300, bg = \"white\")\ncat(\"  ‚úì plot/ols_4model_diagnostics.png\\n\")\n\n  ‚úì plot/ols_4model_diagnostics.png\n\n\n\n# =========================================================\n# Spatial Prediction Comparison Plot\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Generating Spatial Prediction Comparison Plot ===\\n\\n\")\n  \n  # Prepare data\n  spatial_pred_data &lt;- df %&gt;%\n    filter(!is.na(x_coord) & !is.na(y_coord)) %&gt;%\n    mutate(\n      predicted = fitted(models[[\"M4: +Interact+FE\"]]),\n      residual = predicted - .data[[y_var]]\n    ) %&gt;%\n    filter(!is.na(predicted))\n  \n  # If the dataset is too large, randomly sample to speed up plotting\n  if (nrow(spatial_pred_data) &gt; 10000) {\n    set.seed(2025)\n    spatial_pred_data &lt;- spatial_pred_data %&gt;% \n      slice_sample(n = 10000)\n    cat(\"  Large dataset detected; randomly sampled 10,000 points for plotting\\n\")\n  }\n\n# 1. Spatial Distribution of Actual Sale Prices\np_actual &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = .data[[y_var]])) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Actual Sale Price\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 2. Spatial Distribution of Predicted Sale Prices\np_predicted &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = predicted)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Predicted Sale Price (Model 4)\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 3. Spatial Distribution of Residuals\np_residual_spatial &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = residual)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = 0,\n    name = \"Residual\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Prediction Residuals\",\n    subtitle = \"Blue = Underpredicted | Red = Overpredicted\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# Combine the 3 plots\np_spatial_comparison &lt;- (p_actual | p_predicted | p_residual_spatial) +\n  plot_annotation(\n    title = \"Spatial Distribution: Actual vs. Predicted Sale Prices\",\n    subtitle = sprintf(\"Model 4 predictions (R¬≤ = %.4f, RMSE = %.4f)\", \n                      perf_table$R2[4], perf_table$RMSE[4]),\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/spatial_prediction_comparison.png\", p_spatial_comparison,\n       width = 18, height = 6, dpi = 300, bg = \"white\")\ncat(\"   plot/spatial_prediction_comparison.png\\n\")\n\n  # 4. High-Resolution Hexbin Heatmap Version\n  p_actual_hex &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = .data[[y_var]])) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Actual (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_predicted_hex &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = predicted)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Predicted (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_residual_hex &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = residual)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0,\n      name = \"Avg Residual\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Residuals (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_spatial_hexbin &lt;- (p_actual_hex | p_predicted_hex | p_residual_hex) +\n    plot_annotation(\n      title = \"Spatial Distribution (Hexbin Aggregation): Actual vs. Predicted\",\n      subtitle = \"Hexagonal bins show average values in each area\",\n      theme = theme(\n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/spatial_prediction_hexbin.png\", p_spatial_hexbin,\n         width = 18, height = 6, dpi = 300, bg = \"white\")\n  cat(\"  ‚úì plot/spatial_prediction_hexbin.png\\n\")\n  \n  # 5. Spatial Clustering of Residuals\n  # Calculate spatial statistics of residuals\n  residual_stats &lt;- spatial_pred_data %&gt;%\n    summarise(\n      mean_residual = mean(residual, na.rm = TRUE),\n      sd_residual = sd(residual, na.rm = TRUE),\n      min_residual = min(residual, na.rm = TRUE),\n      max_residual = max(residual, na.rm = TRUE)\n    )\n  \n  cat(\"\\n  Spatial Residual Statistics:\\n\")\n  cat(sprintf(\"    Mean: %.4f\\n\", residual_stats$mean_residual))\n  cat(sprintf(\"    Standard Deviation: %.4f\\n\", residual_stats$sd_residual))\n  cat(sprintf(\"    Range: [%.4f, %.4f]\\n\", \n              residual_stats$min_residual, residual_stats$max_residual))\n  \n  # Identify high-residual areas\n  high_residual_areas &lt;- spatial_pred_data %&gt;%\n    filter(abs(residual) &gt; 2 * residual_stats$sd_residual) %&gt;%\n    mutate(residual_type = ifelse(residual &gt; 0, \"Overpredicted\", \"Underpredicted\"))\n  \n  if (nrow(high_residual_areas) &gt; 0) {\n    cat(sprintf(\"\\n  Ô∏è Detected %d high-residual points (|residual| &gt; 2œÉ):\\n\", \n                nrow(high_residual_areas)))\n    cat(sprintf(\"    Overpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Overpredicted\")))\n    cat(sprintf(\"    Underpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Underpredicted\")))\n    \n    # Save high-residual point data\n    write_csv(high_residual_areas %&gt;% \n                dplyr::select(x_coord, y_coord, !!sym(y_var), predicted, residual, residual_type),\n              \"file/high_residual_locations.csv\")\n    cat(\"     file/high_residual_locations.csv\\n\")\n  }\n}\n\n\n================================================================================\n=== Generating Spatial Prediction Comparison Plot ===\n\n  Large dataset detected; randomly sampled 10,000 points for plotting\n\n\n   plot/spatial_prediction_comparison.png\n\n\n  ‚úì plot/spatial_prediction_hexbin.png\n\n  Spatial Residual Statistics:\n    Mean: 0.0036\n    Standard Deviation: 0.5571\n    Range: [-3.8418, 3.8371]\n\n  Ô∏è Detected 483 high-residual points (|residual| &gt; 2œÉ):\n    Overpredicted: 294\n    Underpredicted: 189\n     file/high_residual_locations.csv\n\n\n\n# =========================================================\n# Save Results\n# =========================================================\n\nwrite_csv(perf_table, \"file/ols_4model_performance.csv\")\nsaveRDS(models, \"file/ols_4models.rds\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 4 Completed\\n\")\n\n Step 4 Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Output Files:\\n\\n\")\n\n Output Files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\ncat(\"  ‚Ä¢ table/model4_full_coefficients.csv - Full coefficient table (Model 4)\\n\")\n\n  ‚Ä¢ table/model4_full_coefficients.csv - Full coefficient table (Model 4)\n\ncat(\"  ‚Ä¢ table/model4_significant_coefficients.csv - Significant coefficients (Model 4)\\n\")\n\n  ‚Ä¢ table/model4_significant_coefficients.csv - Significant coefficients (Model 4)\n\ncat(\"  ‚Ä¢ table/regression_table.txt - Stargazer regression summary\\n\")\n\n  ‚Ä¢ table/regression_table.txt - Stargazer regression summary\n\ncat(\"  ‚Ä¢ table/feature_importance_all_models.csv - Cross-model feature importance\\n\")\n\n  ‚Ä¢ table/feature_importance_all_models.csv - Cross-model feature importance\n\ncat(\"  ‚Ä¢ file/ols_4model_performance.csv - Model performance comparison\\n\")\n\n  ‚Ä¢ file/ols_4model_performance.csv - Model performance comparison\n\ncat(\"  ‚Ä¢ file/ols_4models.rds - RDS objects of all 4 models\\n\\n\")\n\n  ‚Ä¢ file/ols_4models.rds - RDS objects of all 4 models\n\ncat(\"Figures:\\n\")\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/ols_4model_performance.png - Performance evolution\\n\")\n\n  ‚Ä¢ plot/ols_4model_performance.png - Performance evolution\n\ncat(\"  ‚Ä¢ plot/ols_4model_prediction.png - Model 4 prediction vs. actual\\n\")\n\n  ‚Ä¢ plot/ols_4model_prediction.png - Model 4 prediction vs. actual\n\ncat(\"  ‚Ä¢ plot/ols_4model_diagnostics.png - Model 4 diagnostic plots\\n\")\n\n  ‚Ä¢ plot/ols_4model_diagnostics.png - Model 4 diagnostic plots\n\ncat(\"  ‚Ä¢ plot/feature_importance_across_models.png - Cross-model feature importance\\n\")\n\n  ‚Ä¢ plot/feature_importance_across_models.png - Cross-model feature importance\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  ‚Ä¢ plot/spatial_prediction_comparison.png - Spatial prediction comparison\\n\")\n  cat(\"  ‚Ä¢ plot/spatial_prediction_hexbin.png - Spatial prediction heatmap\\n\")\n}\n\n  ‚Ä¢ plot/spatial_prediction_comparison.png - Spatial prediction comparison\n  ‚Ä¢ plot/spatial_prediction_hexbin.png - Spatial prediction heatmap\n\ncat(\"\\n\")\ncat(\" Model Summary:\\n\")\n\n Model Summary:\n\nfor (i in 1:4) {\n  rmse_fmt &lt;- formatC(perf_table$RMSE_original[i], format = \"f\", digits = 0, big.mark = \",\")\n  cat(sprintf(\"  Model %d: R¬≤ = %.4f, RMSE = $%s [%d variables]\\n\",\n              i, perf_table$R2[i], rmse_fmt, perf_table$N_vars[i]))\n}\n\n  Model 1: R¬≤ = 0.3442, RMSE = $172,981 [5 variables]\n  Model 2: R¬≤ = 0.5126, RMSE = $137,239 [8 variables]\n  Model 3: R¬≤ = 0.5202, RMSE = $135,991 [11 variables]\n  Model 4: R¬≤ = 0.5695, RMSE = $121,479 [142 variables]\n\ncat(\"\\n Paper Writing Suggestions:\\n\")\n\n\n Paper Writing Suggestions:\n\ncat(\"  1. Results (Opening): Descriptive statistics table\\n\")\n\n  1. Results (Opening): Descriptive statistics table\n\ncat(\"  2. Results (Core): Stargazer regression summary (comparison of 4 models)\\n\")\n\n  2. Results (Core): Stargazer regression summary (comparison of 4 models)\n\ncat(\"  3. Results (Details): Significant coefficients of Model 4\\n\")\n\n  3. Results (Details): Significant coefficients of Model 4\n\ncat(\"  4. Discussion: Cross-model feature importance comparison\\n\")\n\n  4. Discussion: Cross-model feature importance comparison\n\ncat(\"  5. Mention the use of robust standard errors to address heteroskedasticity\\n\\n\")\n\n  5. Mention the use of robust standard errors to address heteroskedasticity\n\n\nStep4 plus\n\n# =========================================================\n# Step 4 Supplement: Cook‚Äôs Distance + Moran‚Äôs I Spatial Autocorrelation\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(spdep)  # Moran‚Äôs I\nlibrary(sf)     # Spatial data\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 4 Supplement: Influential Point Diagnostics and Spatial Autocorrelation Test\\n\")\n\n Step 4 Supplement: Influential Point Diagnostics and Spatial Autocorrelation Test\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n# === Load Data and Models ===\ndf &lt;- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nmodels &lt;- readRDS(\"file/ols_4models.rds\")\nmodel4 &lt;- models[[\"M4: +Interact+FE\"]]\n\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n\n# =========================================================\n# 1. Cook‚Äôs Distance ‚Äì Influential Observation Detection\n# =========================================================\n\ncat(\"=== Cook‚Äôs Distance Analysis ===\\n\\n\")\n\n=== Cook‚Äôs Distance Analysis ===\n\n# Calculate Cook‚Äôs distance\ncooks_d &lt;- cooks.distance(model4)\nn &lt;- length(cooks_d)\nthreshold &lt;- 4/n  # Classic threshold\n\n# Identify influential points\ninfluential &lt;- which(cooks_d &gt; threshold)\ncat(sprintf(\"Sample size: %d\\n\", n))\n\nSample size: 24023\n\ncat(sprintf(\"Cook‚Äôs D threshold: %.6f (4/n)\\n\", threshold))\n\nCook‚Äôs D threshold: 0.000167 (4/n)\n\ncat(sprintf(\"Number of influential points: %d (%.2f%%)\\n\", length(influential), 100*length(influential)/n))\n\nNumber of influential points: 1060 (4.41%)\n\n# Save influential observations\nif (length(influential) &gt; 0) {\n  influential_data &lt;- df[influential, ] %&gt;%\n    mutate(\n      cooks_d = cooks_d[influential],\n      fitted = fitted(model4)[influential],\n      residual = residuals(model4)[influential]\n    ) %&gt;%\n    arrange(desc(cooks_d)) %&gt;%\n    head(100)  # Save Top 100\n  \n  write_csv(influential_data, \"file/influential_observations.csv\")\n  cat(\"  ‚úì file/influential_observations.csv (Top 100 Influential Points)\\n\")\n  \n  # Display Top 10\n  cat(\"\\nTop 10 Influential Points:\\n\")\n  print(influential_data %&gt;% \n          select(parcel_number, sale_price, cooks_d, fitted, residual) %&gt;% \n          head(10), \n        n = 10)\n}\n\n  ‚úì file/influential_observations.csv (Top 100 Influential Points)\n\nTop 10 Influential Points:\n# A tibble: 10 √ó 5\n   parcel_number sale_price cooks_d fitted residual\n   &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 211051105           9.39 0.00407   13.2    -3.84\n 2 071348000          15.0  0.00383   11.0     4.04\n 3 401237000          15.0  0.00343   11.6     3.44\n 4 401237400          15.0  0.00343   11.6     3.44\n 5 043166400          15.4  0.00335   11.6     3.85\n 6 041330100          15.4  0.00333   11.6     3.84\n 7 043055800          15.4  0.00329   11.6     3.82\n 8 381091300          14.7  0.00320   11.6     3.04\n 9 401220400          15.0  0.00311   11.8     3.28\n10 611317900           9.21 0.00303   13.3    -4.04\n\n# Visualize Cook‚Äôs Distance\ncooks_df &lt;- data.frame(\n  index = 1:n,\n  cooks_d = cooks_d,\n  influential = cooks_d &gt; threshold\n)\n\np_cooks &lt;- ggplot(cooks_df, aes(index, cooks_d, color = influential)) +\n  geom_point(alpha = 0.4, size = 0.8) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"Normal\", \"TRUE\" = \"Influential\"),\n    name = NULL\n  ) +\n  annotate(\"text\", x = n*0.8, y = threshold*1.2,\n           label = sprintf(\"Threshold = %.6f\", threshold),\n           color = \"#67001F\", size = 4) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Cook‚Äôs Distance ‚Äì Detection of Influential Observations\",\n    subtitle = sprintf(\"%d influential points (%.2f%% of data)\", \n                      length(influential), 100*length(influential)/n),\n    x = \"Observation Index\",\n    y = \"Cook‚Äôs Distance\",\n    caption = \"Threshold = 4/n (classic rule)\"\n  )\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nggsave(\"plot/cooks_distance.png\", p_cooks, width = 12, height = 6, dpi = 300, bg = \"white\")\ncat(\"\\n   plot/cooks_distance.png\\n\")\n\n\n   plot/cooks_distance.png\n\n\n\n# =========================================================\n# 2. Moran‚Äôs I ‚Äì Spatial Autocorrelation Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Moran‚Äôs I Spatial Autocorrelation Test ===\\n\\n\")\n\n=== Moran‚Äôs I Spatial Autocorrelation Test ===\n\n# Check coordinates\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  \n  # Prepare spatial data\n  df_spatial &lt;- df %&gt;%\n    filter(!is.na(x_coord), !is.na(y_coord)) %&gt;%\n    mutate(\n      residual = residuals(model4)[!is.na(x_coord) & !is.na(y_coord)],\n      sale_price_actual = .data[[y_var]]\n    ) %&gt;%\n    filter(!is.na(residual))\n  \n  # Convert to sf object\n  coords &lt;- df_spatial %&gt;% select(x_coord, y_coord)\n  coords_sf &lt;- st_as_sf(coords, coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n  \n  cat(sprintf(\"Valid samples: %d\\n\\n\", nrow(df_spatial)))\n  \n  # === 2.1 Moran‚Äôs I for Actual Sale Price ===\n  cat(\"--- Spatial Autocorrelation of Actual Sale Price ---\\n\")\n  \n  # Build spatial weight matrix (K-nearest neighbors, k=8)\n  nb_knn &lt;- knn2nb(knearneigh(coords_sf, k = 8))\n  listw_knn &lt;- nb2listw(nb_knn, style = \"W\")\n  \n  # Moran‚Äôs I test\n  moran_price &lt;- moran.test(df_spatial$sale_price_actual, listw_knn)\n  \n  cat(sprintf(\"  Moran‚Äôs I = %.4f\\n\", moran_price$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_price$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_price$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_price$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_price$p.value))\n  \n  if (moran_price$p.value &lt; 0.001) {\n    cat(\"  *** Highly significant positive spatial autocorrelation (p &lt; 0.001)\\n\")\n  } else if (moran_price$p.value &lt; 0.05) {\n    cat(\"  ** Significant positive spatial autocorrelation (p &lt; 0.05)\\n\")\n  } else {\n    cat(\"  No significant spatial autocorrelation detected\\n\")\n  }\n  \n  # === 2.2 Moran‚Äôs I for Residuals ===\n  cat(\"\\n--- Spatial Autocorrelation of Model 4 Residuals ---\\n\")\n  \n  moran_resid &lt;- moran.test(df_spatial$residual, listw_knn)\n  \n  cat(sprintf(\"  Moran‚Äôs I = %.4f\\n\", moran_resid$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_resid$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_resid$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_resid$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_resid$p.value))\n  \n  if (moran_resid$p.value &lt; 0.001) {\n    cat(\"  Ô∏è Strong residual spatial autocorrelation detected (p &lt; 0.001)\\n\")\n    cat(\"  Suggestion: Consider Spatial Lag Model (SAR) or Spatial Error Model (SEM)\\n\")\n  } else if (moran_resid$p.value &lt; 0.05) {\n    cat(\"  Ô∏è Residual spatial autocorrelation detected (p &lt; 0.05)\\n\")\n    cat(\"  Suggestion: Add more spatial variables or use spatial regression models\\n\")\n  } else {\n    cat(\"  ‚úì No significant residual spatial autocorrelation ‚Äì spatial effects well captured\\n\")\n  }\n  \n  # === 2.3 Moran Scatterplots ===\n  \n  # Calculate spatial lags\n  lag_price &lt;- lag.listw(listw_knn, df_spatial$sale_price_actual)\n  lag_resid &lt;- lag.listw(listw_knn, df_spatial$residual)\n  \n  # Standardize\n  price_std &lt;- scale(df_spatial$sale_price_actual)[,1]\n  lag_price_std &lt;- scale(lag_price)[,1]\n  resid_std &lt;- scale(df_spatial$residual)[,1]\n  lag_resid_std &lt;- scale(lag_resid)[,1]\n  \n  # Define quadrants\n  quadrant_price &lt;- case_when(\n    price_std &gt; 0 & lag_price_std &gt; 0 ~ \"HH (High-High)\",\n    price_std &lt; 0 & lag_price_std &lt; 0 ~ \"LL (Low-Low)\",\n    price_std &gt; 0 & lag_price_std &lt; 0 ~ \"HL (High-Low)\",\n    TRUE ~ \"LH (Low-High)\"\n  )\n  \n  quadrant_resid &lt;- case_when(\n    resid_std &gt; 0 & lag_resid_std &gt; 0 ~ \"HH\",\n    resid_std &lt; 0 & lag_resid_std &lt; 0 ~ \"LL\",\n    resid_std &gt; 0 & lag_resid_std &lt; 0 ~ \"HL\",\n    TRUE ~ \"LH\"\n  )\n  \n  moran_df_price &lt;- data.frame(\n    value = price_std,\n    lag_value = lag_price_std,\n    quadrant = quadrant_price\n  )\n  \n  moran_df_resid &lt;- data.frame(\n    value = resid_std,\n    lag_value = lag_resid_std,\n    quadrant = quadrant_resid\n  )\n  \n  # Plot ‚Äì Sale Price\n  p_moran_price &lt;- ggplot(moran_df_price, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH (High-High)\" = \"#67001F\", \n                 \"LL (Low-Low)\" = \"#053061\",\n                 \"HL (High-Low)\" = \"#F4A582\",\n                 \"LH (Low-High)\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran‚Äôs I = %.4f***\", moran_price$estimate[1]),\n             size = 5, fontface = \"bold\", color = \"#67001F\") +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran‚Äôs I Scatter Plot ‚Äì Sale Price\",\n      subtitle = \"Strong positive spatial autocorrelation\",\n      x = \"Standardized Sale Price\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Plot ‚Äì Residuals\n  p_moran_resid &lt;- ggplot(moran_df_resid, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH\" = \"#67001F\", \"LL\" = \"#053061\",\n                 \"HL\" = \"#F4A582\", \"LH\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran‚Äôs I = %.4f%s\", \n                           moran_resid$estimate[1],\n                           ifelse(moran_resid$p.value &lt; 0.001, \"***\",\n                                  ifelse(moran_resid$p.value &lt; 0.05, \"**\", \"\"))),\n             size = 5, fontface = \"bold\", \n             color = ifelse(moran_resid$p.value &lt; 0.05, \"#67001F\", \"gray50\")) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran‚Äôs I Scatter Plot ‚Äì Model 4 Residuals\",\n      subtitle = ifelse(moran_resid$p.value &lt; 0.05, \n                       \"Residual spatial autocorrelation detected\",\n                       \"No significant residual spatial autocorrelation\"),\n      x = \"Standardized Residual\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Combine plots\n  p_moran_combined &lt;- (p_moran_price | p_moran_resid) +\n    plot_annotation(\n      title = \"Spatial Autocorrelation Analysis (Moran‚Äôs I)\",\n      subtitle = \"Left: Original prices show strong clustering | Right: Model residuals\",\n      theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/morans_i_scatter.png\", p_moran_combined, \n         width = 16, height = 7, dpi = 300, bg = \"white\")\n  cat(\"\\n   plot/morans_i_scatter.png\\n\")\n  \n  # Save Moran‚Äôs I results\n  moran_summary &lt;- data.frame(\n    Variable = c(\"Sale Price\", \"Model 4 Residuals\"),\n    Morans_I = c(moran_price$estimate[1], moran_resid$estimate[1]),\n    Expected = c(moran_price$estimate[2], moran_resid$estimate[2]),\n    Variance = c(moran_price$estimate[3], moran_resid$estimate[3]),\n    Z_score = c(moran_price$statistic, moran_resid$statistic),\n    P_value = c(moran_price$p.value, moran_resid$p.value),\n    Significant = c(moran_price$p.value &lt; 0.05, moran_resid$p.value &lt; 0.05)\n  )\n  \n  write_csv(moran_summary, \"file/morans_i_results.csv\")\n  cat(\"  ‚úì file/morans_i_results.csv\\n\")\n  \n} else {\n  cat(\" Missing coordinate data ‚Äì Moran‚Äôs I test cannot be performed\\n\")\n}\n\nValid samples: 24023\n\n--- Spatial Autocorrelation of Actual Sale Price ---\n\n\n  Moran‚Äôs I = 0.5328\n  Expected = -0.0000\n  Variance = 0.000009\n  Z-score = 175.1095\n  p-value = 0.0000e+00\n  *** Highly significant positive spatial autocorrelation (p &lt; 0.001)\n\n--- Spatial Autocorrelation of Model 4 Residuals ---\n  Moran‚Äôs I = 0.0820\n  Expected = -0.0000\n  Variance = 0.000009\n  Z-score = 26.9827\n  p-value = 1.1797e-160\n  Ô∏è Strong residual spatial autocorrelation detected (p &lt; 0.001)\n  Suggestion: Consider Spatial Lag Model (SAR) or Spatial Error Model (SEM)\n\n\n\n   plot/morans_i_scatter.png\n  ‚úì file/morans_i_results.csv\n\n\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 4 Supplement Completed\\n\")\n\n Step 4 Supplement Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Output Files:\\n\\n\")\n\n Output Files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\nif (exists(\"influential_data\")) {\n  cat(\"  ‚Ä¢ file/influential_observations.csv ‚Äì List of influential observations\\n\")\n}\n\n  ‚Ä¢ file/influential_observations.csv ‚Äì List of influential observations\n\nif (exists(\"moran_summary\")) {\n  cat(\"  ‚Ä¢ file/morans_i_results.csv ‚Äì Moran‚Äôs I test results\\n\")\n}\n\n  ‚Ä¢ file/morans_i_results.csv ‚Äì Moran‚Äôs I test results\n\ncat(\"\\nFigures:\\n\")\n\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/cooks_distance.png ‚Äì Cook‚Äôs Distance Scatter Plot\\n\")\n\n  ‚Ä¢ plot/cooks_distance.png ‚Äì Cook‚Äôs Distance Scatter Plot\n\nif (exists(\"p_moran_combined\")) {\n  cat(\"  ‚Ä¢ plot/morans_i_scatter.png ‚Äì Moran‚Äôs I Scatter Plot\\n\")\n}\n\n  ‚Ä¢ plot/morans_i_scatter.png ‚Äì Moran‚Äôs I Scatter Plot\n\ncat(\"\\n Key Findings:\\n\")\n\n\n Key Findings:\n\nif (exists(\"moran_resid\")) {\n  if (moran_resid$p.value &lt; 0.05) {\n    cat(\"  Ô∏è Significant spatial autocorrelation detected in residuals\\n\")\n    cat(\"     ‚Üí The model did not fully capture spatial effects\\n\")\n    cat(\"     ‚Üí Suggest using spatial econometric models (SAR/SEM)\\n\")\n  } else {\n    cat(\"   No significant spatial autocorrelation in residuals\\n\")\n    cat(\"     ‚Üí Fixed effects adequately controlled for spatial dependence\\n\")\n  }\n}\n\n  Ô∏è Significant spatial autocorrelation detected in residuals\n     ‚Üí The model did not fully capture spatial effects\n     ‚Üí Suggest using spatial econometric models (SAR/SEM)\n\ncat(\"\\n\")\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(patchwork)\n\n# tidytext\nif (!require(\"tidytext\", quietly = TRUE)) {\n  reorder_within &lt;- function(x, by, within, fun = mean, sep = \"___\", ...) {\n    new_x &lt;- paste(x, within, sep = sep)\n    stats::reorder(new_x, by, FUN = fun)\n  }\n  \n  scale_x_reordered &lt;- function(..., sep = \"___\") {\n    reg &lt;- paste0(sep, \".+$\")\n    ggplot2::scale_x_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n  \n  scale_y_reordered &lt;- function(..., sep = \"___\") {\n    reg &lt;- paste0(sep, \".+$\")\n    ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n} else {\n  library(tidytext)\n}\n\n\n# Read data\ndf &lt;- read_csv(\"./table/feature_importance_all_models.csv\", show_col_types = FALSE)\n\n# Data cleaning\ndf &lt;- df %&gt;%\n  mutate(\n    estimate = as.numeric(estimate),\n    abs_estimate = as.numeric(abs_estimate)\n  )\n\n# Variable classification\ndf &lt;- df %&gt;%\n  mutate(\n    var_type = case_when(\n      grepl(\"census_tract|zip_code\", term) ~ \"Location Fixed Effects\",\n      grepl(\"income|POVERTY\", term) ~ \"Socioeconomic\",\n      grepl(\"bathroom|bedroom|livable_area\", term) ~ \"Structural\",\n      grepl(\"age\", term) ~ \"Age\",\n      grepl(\"coord|dist\", term) ~ \"Spatial\",\n      TRUE ~ \"Other\"\n    ),\n    # Clean variable names\n    term_clean = case_when(\n      term == \"number_of_bathrooms\" ~ \"Bathrooms\",\n      term == \"number_of_bedrooms\" ~ \"Bedrooms\",\n      term == \"total_livable_area\" ~ \"Livable Area\",\n      term == \"per_cap_incomeE\" ~ \"Per Capita Income\",\n      term == \"median_incomeE\" ~ \"Median Income\",\n      term == \"PCTPOVERTY\" ~ \"Poverty Rate\",\n      term == \"age2\" ~ \"Age¬≤\",\n      term == \"dist_to_center\" ~ \"Distance to Center\",\n      term == \"x_coord\" ~ \"X Coordinate\",\n      term == \"y_coord\" ~ \"Y Coordinate\",\n      grepl(\"census_tract\", term) ~ gsub(\"census_tract\", \"Census Tract \", term),\n      grepl(\"zip_code\", term) ~ gsub(\"zip_code\", \"ZIP \", term),\n      TRUE ~ term\n    )\n  )\n\n# Keep only top 10 most important variables per model\ntop_features &lt;- df %&gt;%\n  group_by(Model) %&gt;%\n  slice_max(abs_estimate, n = 10) %&gt;%\n  ungroup()\n\n# Color palette (deep blue to deep red)\ncolor_palette &lt;- c(\n  \"Structural\" = \"#67001F\",        # deep red\n  \"Socioeconomic\" = \"#4393C3\",     # medium blue  \n  \"Age\" = \"#2166AC\",               # darker blue\n  \"Spatial\" = \"#053061\",           # darkest blue\n  \"Location Fixed Effects\" = \"#D6604D\"  # reddish tone\n)\n\ncat(\"Generating visualization (using deep blue‚Äìdeep red palette)...\\n\\n\")\n\nGenerating visualization (using deep blue‚Äìdeep red palette)...\n\n\n\n# ========== Figure 1: Lollipop Chart ==========\ncat(\"  Generating Figure 1: Lollipop chart...\\n\")\n\n  Generating Figure 1: Lollipop chart...\n\np1 &lt;- ggplot(top_features, aes(x = reorder_within(term_clean, abs_estimate, Model), \n                                y = abs_estimate, color = var_type)) +\n  geom_segment(aes(xend = reorder_within(term_clean, abs_estimate, Model), yend = 0), \n               size = 1.2, alpha = 0.7) +\n  geom_point(size = 4, alpha = 0.9) +\n  coord_flip() +\n  facet_wrap(~Model, scales = \"free_y\", ncol = 2) +\n  scale_x_reordered() +\n  scale_color_manual(values = color_palette, name = \"Feature Type\") +\n  labs(\n    title = \"Feature Importance Across Progressive Models\",\n    subtitle = \"Lollipop chart showing top 10 features per model\",\n    x = NULL,\n    y = \"Absolute Coefficient\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    strip.background = element_rect(fill = \"gray95\", color = NA),\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray90\", size = 0.3),\n    axis.text.y = element_text(size = 9),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_lollipop_v2.png\", p1, \n       width = 14, height = 10, dpi = 300, bg = \"white\")\n\ncat(\"    ‚úì Saved to: plot/feature_importance_lollipop_v2.png\\n\")\n\n    ‚úì Saved to: plot/feature_importance_lollipop_v2.png\n\n\n\n# ========== Figure 2: Heatmap ==========\ncat(\"  Generating Figure 2: Heatmap...\\n\")\n\n  Generating Figure 2: Heatmap...\n\nheatmap_data &lt;- df %&gt;%\n  group_by(Model) %&gt;%\n  slice_max(abs_estimate, n = 8) %&gt;%\n  ungroup() %&gt;%\n  mutate(Model_short = gsub(\"M[0-9]: \", \"\", Model))\n\np2 &lt;- ggplot(heatmap_data, aes(x = Model_short, y = fct_reorder(term_clean, abs_estimate), \n                                fill = abs_estimate)) +\n  geom_tile(color = \"white\", size = 1) +\n  geom_text(aes(label = sprintf(\"%.3f\", abs_estimate)), \n            color = \"white\", size = 3.5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(heatmap_data$abs_estimate),\n    name = \"Coefficient\\nMagnitude\"\n  ) +\n  labs(\n    title = \"Feature Importance Heatmap Across Models\",\n    subtitle = \"Top 8 features per model (darker = stronger effect)\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\", size = 11),\n    axis.text.y = element_text(size = 10),\n    legend.position = \"right\",\n    panel.grid = element_blank(),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_heatmap_v2.png\", p2, \n       width = 12, height = 8, dpi = 300, bg = \"white\")\n\ncat(\"    ‚úì Saved to: plot/feature_importance_heatmap_v2.png\\n\")\n\n    ‚úì Saved to: plot/feature_importance_heatmap_v2.png\n\n\n\n# ========== Figure 3: Nightingale Rose Chart ==========\ncat(\"  Generating Figure 3: Nightingale rose chart...\\n\")\n\n  Generating Figure 3: Nightingale rose chart...\n\nmodels &lt;- unique(top_features$Model)\n\nrose_plots &lt;- lapply(models, function(m) {\n  data_m &lt;- top_features %&gt;%\n    filter(Model == m) %&gt;%\n    arrange(desc(abs_estimate)) %&gt;%\n    head(8) %&gt;%\n    mutate(term_clean = factor(term_clean, levels = term_clean))\n  \n  ggplot(data_m, aes(x = term_clean, y = abs_estimate, fill = abs_estimate)) +\n    geom_col(width = 1, alpha = 0.9, color = \"white\", size = 0.5) +\n    coord_polar(theta = \"x\") +\n    scale_fill_gradient2(\n      low = \"#4393C3\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(data_m$abs_estimate),\n      name = \"Importance\"\n    ) +\n    labs(title = m, x = NULL, y = NULL) +\n    theme_minimal(base_size = 10) +\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5, size = 11),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 12, face = \"bold\"),\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      legend.position = \"none\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n})\n\np3 &lt;- wrap_plots(rose_plots, ncol = 2) +\n  plot_annotation(\n    title = \"Feature Importance: Nightingale Rose Chart\",\n    subtitle = \"Top 8 features visualized as rose petals (larger petal = more important)\",\n    theme = theme(\n      plot.title = element_text(face = \"bold\", size = 18, hjust = 0.5),\n      plot.subtitle = element_text(size = 13, hjust = 0.5, color = \"gray40\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/feature_importance_rose_v2.png\", p3, \n       width = 14, height = 12, dpi = 300, bg = \"white\")\n\ncat(\"     Saved to: plot/feature_importance_rose_v2.png\\n\")\n\n     Saved to: plot/feature_importance_rose_v2.png\n\n\n\n# Print Summary\ncat(\"\\n==============================================\\n\")\n\n\n==============================================\n\ncat(\"üé® All visualizations have been successfully generated! (Deep Blue‚ÄìRed Palette)\\n\")\n\nüé® All visualizations have been successfully generated! (Deep Blue‚ÄìRed Palette)\n\ncat(\"==============================================\\n\\n\")\n\n==============================================\n\ncat(\"üìä Generated Figures:\\n\")\n\nüìä Generated Figures:\n\ncat(\"  1. feature_importance_lollipop_v2.png  - Lollipop Chart\\n\")\n\n  1. feature_importance_lollipop_v2.png  - Lollipop Chart\n\ncat(\"  2. feature_importance_heatmap_v2.png   - Heatmap\\n\")\n\n  2. feature_importance_heatmap_v2.png   - Heatmap\n\ncat(\"  3. feature_importance_rose_v2.png      - Nightingale Rose Chart\\n\\n\")\n\n  3. feature_importance_rose_v2.png      - Nightingale Rose Chart\n\ncat(\"üé® Color Palette (following Moran's I visualization):\\n\")\n\nüé® Color Palette (following Moran's I visualization):\n\ncat(\"  ‚Ä¢ Structural: Deep Red (#67001F)\\n\")\n\n  ‚Ä¢ Structural: Deep Red (#67001F)\n\ncat(\"  ‚Ä¢ Socioeconomic: Deep Blue (#4393C3)\\n\")\n\n  ‚Ä¢ Socioeconomic: Deep Blue (#4393C3)\n\ncat(\"  ‚Ä¢ Age: Darker Blue (#2166AC)\\n\")\n\n  ‚Ä¢ Age: Darker Blue (#2166AC)\n\ncat(\"  ‚Ä¢ Spatial: Deepest Blue (#053061)\\n\")\n\n  ‚Ä¢ Spatial: Deepest Blue (#053061)\n\ncat(\"  ‚Ä¢ Location FE: Reddish Tone (#D6604D)\\n\\n\")\n\n  ‚Ä¢ Location FE: Reddish Tone (#D6604D)"
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Appendix.html",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Appendix.html",
    "title": "Philadelphia Housing Price Prediction ‚Äì Technical Appendix",
    "section": "",
    "text": "Step0\n\n# =========================================================\n# Step 1: Load libraries and data\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(stringr)\n\nopa_raw &lt;- read_csv(\"data/opa_properties_public.csv\",\n                    na = c(\"\", \"NA\", \"NaN\", \"NULL\"),\n                    guess_max = 1e6)\n\ncat(\"Rows (loaded):\", nrow(opa_raw), \"\\n\")\n\nRows (loaded): 583754 \n\n\n\n# =========================================================\n# Step 2: Data cleaning and filtering\n# =========================================================\nopa_res &lt;- opa_raw %&gt;%\n  mutate(\n    sale_date = as_date(sale_date),\n    # Prices are standardized as numeric values: regardless of whether they were originally numbers or strings (including $ or commas).\n    sale_price_num = suppressWarnings(\n      coalesce(as.numeric(sale_price), readr::parse_number(as.character(sale_price)))\n    ),\n    # Categories are standardized to a consistent data type.\n    cat_chr = as.character(category_code)\n  ) %&gt;%\n  filter(\n    cat_chr %in% c(\"1\"),  \n    sale_date &gt;= as_date(\"2023-01-01\"),\n    sale_date &lt;= as_date(\"2024-12-31\"),\n    sale_price_num &gt;= 10000,\n    total_livable_area &gt; 0,\n    year_built &gt; 0,\n    number_of_bedrooms &gt; 0,\n    number_of_bathrooms &gt; 0,\n    !is.na(census_tract),\n    census_tract != 0,\n    census_tract != \"\",\n    !is.na(zip_code),\n    zip_code != \"\",\n    !is.na(exterior_condition),\n    exterior_condition != \"\",\n    !is.na(interior_condition),\n    interior_condition != \"\",\n    !is.na(shape),\n    shape != \"\"\n  ) %&gt;%\n  select(\n    parcel_number, sale_date,\n    sale_price = sale_price_num,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, shape\n  ) %&gt;%\n  distinct() %&gt;%\n  drop_na()\n\ncat(\"Rows (after cleaning and filtering):\", nrow(opa_res), \"\\n\")\n\nRows (after cleaning and filtering): 24123 \n\n\n\n# =========================================================\n# Step 3: Extract coordinates from shape field (using sf)\n# =========================================================\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(viridis)\n\n# Remove the prefix SRID=2272\nwkt &lt;- sub(\"^SRID=\\\\d+;\\\\s*\", \"\", opa_res$shape)\n\n# Convert to sf format.\ngeom &lt;- st_as_sfc(wkt, crs = 2272)\nopa_sf &lt;- st_sf(opa_res, geometry = geom)\n\n# Extract coordinates (for scatter plot).\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf$X &lt;- coords[, 1]\nopa_sf$Y &lt;- coords[, 2]\n\n# Debug output.\ncat(\"Rows with valid coordinates:\", nrow(opa_sf), \"\\n\")\n\nRows with valid coordinates: 24123 \n\ncat(\"Sample X coordinates:\", head(opa_sf$X, 3), \"\\n\")\n\nSample X coordinates: 2726356 2709355 2718833 \n\ncat(\"Sample Y coordinates:\", head(opa_sf$Y, 3), \"\\n\")\n\nSample Y coordinates: 290862.2 257213.9 269581 \n\n\n\n# =========================================================\n# Step 4: Save cleaned data\n# =========================================================\n# Create output directory\ndir.create(here::here(\"data\"), recursive = TRUE, showWarnings = FALSE)\n\n# Export data (remove the geometry column, keeping only coordinates)\nopa_export &lt;- opa_sf %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract, x_coord = X, y_coord = Y\n  ) %&gt;%\n  st_drop_geometry()  \n\nwrite_csv(opa_export, \"./data/opa_sales_2023_2024_residential_clean.csv\")\n\ncat(\"Cleaned data saved to: opa_sales_2023_2024_residential_clean.csv\\n\")\n\nCleaned data saved to: opa_sales_2023_2024_residential_clean.csv\n\ncat(\"Final dataset contains\", nrow(opa_export), \"rows with\", ncol(opa_export), \"columns\\n\")\n\nFinal dataset contains 24123 rows with 14 columns\n\ncat(\"Columns:\", paste(names(opa_export), collapse = \", \"), \"\\n\")\n\nColumns: parcel_number, sale_date, sale_price, number_of_bedrooms, number_of_bathrooms, total_livable_area, year_built, zip_code, category_code, exterior_condition, interior_condition, census_tract, x_coord, y_coord \n\n\n\n# =========================================================\n# Step 5: Load, clean crime data and calculate crime count for properties\n# =========================================================\nlibrary(readr)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(here)\nlibrary(tidyr)\nlibrary(sf)\n\n# Load crime data for 2023 and 2024\ncrime_2023 &lt;- read_csv(here(\"./data/crime_2023.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\ncrime_2024 &lt;- read_csv(here(\"./data/crime_2024.csv\"), na = c(\"\", \"NA\", \"NaN\", \"NULL\"), guess_max = 1e6)\n\n# Merge, clean, and process crime data\ncrime_clean &lt;- bind_rows(\n  crime_2023 %&gt;% mutate(year = 2023),\n  crime_2024 %&gt;% mutate(year = 2024)\n) %&gt;%\n  mutate(\n    dispatch_date_time = as_datetime(dispatch_date_time),\n    dispatch_date = as_date(dispatch_date),\n    lat = as.numeric(lat),\n    lng = as.numeric(lng),\n    point_x = as.numeric(point_x),\n    point_y = as.numeric(point_y),\n    ucr_general = as.numeric(ucr_general),\n    text_general_code = as.character(text_general_code)\n  ) %&gt;%\n  filter(\n    !is.na(lat) & !is.na(lng),\n    lat != 0 & lng != 0,\n    dispatch_date &gt;= as_date(\"2023-01-01\"),\n    dispatch_date &lt;= as_date(\"2024-12-31\")\n  ) %&gt;%\n  select(\n    objectid, dc_dist, psa, dispatch_date_time, dispatch_date, \n    dispatch_time, hour, dc_key, location_block, \n    ucr_general, text_general_code, \n    lat, lng, point_x, point_y, year\n  ) %&gt;%\n  distinct() %&gt;%\n  drop_na()\n\n# Coordinate transformation: convert from WGS84 (EPSG:4326) to EPSG:2272 coordinate system\ncrime_sf &lt;- crime_clean %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  st_transform(crs = 2272) %&gt;%\n  mutate(\n    x_coord_2272 = st_coordinates(.)[, 1],\n    y_coord_2272 = st_coordinates(.)[, 2]\n  ) %&gt;%\n  st_drop_geometry()\n\n# Read the cleaned real estate data\nopa_clean &lt;- read_csv(here(\"./data/opa_sales_2023_2024_residential_clean.csv\"))\n\n# Convert the real estate data to an sf object (EPSG:2272)\nopa_sf &lt;- opa_clean %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n\n# Convert the crime data to an sf object (EPSG:2272)\ncrime_sf_spatial &lt;- crime_sf %&gt;%\n  st_as_sf(coords = c(\"x_coord_2272\", \"y_coord_2272\"), crs = 2272)\n\n# Create a 0.75-mile buffer around each home and count crimes (15-minute walkshed)\nopa_buffers &lt;- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\ncrime_count &lt;- st_intersects(opa_buffers, crime_sf_spatial)\n\n# Add the crime counts to the real estate dataset\nopa_with_crime &lt;- opa_clean %&gt;%\n  mutate(\n    crime_count_15min_walk = sapply(crime_count, length)\n  )\n\ncat(\"Crime data processing completed:\\n\")\n\nCrime data processing completed:\n\ncat(\"Number of 2023 crime records:\", nrow(crime_2023), \"\\n\")\n\nNumber of 2023 crime records: 169017 \n\ncat(\"Number of 2024 crime records:\", nrow(crime_2024), \"\\n\")\n\nNumber of 2024 crime records: 160388 \n\ncat(\"Number of valid crime records after cleaning:\", nrow(crime_clean), \"\\n\")\n\nNumber of valid crime records after cleaning: 218725 \n\ncat(\"Number of records after coordinate transformation:\", nrow(crime_sf), \"\\n\")\n\nNumber of records after coordinate transformation: 218725 \n\ncat(\"Number of real estate records:\", nrow(opa_with_crime), \"\\n\")\n\nNumber of real estate records: 24123 \n\ncat(\"Average number of crimes within a 15-minute walkshed per property:\", round(mean(opa_with_crime$crime_count_15min_walk), 2), \"\\n\")\n\nAverage number of crimes within a 15-minute walkshed per property: 4466.34 \n\ncat(\"Crime count range:\", min(opa_with_crime$crime_count_15min_walk), \"-\", max(opa_with_crime$crime_count_15min_walk), \"\\n\")\n\nCrime count range: 35 - 13238 \n\n\n\n# =========================================================\n# Step 6: Add park accessibility metrics (distance in feet, keep coordinate columns)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# Use the crime data processing results from the previous step\nopa_with_crime &lt;- opa_with_crime\n\n# Convert to an sf object (coordinate system: EPSG:2272, unit: feet)\nopa_sf &lt;- st_as_sf(\n  opa_with_crime,\n  coords = c(\"x_coord\", \"y_coord\"),\n  crs = 2272\n)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of real estate records read: 24123 \n\n# Read and project the park data.\nparks &lt;- st_read(here(\"./data/PPR_Program_Sites.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(2272)\n\n# Calculate park accessibility metrics (unit: feet)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    dist_to_park_ft = apply(st_distance(opa_sf, parks), 1, min),   \n    # Nearest park distance (feet).\n    park_within_15min_walk = lengths(st_within(opa_sf, st_buffer(parks, 3960)))  # 0.75 mile = 3960 feet\n  )\n\n# Extract coordinates to prevent st_drop_geometry from removing them\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export data (retain coordinates and newly added indicators)\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,              # keep coordinates\n    crime_count_15min_walk,        # number of crimes within 15-minute walkshed\n    dist_to_park_ft,               # nearest park distance (feet)\n    park_within_15min_walk         # number of parks within 15-minute walkshed\n  )\n\n# Output summary information\n# Save the complete dataset with park accessibility indicators\nwrite_csv(opa_export, here(\"./data/opa_sales_with_parks.csv\"))\n\ncat(\" Park accessibility indicators added (unit: feet)\\n\")\n\n Park accessibility indicators added (unit: feet)\n\ncat(\"  Average nearest park distance:\", round(mean(opa_export$dist_to_park_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average nearest park distance: 1768.6 ft\n\ncat(\"  Average number of parks within 15-minute walkshed:\", round(mean(opa_export$park_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of parks within 15-minute walkshed: 3.53 \n\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\n\n  Number of fields: 17 columns (including coordinate columns)\n\ncat(\"üéâ Data saved to: ./data/opa_sales_with_parks.csv\\n\")\n\nüéâ Data saved to: ./data/opa_sales_with_parks.csv\n\n\n\n# =========================================================\n# Step 7: Add public transit accessibility metrics\n# (distance to nearest stop & number of stops within 1,000 ft; also 15-min walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# Use the same CRS (EPSG:2272, units = feet)\nanalysis_crs &lt;- 2272\n\n# Read the dataset saved in the previous step (with price, crime, and park metrics)\nopa_data &lt;- read_csv(here(\"./data/opa_sales_with_parks.csv\"))\nopa_sf &lt;- opa_data %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of real estate records read: 24123 \n\n# Read and project transit stop data\ntransit_path &lt;- here(\"./data/Transit_Stops_(Spring_2025).geojson\")\nstopifnot(file.exists(transit_path))\n\ntransit_stops &lt;- st_read(transit_path, quiet = TRUE) %&gt;%\n  st_transform(analysis_crs) %&gt;%\n  suppressWarnings(st_collection_extract(\"POINT\")) %&gt;%\n  filter(!st_is_empty(geometry)) %&gt;%\n  distinct(geometry, .keep_all = TRUE)\n\ncat(\"Number of transit stops:\", nrow(transit_stops), \"\\n\")\n\nNumber of transit stops: 13839 \n\n# Compute accessibility metrics (units: feet)\n# Distance to nearest transit stop\nnearest_idx &lt;- st_nearest_feature(opa_sf, transit_stops)\ndist_ft &lt;- st_distance(opa_sf, transit_stops[nearest_idx, ], by_element = TRUE)\nopa_sf$dist_transit_ft &lt;- as.numeric(set_units(dist_ft, \"ft\"))\n\n# Count of transit stops within a 15-minute walkshed (0.75 mile = 3960 ft)\nbuffer_3960ft &lt;- st_buffer(opa_sf, dist = 3960)\nopa_sf$transit_15min_walk &lt;- lengths(st_intersects(buffer_3960ft, transit_stops))\n\n# (Optional) Count of transit stops within 1,000 ft\nbuffer_1000ft &lt;- st_buffer(opa_sf, dist = 1000)\nopa_sf$transit_within_1000ft &lt;- lengths(st_intersects(buffer_1000ft, transit_stops))\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and newly added indicators)\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                 # keep coordinates\n    crime_count_15min_walk,           # crime metric\n    dist_to_park_ft, park_within_15min_walk, # park metrics\n    dist_transit_ft, transit_15min_walk,     # transit metrics (walkshed)\n    transit_within_1000ft                    # transit stops within 1,000 ft\n  )\n\n# Output summary information\n# Save the complete dataset with transit metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_transit.csv\"))\n\ncat(\"  Public transit metrics added\\n\")\n\n  Public transit metrics added\n\ncat(\"  Average distance to nearest transit stop:\", round(mean(opa_export$dist_transit_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average distance to nearest transit stop: 428.8 ft\n\ncat(\"  Average number of transit stops within 15-minute walkshed:\", round(mean(opa_export$transit_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of transit stops within 15-minute walkshed: 151.79 \n\ncat(\"  Average number of transit stops within 1,000 ft:\", round(mean(opa_export$transit_within_1000ft, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of transit stops within 1,000 ft: 10.73 \n\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\n\n  Number of fields: 20 columns (including coordinate columns)\n\ncat(\"üéâ Data saved to: ./data/opa_sales_with_transit.csv\\n\")\n\nüéâ Data saved to: ./data/opa_sales_with_transit.csv\n\n\n\n# =========================================================\n# Step 8: Add hospital accessibility metrics\n# (distance to nearest hospital & count within 0.75-mile walkshed)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# CRS: EPSG:2272 (units: feet)\nanalysis_crs &lt;- 2272\n\n# Read the dataset saved in the previous step (with price, crime, park, and transit metrics)\nopa_data &lt;- read_csv(here(\"./data/opa_sales_with_transit.csv\"))\nopa_sf &lt;- opa_data %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = analysis_crs)\n\ncat(\"Number of real estate records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of real estate records read: 24123 \n\n# Read hospital data and project to the same CRS\nhospitals &lt;- st_read(here(\"./data/Hospitals.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(analysis_crs)\n\ncat(\"Number of hospitals:\", nrow(hospitals), \"\\n\")\n\nNumber of hospitals: 36 \n\n# Compute hospital accessibility metrics (units: feet)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    # Distance to nearest hospital (feet)\n    dist_to_hospital_ft = as.numeric(apply(st_distance(opa_sf, hospitals), 1, min)),\n    # Number of hospitals within a 15-minute walkshed (0.75 mile = 3960 ft)\n    hospitals_15min_walk = lengths(st_within(opa_sf, st_buffer(hospitals, 3960)))\n  )\n\n# Extract coordinates so st_drop_geometry does not remove them\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# Export results (retain coordinates and all features)\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,                         # keep coordinates\n    crime_count_15min_walk,                   # crime metrics\n    dist_to_park_ft, park_within_15min_walk,  # park metrics\n    dist_transit_ft, transit_15min_walk,      # transit metrics\n    dist_to_hospital_ft, hospitals_15min_walk # hospital metrics (new)\n  )\n\n# Output summary information\n# Save the complete dataset with hospital metrics\nwrite_csv(opa_export, here(\"./data/opa_sales_with_hospitals.csv\"))\n\ncat(\"  Hospital accessibility metrics added\\n\")\n\n  Hospital accessibility metrics added\n\ncat(\"  Average distance to nearest hospital:\", round(mean(opa_export$dist_to_hospital_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average distance to nearest hospital: 5169 ft\n\ncat(\"  Average number of hospitals within 15-minute walkshed:\", round(mean(opa_export$hospitals_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of hospitals within 15-minute walkshed: 0.74 \n\ncat(\"  Number of fields:\", ncol(opa_export), \"columns (including coordinate columns)\\n\")\n\n  Number of fields: 21 columns (including coordinate columns)\n\ncat(\"  Includes all features: real estate info + crime + park + transit + hospital\\n\")\n\n  Includes all features: real estate info + crime + park + transit + hospital\n\ncat(\"  Data saved to: ./data/opa_sales_with_hospitals.csv\\n\")\n\n  Data saved to: ./data/opa_sales_with_hospitals.csv\n\n\n\n# =========================================================\n# Step 9: Enrich with ACS (Census) Socioeconomic Indicators\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(readr)\nlibrary(here)\n\n# ---------------------------------------------------------\n# Read the final property data (includes all accessibility metrics)\n# ---------------------------------------------------------\nopa_final &lt;- read_csv(here(\"./data/opa_sales_with_hospitals.csv\"))\n\n# Convert to sf object (ensure CRS is EPSG:2272)\nopa_sf &lt;- opa_final %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of property records read: 24123 \n\n# ---------------------------------------------------------\n# Download ACS data for Philadelphia (year can be changed)\n# ---------------------------------------------------------\nyear_acs &lt;- 2022\ncensus_api_key(\"86993dedbe98d77b9d79db6b8ba21a7fde55cb91\", install = FALSE)\n\nacs_vars &lt;- c(\n  total_pop      = \"B01003_001\",\n  median_income  = \"B19013_001\",\n  per_cap_income = \"B19301_001\",\n  below_pov      = \"B17001_002\",\n  edu_total25    = \"B15003_001\",\n  edu_bach       = \"B15003_022\",\n  edu_mast       = \"B15003_023\",\n  edu_prof       = \"B15003_024\",\n  edu_phd        = \"B15003_025\"\n)\n\nphl_acs &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = year_acs,\n  geometry = TRUE,\n  output = \"wide\",\n  variables = acs_vars\n) %&gt;%\n  mutate(\n    PCBACHMORE = 100 * ((edu_bachE + edu_mastE + edu_profE + edu_phdE) / edu_total25E),\n    PCTPOVERTY = 100 * (below_povE / total_popE)\n  ) %&gt;%\n  select(geometry, GEOID, total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# ---------------------------------------------------------\n# Reproject to align (EPSG:2272)\n# ---------------------------------------------------------\nphl_acs &lt;- st_transform(phl_acs, 2272)\n\n# ---------------------------------------------------------\n# Spatial join: assign each home to the tract it falls within\n# ---------------------------------------------------------\nopa_joined &lt;- st_join(\n  opa_sf,\n  phl_acs,\n  join = st_within,\n  left = TRUE\n)\n\n# ---------------------------------------------------------\n# For out-of-bound samples (occasional NAs), fill with nearest tract\n# ---------------------------------------------------------\nmissing &lt;- is.na(opa_joined$median_incomeE)\nif (any(missing)) {\n  idx &lt;- st_nearest_feature(opa_joined[missing, ], phl_acs)\n  repl &lt;- phl_acs[idx, ] %&gt;% st_drop_geometry()\n  cols &lt;- names(repl)\n  opa_joined[missing, cols] &lt;- repl\n}\n\n# ---------------------------------------------------------\n# Export results\n# ---------------------------------------------------------\nopa_export &lt;- opa_joined %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  # crime\n    dist_to_park_ft, park_within_15min_walk, # parks\n    dist_transit_ft, transit_15min_walk,     # transit\n    dist_to_hospital_ft, hospitals_15min_walk, # hospitals\n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY  # ACS\n  )\n\n# Save the complete dataset with Census indicators\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  ACS socioeconomic indicators added\\n\")\n\n  ACS socioeconomic indicators added\n\ncat(\"  Mean household income (USD):\", round(mean(opa_export$median_incomeE, na.rm = TRUE), 0), \"\\n\")\n\n  Mean household income (USD): 66446 \n\ncat(\"  Mean poverty rate (%):\", round(mean(opa_export$PCTPOVERTY, na.rm = TRUE), 2), \"\\n\")\n\n  Mean poverty rate (%): 20.85 \n\ncat(\"  Mean share with bachelor's degree or higher (%):\", round(mean(opa_export$PCBACHMORE, na.rm = TRUE), 2), \"\\n\")\n\n  Mean share with bachelor's degree or higher (%): 34.62 \n\ncat(\"  Includes all features: price + crime + parks + transit + hospitals + socioeconomic\\n\")\n\n  Includes all features: price + crime + parks + transit + hospitals + socioeconomic\n\ncat(\"  Data saved to: opa_sales_final_complete.csv\\n\")\n\n  Data saved to: opa_sales_final_complete.csv\n\n\n\n# =========================================================\n# Step 10: Add Education Accessibility Indicators (Schools)\n# =========================================================\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\nlibrary(units)\n\n# ---------------------------------------------------------\n# 1.Read the complete dataset saved from the previous step (including Census and accessibility features)\n# ---------------------------------------------------------\nopa_data &lt;- read_csv(here(\"opa_sales_final_complete.csv\"))\nopa_sf &lt;- opa_data %&gt;%\n  st_as_sf(coords = c(\"x_coord\", \"y_coord\"), crs = 2272, remove = FALSE)\n\ncat(\"Number of property records read:\", nrow(opa_sf), \"\\n\")\n\nNumber of property records read: 24123 \n\ncat(\"Data column names:\", paste(names(opa_sf), collapse = \", \"), \"\\n\")\n\nData column names: parcel_number, sale_date, sale_price, number_of_bedrooms, number_of_bathrooms, total_livable_area, year_built, zip_code, category_code, exterior_condition, interior_condition, census_tract, x_coord, y_coord, crime_count_15min_walk, dist_to_park_ft, park_within_15min_walk, dist_transit_ft, transit_15min_walk, dist_to_hospital_ft, hospitals_15min_walk, total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY, geometry \n\n# ---------------------------------------------------------\n# 2.Read the school data (only one file)Ôºâ\n# ---------------------------------------------------------\nschools &lt;- st_read(here(\"./data/Schools_Parcels.geojson\"), quiet = TRUE) %&gt;%\n  st_transform(2272) %&gt;%\n  filter(!st_is_empty(geometry))  \n\ncat(\"School data loaded:\", nrow(schools), \"records\\n\")\n\nSchool data loaded: 495 records\n\ncat(\"School data coordinate system:\", st_crs(schools)$input, \"\\n\")\n\nSchool data coordinate system: EPSG:2272 \n\ncat(\"Property data coordinate system:\", st_crs(opa_sf)$input, \"\\n\")\n\nProperty data coordinate system: EPSG:2272 \n\n# ---------------------------------------------------------\n# 3.Calculate education accessibility indicators\n# ---------------------------------------------------------\n# Distance to the nearest school (in feet)\ncat(\"Starting to calculate the nearest school distance...\\n\")\n\nStarting to calculate the nearest school distance...\n\ndist_matrix &lt;- st_distance(opa_sf, schools)\ncat(\"Distance matrix dimensions:\", dim(dist_matrix), \"\\n\")\n\nDistance matrix dimensions: 24123 495 \n\nopa_sf$dist_to_nearest_school_ft &lt;- as.numeric(apply(dist_matrix, 1, min))\ncat(\"Nearest school distance calculation completed, range:\",\n    min(opa_sf$dist_to_nearest_school_ft), \"-\", \n    max(opa_sf$dist_to_nearest_school_ft), \"feet\\n\")\n\nNearest school distance calculation completed, range: 0 - 5286.279 feet\n\n# Number of schools within a 15-minute walking distance\ncat(\"Starting to calculate the number of schools within a 15-minute walking distance...\\n\")\n\nStarting to calculate the number of schools within a 15-minute walking distance...\n\nbuffer_3960ft &lt;- st_buffer(opa_sf, dist = 3960)  # 0.75 mile = 3960 feet\nopa_sf$schools_within_15min_walk &lt;- lengths(st_intersects(buffer_3960ft, schools))\ncat(\"Calculation of schools within a 15-minute walking distance completed, range:\",\n    min(opa_sf$schools_within_15min_walk), \"-\", \n    max(opa_sf$schools_within_15min_walk), \"\\n\")\n\nCalculation of schools within a 15-minute walking distance completed, range: 0 - 28 \n\n# ---------------------------------------------------------\n# 4.Extract coordinates and retain all columns\n# ---------------------------------------------------------\ncoords &lt;- st_coordinates(opa_sf)\nopa_sf &lt;- opa_sf %&gt;%\n  mutate(\n    x_coord = coords[, 1],\n    y_coord = coords[, 2]\n  )\n\n# ---------------------------------------------------------\n# 5.Export the complete table with education accessibility indicators\n# ---------------------------------------------------------\nopa_export &lt;- opa_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(\n    parcel_number, sale_date, sale_price,\n    number_of_bedrooms, number_of_bathrooms,\n    total_livable_area, year_built,\n    zip_code, category_code,\n    exterior_condition, interior_condition,\n    census_tract,\n    x_coord, y_coord,\n    crime_count_15min_walk,                  \n    dist_to_park_ft, park_within_15min_walk, \n    dist_transit_ft, transit_15min_walk,     \n    dist_to_hospital_ft, hospitals_15min_walk, \n    total_popE, median_incomeE, per_cap_incomeE, PCBACHMORE, PCTPOVERTY,  # Census\n    dist_to_nearest_school_ft, schools_within_15min_walk                  \n  )\n\n# ---------------------------------------------------------\n# 6.Clean the data: remove all rows containing empty or NA values\n# ---------------------------------------------------------\nrows_before &lt;- nrow(opa_export)\n\nopa_export &lt;- opa_export %&gt;%\n  mutate(across(everything(), ~ {\n    if (is.character(.x)) {\n      clean_val &lt;- trimws(tolower(as.character(.x)))\n      ifelse(clean_val == \"\" | clean_val == \"na\" | clean_val == \"n/a\" | clean_val == \"null\", \n             NA, .x)\n    } else {\n      .x\n    }\n  })) %&gt;%\n  drop_na()\n\nrows_after &lt;- nrow(opa_export)\nrows_removed &lt;- rows_before - rows_after\n\nwrite_csv(opa_export, here(\"opa_sales_final_complete.csv\"))\n\ncat(\"  Education accessibility indicators have been added\\n\")\n\n  Education accessibility indicators have been added\n\ncat(\"  Average distance to the nearest school:\", \n    round(mean(opa_export$dist_to_nearest_school_ft, na.rm = TRUE), 1), \"ft\\n\")\n\n  Average distance to the nearest school: 949.6 ft\n\ncat(\"  Average number of schools within a 15-minute walking distance:\", \n    round(mean(opa_export$schools_within_15min_walk, na.rm = TRUE), 2), \"\\n\")\n\n  Average number of schools within a 15-minute walking distance: 10.56 \n\ncat(\"  Data cleaning completed: removed\", rows_removed, \"rows containing empty or NA values\\n\")\n\n  Data cleaning completed: removed 100 rows containing empty or NA values\n\ncat(\"  Before cleaning:\", rows_before, \"rows ‚Üí After cleaning:\", rows_after, \"rows\\n\")\n\n  Before cleaning: 24123 rows ‚Üí After cleaning: 24023 rows\n\ncat(\"  Final complete dataset saved to: opa_sales_final_complete.csv\\n\")\n\n  Final complete dataset saved to: opa_sales_final_complete.csv\n\ncat(\"  Includes all features: housing price + crime + parks + transit + hospitals + Census + schools\\n\")\n\n  Includes all features: housing price + crime + parks + transit + hospitals + Census + schools\n\ncat(\"  Number of columns:\", ncol(opa_export), \"(including coordinate columns)\\n\")\n\n  Number of columns: 28 (including coordinate columns)\n\n\nStep1\n\n# =========================================================\n# Step 1: Skewness Detection + Log Transformation + Descriptive Statistics\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(e1071)\nlibrary(patchwork)\nlibrary(readr)\nlibrary(tidyr)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"üìä Step 1: Data Cleaning, Transformation, and Descriptive Statistics\\n\")\n\nüìä Step 1: Data Cleaning, Transformation, and Descriptive Statistics\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n# === Read Data ===\ndf &lt;- read_csv(\"opa_sales_final_complete.csv\", show_col_types = FALSE)\ncat(sprintf(\"Original sample size: %d\\n\", nrow(df)))\n\nOriginal sample size: 24023\n\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\nNumber of variables: 28\n\n# === Basic Processing ===\nif (\"year_built\" %in% names(df)) {\n  df &lt;- df %&gt;%\n    mutate(age = 2025 - year_built,\n           age2 = age^2) %&gt;%\n    dplyr::select(-year_built)\n}\n\ncat_vars &lt;- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncoord_vars &lt;- c(\"x_coord\", \"y_coord\")\ndf[cat_vars] &lt;- lapply(df[cat_vars], factor)\n\n# === Skewness Detection ===\ncat(\"=== Skewness Detection ===\\n\")\n\n=== Skewness Detection ===\n\nnum_vars &lt;- df %&gt;% dplyr::select(where(is.numeric))\nskews &lt;- sapply(num_vars, function(x) {\n  if (all(is.na(x)) || sd(x, na.rm = TRUE) == 0) return(0)\n  skewness(x, na.rm = TRUE)\n})\nlogged_vars &lt;- names(skews[abs(skews) &gt; 1])\ncat(sprintf(\"Variables with |skew| &gt; 1: %d\\n\", length(logged_vars)))\n\nVariables with |skew| &gt; 1: 10\n\n# === Apply log1p Transformation ===\ndf_trans &lt;- df\nfor (v in logged_vars) {\n  df_trans[[v]] &lt;- log1p(pmax(df[[v]], 0))\n}\n\n\n# =========================================================\n# 1. Descriptive Statistics Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating Descriptive Statistics Table ===\\n\\n\")\n\n=== Generating Descriptive Statistics Table ===\n\n# Identify target variable\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df_trans) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df_trans) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df_trans) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n# Select key numeric variables\nkey_vars &lt;- c(\n  y_var,\n  \"total_livable_area\", \"number_of_bedrooms\", \"number_of_bathrooms\", \"age\",\n  \"median_incomeE\", \"per_cap_incomeE\", \"PCTPOVERTY\", \"PCBACHMORE\"\n)\nkey_vars &lt;- intersect(key_vars, names(df_trans))\n\nif (length(key_vars) &gt; 0) {\n  # Compute descriptive statistics (more meaningful on pre-transformation data)\n  desc_stats &lt;- df %&gt;%\n    dplyr::select(all_of(key_vars)) %&gt;%\n    summarise(across(everything(), list(\n      N = ~sum(!is.na(.)),\n      Mean = ~mean(., na.rm = TRUE),\n      SD = ~sd(., na.rm = TRUE),\n      Min = ~min(., na.rm = TRUE),\n      Q25 = ~quantile(., 0.25, na.rm = TRUE),\n      Median = ~median(., na.rm = TRUE),\n      Q75 = ~quantile(., 0.75, na.rm = TRUE),\n      Max = ~max(., na.rm = TRUE)\n    ), .names = \"{.col}_{.fn}\")) %&gt;%\n    pivot_longer(everything(), names_to = \"stat\", values_to = \"value\") %&gt;%\n    separate(stat, into = c(\"Variable\", \"Statistic\"), sep = \"_(?=[^_]+$)\") %&gt;%\n    pivot_wider(names_from = Statistic, values_from = value) %&gt;%\n    dplyr::select(Variable, N, Mean, SD, Min, Q25, Median, Q75, Max) %&gt;%\n    mutate(across(c(Mean, SD, Min, Q25, Median, Q75, Max), ~round(., 3)))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\")\n  write_csv(desc_stats, \"file/descriptive_statistics.csv\")\n  cat(\"  ‚úì file/descriptive_statistics.csv\\n\")\n  print(as.data.frame(desc_stats), row.names = FALSE)\n}\n\n  ‚úì file/descriptive_statistics.csv\n            Variable     N       Mean         SD       Min        Q25\n          sale_price 24023 340590.503 466810.992 10000.000 151970.000\n  total_livable_area 24023   1359.120    576.839   308.000   1050.000\n  number_of_bedrooms 24023      2.972      0.789     1.000      3.000\n number_of_bathrooms 24023      1.444      0.686     1.000      1.000\n                 age 24023     85.799     32.549     0.000     72.000\n      median_incomeE 24023  66445.992  31346.114 14983.000  41325.000\n     per_cap_incomeE 24023  39700.301  25344.230  7802.000  21372.000\n          PCTPOVERTY 24023     20.826     13.396     0.719     10.596\n          PCBACHMORE 24023     34.706     24.837     0.530     15.362\n     Median        Q75          Max\n 248000.000 370000.000 15428633.000\n   1208.000   1492.000    14150.000\n      3.000      3.000       12.000\n      1.000      2.000       12.000\n    100.000    105.000      275.000\n  59837.000  86989.000   181066.000\n  31406.000  48558.000   173777.000\n     17.939     29.449       77.833\n     27.435     53.441       95.598\n\n\n\n# =========================================================\n# 2. Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating distribution comparison plots ===\\n\\n\")\n\n=== Generating distribution comparison plots ===\n\nplot_list &lt;- lapply(names(num_vars), function(v) {\n  is_transformed &lt;- v %in% logged_vars\n  \n  # Original distribution\n  p1 &lt;- ggplot(df, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = \"#053061\", color = \"white\", alpha = 0.85) +\n    labs(title = paste0(v, \" (Original)\"), x = NULL, y = \"Count\") +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Transformed distribution\n  fill_color &lt;- if (is_transformed) \"#67001F\" else \"#4393C3\"\n  \n  p2 &lt;- ggplot(df_trans, aes(x = !!sym(v))) +\n    geom_histogram(bins = 30, fill = fill_color, color = \"white\", alpha = 0.85) +\n    labs(\n      title = paste0(v, if (is_transformed) \" (Log-transformed)\" else \" (No transformation)\"),\n      x = NULL, y = \"Count\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 9),\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.3),\n      axis.text = element_text(size = 7),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n  \n  # Add skewness annotation\n  skew_val &lt;- round(skews[v], 2)\n  p1 &lt;- p1 + annotate(\"text\", x = Inf, y = Inf, \n                      label = paste0(\"Skewness: \", skew_val),\n                      hjust = 1.1, vjust = 1.5, size = 2.5, color = \"gray30\")\n  \n  pair_plot &lt;- (p1 | p2) + plot_layout(widths = c(1, 1)) &\n    theme(plot.margin = margin(3, 3, 3, 3))\n  \n  wrap_elements(pair_plot) + \n    theme(\n      plot.background = element_rect(fill = \"white\", color = \"#B0B0B0\", linewidth = 1.2),\n      plot.margin = margin(6, 6, 6, 6)\n    )\n})\n\nall_plot &lt;- wrap_plots(plot_list, ncol = 2) + \n  plot_annotation(\n    title = \"Variable Distributions: Original vs. Transformed\",\n    subtitle = paste0(\n      \"Dark Blue = Original | Dark Red = Log-transformed (|skew| &gt; 1) | Medium Blue = No transformation\"\n    ),\n    theme = theme(\n      plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray30\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\n\n# Skewness Summary Plot\nskew_df &lt;- data.frame(\n  variable = names(skews),\n  skewness = skews,\n  transformed = names(skews) %in% logged_vars\n) %&gt;% arrange(desc(abs(skewness)))\n\np_skew &lt;- ggplot(skew_df, aes(x = reorder(variable, abs(skewness)), y = skewness, fill = transformed)) +\n  geom_col(alpha = 0.85) +\n  geom_hline(yintercept = c(-1, 1), linetype = \"dashed\", color = \"#67001F\", linewidth = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  scale_fill_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"No transformation\", \"TRUE\" = \"Log-transformed\"),\n    name = NULL\n  ) +\n  coord_flip() +\n  labs(\n    title = \"Skewness of All Variables\",\n    subtitle = \"Variables with |skewness| &gt; 1 are log-transformed\",\n    x = \"Variable\", y = \"Skewness\",\n    caption = \"Dashed lines indicate skewness thresholds at ¬±1\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\n\n# =========================================================\n# 3. Categorical Variable Distribution Plots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating categorical variable distribution plots ===\\n\\n\")\n\n=== Generating categorical variable distribution plots ===\n\ncat_vars_viz &lt;- c(\"interior_condition\", \"exterior_condition\")\ncat_vars_viz &lt;- intersect(cat_vars_viz, names(df_trans))\n\nif (length(cat_vars_viz) &gt; 0 && !is.na(y_var)) {\n  plot_list_cat &lt;- list()\n  \n  for (var in cat_vars_viz) {\n    cat_summary &lt;- df_trans %&gt;%\n      group_by(!!sym(var)) %&gt;%\n      summarise(\n        count = n(),\n        mean_price = mean(.data[[y_var]], na.rm = TRUE),\n        .groups = \"drop\"\n      ) %&gt;%\n      filter(!is.na(!!sym(var))) %&gt;%\n      arrange(desc(mean_price))\n    \n    if (nrow(cat_summary) &gt; 0) {\n      p &lt;- ggplot(cat_summary, aes(reorder(!!sym(var), mean_price), mean_price, fill = mean_price)) +\n        geom_col(alpha = 0.9) +\n        geom_text(aes(label = count), vjust = -0.5, size = 3) +\n        scale_fill_gradient2(\n          low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n          midpoint = median(cat_summary$mean_price, na.rm = TRUE),\n          guide = \"none\"\n        ) +\n        coord_flip() +\n        theme_minimal(base_size = 10) +\n        theme(\n          panel.grid.major.y = element_blank(),\n          plot.title = element_text(hjust = 0.5, face = \"bold\", size = 12),\n          plot.background = element_rect(fill = \"white\", color = NA),\n          panel.background = element_rect(fill = \"white\", color = NA)\n        ) +\n        labs(\n          title = gsub(\"_\", \" \", toupper(var)),\n          x = NULL,\n          y = \"Average Sale Price (log)\",\n          caption = \"Numbers indicate the count in each category\"\n        )\n      \n      plot_list_cat[[var]] &lt;- p\n    }\n  }\n  \n  if (length(plot_list_cat) &gt; 0) {\n    p_cat_combined &lt;- wrap_plots(plot_list_cat, ncol = 2) +\n      plot_annotation(\n        title = \"Average Sale Price by Property Condition\",\n        subtitle = \"Log-transformed sale prices\",\n        theme = theme(\n          plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n          plot.subtitle = element_text(size = 11, hjust = 0.5),\n          plot.background = element_rect(fill = \"white\", color = NA)\n        )\n      )\n    \n    if (!dir.exists(\"plot\")) dir.create(\"plot\")\n    ggsave(\"plot/categorical_price_comparison.png\", p_cat_combined,\n           width = 12, height = 6, dpi = 300, bg = \"white\")\n    cat(\"  ‚úì plot/categorical_price_comparison.png\\n\")\n  }\n}\n\n  ‚úì plot/categorical_price_comparison.png\n\n\n\n# =========================================================\n# Output Files\n# =========================================================\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nif (!dir.exists(\"file\")) dir.create(\"file\")\n\nggsave(\"plot/all_variables_distribution.png\", plot = all_plot, width = 20, height = 12, dpi = 300, bg = \"white\")\nggsave(\"plot/skewness_summary.png\", plot = p_skew, width = 10, height = 8, dpi = 300, bg = \"white\")\nwriteLines(logged_vars, \"file/logged_variables.txt\")\nwrite_csv(df_trans, \"file/opa_sales_step1_clean.csv\")\n\ntransform_summary &lt;- data.frame(\n  Variable = names(skews),\n  Original_Skewness = round(skews, 3),\n  Transformed = names(skews) %in% logged_vars\n) %&gt;% arrange(desc(abs(Original_Skewness)))\n\nwrite_csv(transform_summary, \"file/transformation_summary.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 1 Completed\\n\")\n\n Step 1 Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Output Files:\\n\\n\")\n\n Output Files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\ncat(\"  ‚Ä¢ file/descriptive_statistics.csv - Descriptive statistics\\n\")\n\n  ‚Ä¢ file/descriptive_statistics.csv - Descriptive statistics\n\ncat(\"  ‚Ä¢ file/transformation_summary.csv - Transformation summary\\n\")\n\n  ‚Ä¢ file/transformation_summary.csv - Transformation summary\n\ncat(\"  ‚Ä¢ file/logged_variables.txt - List of log-transformed variables\\n\")\n\n  ‚Ä¢ file/logged_variables.txt - List of log-transformed variables\n\ncat(\"  ‚Ä¢ file/opa_sales_step1_clean.csv - Cleaned dataset\\n\\n\")\n\n  ‚Ä¢ file/opa_sales_step1_clean.csv - Cleaned dataset\n\ncat(\"Figures:\\n\")\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/all_variables_distribution.png - All variables: original vs. transformed\\n\")\n\n  ‚Ä¢ plot/all_variables_distribution.png - All variables: original vs. transformed\n\ncat(\"  ‚Ä¢ plot/skewness_summary.png - Skewness summary\\n\")\n\n  ‚Ä¢ plot/skewness_summary.png - Skewness summary\n\ncat(\"  ‚Ä¢ plot/categorical_price_comparison.png - Categorical variables vs. sale price\\n\\n\")\n\n  ‚Ä¢ plot/categorical_price_comparison.png - Categorical variables vs. sale price\n\ncat(\"üìà Stats:\\n\")\n\nüìà Stats:\n\ncat(sprintf(\"  ‚Ä¢ Variables with |skew| &gt; 1: %d\\n\", length(logged_vars)))\n\n  ‚Ä¢ Variables with |skew| &gt; 1: 10\n\ncat(sprintf(\"  ‚Ä¢ Final number of variables: %d\\n\", ncol(df_trans)))\n\n  ‚Ä¢ Final number of variables: 29\n\ncat(sprintf(\"  ‚Ä¢ Final sample size: %d\\n\\n\", nrow(df_trans)))\n\n  ‚Ä¢ Final sample size: 24023\n\n\nStep2\n\n# =========================================================\n# Step 2 Enhanced: Correlation Matrix + VIF + Spatial Visualization + Scatterplots\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(car)\nlibrary(reshape2)\nlibrary(readr)\nlibrary(patchwork)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"  Step 2: Correlation Analysis, Multicollinearity, and Spatial Exploration\\n\")\n\n  Step 2: Correlation Analysis, Multicollinearity, and Spatial Exploration\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n# === Read Data ===\ndf &lt;- read_csv(\"file/opa_sales_step1_clean.csv\", show_col_types = FALSE)\n\ncat(sprintf(\"Sample size: %d\\n\", nrow(df)))\n\nSample size: 24023\n\ncat(sprintf(\"Number of variables: %d\\n\\n\", ncol(df)))\n\nNumber of variables: 29\n\n# === Auto-detect target variable ===\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat_vars &lt;- intersect(c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\"), names(df))\ncoord_vars &lt;- intersect(c(\"x_coord\", \"y_coord\"), names(df))\ndf[cat_vars] &lt;- lapply(df[cat_vars], factor)\n\n\n# =========================================================\n# 1. Correlation Matrix\n# =========================================================\n\ncat(\"=== Generating correlation matrix ===\\n\")\n\n=== Generating correlation matrix ===\n\nnum_df &lt;- df %&gt;% dplyr::select(where(is.numeric))\nnum_df &lt;- num_df[, sapply(num_df, function(x) sd(x, na.rm = TRUE) &gt; 0), drop = FALSE]\n\nif (ncol(num_df) &gt; 1) {\n  corr_mat &lt;- cor(num_df, use = \"pairwise.complete.obs\")\n  corr_mat[upper.tri(corr_mat)] &lt;- NA\n  corr_melt &lt;- melt(corr_mat, na.rm = TRUE)\n  \n  # If there are too many variables, keep only the first 50\n  if (ncol(num_df) &gt; 50) {\n    vars_top &lt;- names(num_df)[1:50]\n    corr_melt &lt;- corr_melt %&gt;% filter(Var1 %in% vars_top & Var2 %in% vars_top)\n  }\n  \n  p_corr &lt;- ggplot(corr_melt, aes(Var2, Var1, fill = value, size = abs(value))) +\n    geom_point(shape = 21, color = \"white\", stroke = 0.5) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0, limits = c(-1, 1), name = NULL,\n      breaks = seq(-1, 1, 0.2)\n    ) +\n    scale_size_continuous(range = c(0.5, 10), guide = \"none\") +\n    scale_x_discrete(position = \"top\") +\n    scale_y_discrete(limits = rev) +\n    theme_minimal(base_size = 11) +\n    theme(\n      axis.text.x.top = element_text(angle = 45, hjust = 0, vjust = 0, size = 9, color = \"black\"),\n      axis.text.y = element_text(size = 9, color = \"black\"),\n      axis.title = element_blank(),\n      panel.grid.major = element_line(color = \"#E0E0E0\", linewidth = 0.5),\n      panel.grid.minor = element_blank(),\n      panel.background = element_rect(fill = \"white\", color = NA),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"bottom\",\n      legend.direction = \"horizontal\",\n      legend.key.width = unit(3, \"cm\"),\n      legend.key.height = unit(0.4, \"cm\"),\n      plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n      plot.margin = margin(10, 10, 10, 10)\n    ) +\n    labs(title = \"Correlation Matrix\") +\n    coord_fixed()\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n  ggsave(\"plot/corr_matrix_enhanced.png\", p_corr, width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/corr_matrix_enhanced.png\\n\")\n}\n\n   plot/corr_matrix_enhanced.png\n\n\n\n# =========================================================\n# 2. VIF Analysis\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== VIF Analysis ===\\n\\n\")\n\n=== VIF Analysis ===\n\nvif_vars &lt;- setdiff(names(df), c(y_var, cat_vars, coord_vars))\nnum_vif &lt;- df %&gt;% dplyr::select(all_of(vif_vars)) %&gt;% dplyr::select(where(is.numeric))\nnum_vif &lt;- num_vif[, sapply(num_vif, function(x) sd(x, na.rm = TRUE) &gt; 0), drop = FALSE]\n\nif (ncol(num_vif) &gt;= 2) {\n  df_vif &lt;- df %&gt;% dplyr::select(all_of(c(y_var, names(num_vif)))) %&gt;% na.omit()\n  f_vif &lt;- as.formula(paste(y_var, \"~ .\"))\n  vif_model &lt;- lm(f_vif, data = df_vif)\n  vif_vals &lt;- car::vif(vif_model)\n  vif_tbl &lt;- data.frame(variable = names(vif_vals), VIF = round(vif_vals, 3))\n  vif_tbl &lt;- vif_tbl %&gt;% arrange(desc(VIF))\n  \n  if (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\n  write_csv(vif_tbl, \"file/vif_values.csv\")\n  cat(\"  ‚úì file/vif_values.csv\\n\")\n  \n  # High VIF warning\n  high_vif &lt;- vif_tbl %&gt;% filter(VIF &gt; 10)\n  if (nrow(high_vif) &gt; 0) {\n    cat(\"\\n  Variables with high multicollinearity (VIF &gt; 10):\\n\")\n    print(as.data.frame(high_vif), row.names = FALSE)\n  }\n  \n  # VIF visualization\n  vif_top20 &lt;- vif_tbl %&gt;% top_n(20, VIF)\n  \n  p_vif &lt;- ggplot(vif_top20, aes(x = reorder(variable, VIF), y = VIF, fill = VIF)) +\n    geom_col(alpha = 0.9) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 5, name = \"VIF Value\"\n    ) +\n    geom_hline(yintercept = 10, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n    geom_hline(yintercept = 5, linetype = \"dashed\", color = \"gray50\", linewidth = 0.7) +\n    coord_flip() +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.major.y = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Variance Inflation Factor (VIF) - Top 20\",\n      subtitle = \"Color gradient: Blue (low VIF) ‚Üí Red (high VIF)\",\n      x = \"Variable\", y = \"VIF Value\",\n      caption = \"Dark red line: VIF=10 (High multicollinearity) | Gray line: VIF=5 (Moderate)\"\n    )\n  \n  ggsave(\"plot/vif_analysis.png\", p_vif, width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/vif_analysis.png\\n\")\n}\n\n  ‚úì file/vif_values.csv\n\n  Variables with high multicollinearity (VIF &gt; 10):\n        variable    VIF\n per_cap_incomeE 10.608\n\n\n   plot/vif_analysis.png\n\n\n\n# =========================================================\n# 3. Spatial Visualization\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Spatial Distribution Visualization ===\\n\\n\")\n  \n  # 3.1 Spatial distribution of sale prices\n  p_spatial_price &lt;- ggplot(df, aes(x_coord, y_coord, color = .data[[y_var]])) +\n    geom_point(alpha = 0.6, size = 0.8) +\n    scale_color_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(df[[y_var]], na.rm = TRUE),\n      name = \"Sale Price\\n(log)\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA),\n      legend.position = \"right\"\n    ) +\n    labs(\n      title = \"Spatial Distribution of Sale Prices\",\n      subtitle = \"Philadelphia housing market\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_price_distribution.png\", p_spatial_price,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_price_distribution.png\\n\")\n  \n  # 3.2 Hexbin density map\n  p_hex &lt;- ggplot(df, aes(x_coord, y_coord)) +\n    geom_hex(aes(fill = after_stat(count)), bins = 40) +\n    scale_fill_gradient(\n      low = \"#F7F7F7\", high = \"#67001F\",\n      name = \"Count\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Housing Density Heatmap\",\n      subtitle = \"Hexagonal binning of property locations\",\n      x = \"X Coordinate\", y = \"Y Coordinate\"\n    )\n  \n  ggsave(\"plot/spatial_density_hexbin.png\", p_hex,\n         width = 10, height = 8, dpi = 300, bg = \"white\")\n  cat(\"   plot/spatial_density_hexbin.png\\n\")\n}\n\n\n================================================================================\n=== Spatial Distribution Visualization ===\n\n\n   plot/spatial_price_distribution.png\n\n\n   plot/spatial_density_hexbin.png\n\n\n\n# =========================================================\n# 4. Key Variable Scatterplots\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Scatterplots: Key Variables vs. Sale Price ===\\n\\n\")\n\n=== Scatterplots: Key Variables vs. Sale Price ===\n\nkey_numeric &lt;- c(\"total_livable_area\", \"median_incomeE\", \"age\", \"number_of_bathrooms\")\nkey_numeric &lt;- intersect(key_numeric, names(df))\n\nif (length(key_numeric) &gt;= 4) {\n  scatter_plots &lt;- list()\n  \n  for (var in key_numeric[1:4]) {\n    # Correlation coefficient\n    corr_val &lt;- cor(df[[var]], df[[y_var]], use = \"pairwise.complete.obs\")\n    \n    p &lt;- ggplot(df, aes(.data[[var]], .data[[y_var]])) +\n      geom_point(alpha = 0.3, size = 0.8, color = \"#053061\") +\n      geom_smooth(method = \"lm\", se = TRUE, color = \"#67001F\", linewidth = 1.2) +\n      annotate(\"text\", x = Inf, y = -Inf, \n               label = sprintf(\"r = %.3f\", corr_val),\n               hjust = 1.1, vjust = -0.5, size = 4, color = \"#67001F\", fontface = \"bold\") +\n      theme_minimal(base_size = 10) +\n      theme(\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 11),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      ) +\n      labs(\n        title = gsub(\"_\", \" \", var),\n        x = gsub(\"_\", \" \", var),\n        y = \"Sale Price (log)\"\n      )\n    \n    scatter_plots[[var]] &lt;- p\n  }\n  \n  p_scatter &lt;- wrap_plots(scatter_plots, ncol = 2) +\n    plot_annotation(\n      title = \"Key Variables vs. Sale Price\",\n      subtitle = \"Linear fit (OLS) with correlation coefficients\",\n      theme = theme(\n        plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 11, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/key_variables_scatter.png\", p_scatter,\n         width = 12, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/key_variables_scatter.png\\n\")\n}\n\n   plot/key_variables_scatter.png\n\n\n\n# =========================================================\n# Save cleaned data\n# =========================================================\n\nwrite_csv(df, \"file/opa_sales_step2_clean.csv\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"  Step 2 Completed\\n\")\n\n  Step 2 Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\"  Output files:\\n\\n\")\n\n  Output files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\ncat(\"  ‚Ä¢ file/vif_values.csv - VIF analysis results\\n\")\n\n  ‚Ä¢ file/vif_values.csv - VIF analysis results\n\ncat(\"  ‚Ä¢ file/opa_sales_step2_clean.csv - Cleaned dataset\\n\\n\")\n\n  ‚Ä¢ file/opa_sales_step2_clean.csv - Cleaned dataset\n\ncat(\"Figures:\\n\")\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/corr_matrix_enhanced.png - Correlation matrix\\n\")\n\n  ‚Ä¢ plot/corr_matrix_enhanced.png - Correlation matrix\n\ncat(\"  ‚Ä¢ plot/vif_analysis.png - VIF analysis\\n\")\n\n  ‚Ä¢ plot/vif_analysis.png - VIF analysis\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  ‚Ä¢ plot/spatial_price_distribution.png - Spatial distribution of sale prices\\n\")\n  cat(\"  ‚Ä¢ plot/spatial_density_hexbin.png - Housing density heatmap\\n\")\n}\n\n  ‚Ä¢ plot/spatial_price_distribution.png - Spatial distribution of sale prices\n  ‚Ä¢ plot/spatial_density_hexbin.png - Housing density heatmap\n\nif (length(key_numeric) &gt;= 4) {\n  cat(\"  ‚Ä¢ plot/key_variables_scatter.png - Key variables vs. sale price (scatterplots)\\n\")\n}\n\n  ‚Ä¢ plot/key_variables_scatter.png - Key variables vs. sale price (scatterplots)\n\ncat(\"\\n\")\n\nStep3\n\n# =========================================================\n# Step 3: VIF-based Filtering + LASSO Feature Selection (Streamlined)\n# =========================================================\n\nlibrary(glmnet)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\nset.seed(2025)\n\n# === Read data and VIF results ===\ndf &lt;- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nvif_path &lt;- \"file/vif_values.csv\"\nif (!file.exists(vif_path)) stop(\"  file/vif_values.csv not found. Please run step2.R first.\")\nvif_table &lt;- read_csv(vif_path, show_col_types = FALSE)\n\n# === Determine target variable ===\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\"  Target variable sale_price_log or sale_price not found\")\n\n# === Variable filtering (VIF) ===\nvif_threshold &lt;- 5\nvars_keep &lt;- vif_table %&gt;%\n  filter(VIF &lt;= vif_threshold) %&gt;%\n  pull(variable)\nvars_keep &lt;- intersect(vars_keep, names(df))\n\ncat(\"  Number of variables passing VIF ‚â§\", vif_threshold, \":\", length(vars_keep), \"\\n\")\n\n  Number of variables passing VIF ‚â§ 5 : 13 \n\n\n\n# === Prepare data matrix ===\ndf_lasso &lt;- df %&gt;%\n  dplyr::select(all_of(c(y_var, vars_keep))) %&gt;%\n  na.omit()\n\nx &lt;- as.matrix(df_lasso %&gt;% dplyr::select(-all_of(y_var)))\ny &lt;- df_lasso[[y_var]]\n\n# === Run LASSO regression ===\ncvfit &lt;- cv.glmnet(\n  x, y,\n  alpha = 1,            # LASSO\n  nfolds = 10,\n  standardize = TRUE\n)\n\nlambda_min &lt;- cvfit$lambda.min\nlambda_1se &lt;- cvfit$lambda.1se\ncat(\"Œª_min =\", signif(lambda_min, 5), \"\\n\")\n\nŒª_min = 0.00085507 \n\ncat(\"Œª_1se =\", signif(lambda_1se, 5), \"\\n\")\n\nŒª_1se = 0.026727 \n\n# === Extract non-zero coefficients ===\ncoef_min &lt;- coef(cvfit, s = \"lambda.min\")\nselected &lt;- rownames(coef_min)[coef_min[, 1] != 0]\nselected &lt;- selected[selected != \"(Intercept)\"]\n\nselected_tbl &lt;- data.frame(\n  variable = selected,\n  coefficient = as.numeric(coef_min[selected, 1])\n)\n\n# === Remove variables with near-zero coefficients ===\nselected_tbl &lt;- selected_tbl %&gt;%\n  filter(abs(coefficient) &gt;= 1e-5) %&gt;%\n  arrange(desc(abs(coefficient)))\n\ncat(\"  Number of variables retained after LASSO:\", nrow(selected_tbl), \"\\n\")\n\n  Number of variables retained after LASSO: 12 \n\n\n\n# === Output files ===\nif (!dir.exists(\"file\")) dir.create(\"file\", recursive = TRUE)\nif (!dir.exists(\"plot\")) dir.create(\"plot\", recursive = TRUE)\n\nwrite_csv(selected_tbl, \"file/lasso_selected_variables.csv\")\n\n\n# === Visualization 1: LASSO Coefficient Importance (Top 20) ===\ntop_n_vars &lt;- min(20, nrow(selected_tbl))\nselected_top &lt;- selected_tbl %&gt;% \n  top_n(top_n_vars, abs(coefficient)) %&gt;%\n  arrange(coefficient)\n\np_coef &lt;- ggplot(selected_top, aes(x = reorder(variable, coefficient), y = coefficient, fill = coefficient)) +\n  geom_col(alpha = 0.9) +\n  scale_fill_gradient2(\n    low = \"#053061\",      \n    mid = \"#F7F7F7\",      \n    high = \"#67001F\",     \n    midpoint = 0,\n    name = \"Coefficient\"\n  ) +\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"gray50\", linewidth = 0.5) +\n  coord_flip() +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.major.x = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Selected Variables - Coefficient Importance\",\n    subtitle = paste0(\"Top \", top_n_vars, \" variables by absolute coefficient value\"),\n    x = \"Variable\",\n    y = \"Coefficient\",\n    caption = paste0(\"Œª_min = \", signif(lambda_min, 4), \" | Total selected: \", nrow(selected_tbl), \" variables\")\n  )\n\nggsave(\"plot/lasso_coefficients.png\", p_coef, width = 10, height = 8, dpi = 300, bg = \"white\")\n\n\n# === Visualization 2: Cross-Validation Curve (Œª Selection) ===\ncv_df &lt;- data.frame(\n  lambda = cvfit$lambda,\n  cvm = cvfit$cvm,\n  cvsd = cvfit$cvsd,\n  cvlo = cvfit$cvm - cvfit$cvsd,\n  cvup = cvfit$cvm + cvfit$cvsd\n)\n\np_cv &lt;- ggplot(cv_df, aes(x = log(lambda), y = cvm)) +\n  geom_ribbon(aes(ymin = cvlo, ymax = cvup), fill = \"#4393C3\", alpha = 0.3) +\n  geom_line(color = \"#053061\", linewidth = 1) +\n  geom_point(color = \"#053061\", size = 2, alpha = 0.6) +\n  geom_vline(xintercept = log(lambda_min), linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  geom_vline(xintercept = log(lambda_1se), linetype = \"dashed\", color = \"#D73027\", linewidth = 0.7) +\n  annotate(\"text\", x = log(lambda_min), y = max(cv_df$cvm) * 0.95, \n           label = paste0(\"Œª_min = \", signif(lambda_min, 3)), \n           hjust = -0.1, size = 3.5, color = \"#67001F\") +\n  annotate(\"text\", x = log(lambda_1se), y = max(cv_df$cvm) * 0.90, \n           label = paste0(\"Œª_1se = \", signif(lambda_1se, 3)), \n           hjust = -0.1, size = 3.5, color = \"#D73027\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"#E5E5E5\", linewidth = 0.5),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray40\"),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"LASSO Cross-Validation Curve\",\n    subtitle = \"Mean Squared Error vs. log(Œª)\",\n    x = \"log(Œª)\",\n    y = \"Mean Squared Error\",\n    caption = \"Shaded area represents ¬±1 standard error\"\n  )\n\nggsave(\"plot/lasso_cv_curve.png\", p_cv, width = 10, height = 7, dpi = 300, bg = \"white\")\n\n\n# === Selection Summary ===\ncat(\"\\n\", rep(\"=\", 60), \"\\n\", sep = \"\")\n\n\n============================================================\n\ncat(\" Summary of Feature Selection Results\\n\")\n\n Summary of Feature Selection Results\n\ncat(rep(\"=\", 60), \"\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\"Initial number of predictors:\", ncol(x), \"\\n\")\n\nInitial number of predictors: 13 \n\ncat(\"After VIF ‚â§\", vif_threshold, \":\", length(vars_keep), \"\\n\")\n\nAfter VIF ‚â§ 5 : 13 \n\ncat(\"Retained after LASSO:\", nrow(selected_tbl), \"\\n\")\n\nRetained after LASSO: 12 \n\ncat(\"Selection rate:\", round((1 - nrow(selected_tbl) / ncol(x)) * 100, 1), \"%\\n\")\n\nSelection rate: 7.7 %\n\ncat(rep(\"=\", 60), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Step 3 Completed\\n\")\n\n Step 3 Completed\n\ncat(\" LASSO results: file/lasso_selected_variables.csv\\n\")\n\n LASSO results: file/lasso_selected_variables.csv\n\ncat(\" Coefficient importance: plot/lasso_coeff_importance_top20.png\\n\")\n\n Coefficient importance: plot/lasso_coeff_importance_top20.png\n\ncat(\" CV curve: plot/lasso_cv_curve.png\\n\")\n\n CV curve: plot/lasso_cv_curve.png\n\n\nStep4\n\n# =========================================================\n# Step 4 Enhanced: Four Progressive Models + Coefficient Tables + Feature Importance\n# =========================================================\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(car)\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"  Step 4: OLS Progressive Modeling and Full Analysis\\n\")\n\n  Step 4: OLS Progressive Modeling and Full Analysis\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n\n\n# === Read Data ===\ndf &lt;- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\n\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\nif (is.na(y_var)) stop(\" Target variable not found\")\n\ncat(sprintf(\"Target variable: %s\\n\", y_var))\n\nTarget variable: sale_price\n\ncat(sprintf(\"Sample size: %d\\n\\n\", nrow(df)))\n\nSample size: 24023\n\n\n\n# === Data preprocessing ===\ncat(\"=== Data preprocessing ===\\n\")\n\n=== Data preprocessing ===\n\ncat_vars &lt;- c(\"interior_condition\", \"exterior_condition\", \"zip_code\", \"census_tract\")\ncat_vars &lt;- intersect(cat_vars, names(df))\n\nfor (cat_var in cat_vars) {\n  if (cat_var %in% c(\"zip_code\", \"census_tract\")) {\n    freq_table &lt;- table(df[[cat_var]])\n    sparse_levels &lt;- names(freq_table[freq_table &lt; 100])\n    df[[cat_var]] &lt;- as.character(df[[cat_var]])\n    df[[cat_var]][df[[cat_var]] %in% sparse_levels] &lt;- \"Other\"\n    df[[cat_var]] &lt;- factor(df[[cat_var]])\n    cat(sprintf(\"  %s: %d ‚Üí %d categories\\n\", cat_var, length(freq_table), length(levels(df[[cat_var]]))))\n  } else {\n    df[[cat_var]] &lt;- factor(df[[cat_var]])\n  }\n}\n\n  zip_code: 46 ‚Üí 46 categories\n  census_tract: 310 ‚Üí 86 categories\n\ncoord_vars &lt;- c(\"x_coord\", \"y_coord\")\nif (all(coord_vars %in% names(df))) {\n  center_x &lt;- median(df$x_coord, na.rm = TRUE)\n  center_y &lt;- median(df$y_coord, na.rm = TRUE)\n  df$dist_to_center &lt;- sqrt((df$x_coord - center_x)^2 + (df$y_coord - center_y)^2)\n  cat(\"  Added: dist_to_center\\n\")\n} else {\n  coord_vars &lt;- c()\n}\n\n  Added: dist_to_center\n\n# Identify variable groups\nstructural_vars &lt;- c()\nfor (pattern in c(\"livable_area\", \"bedroom\", \"bathroom\", \"stories\", \"garage\", \"age\")) {\n  matched &lt;- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  structural_vars &lt;- c(structural_vars, matched)\n}\nstructural_vars &lt;- unique(structural_vars)\nstructural_vars &lt;- structural_vars[sapply(df[structural_vars], is.numeric)]\n\ncensus_vars &lt;- c()\nfor (pattern in c(\"income\", \"poverty\", \"education\", \"bachelor\", \"population\", \"household\")) {\n  matched &lt;- grep(pattern, names(df), value = TRUE, ignore.case = TRUE)\n  census_vars &lt;- c(census_vars, matched)\n}\ncensus_vars &lt;- unique(census_vars)\ncensus_vars &lt;- census_vars[sapply(df[census_vars], is.numeric)]\n\nspatial_vars &lt;- c(coord_vars, \"dist_to_center\")\nspatial_vars &lt;- intersect(spatial_vars, names(df))\n\nfixed_effects &lt;- c(\"zip_code\", \"census_tract\")\nfixed_effects &lt;- intersect(fixed_effects, names(df))\n\ncat(sprintf(\"\\n  Structural features: %d\\n\", length(structural_vars)))\n\n\n  Structural features: 5\n\ncat(sprintf(\"  Census features: %d\\n\", length(census_vars)))\n\n  Census features: 3\n\ncat(sprintf(\"  Spatial features: %d\\n\", length(spatial_vars)))\n\n  Spatial features: 3\n\ncat(sprintf(\"  Fixed effects: %d\\n\", length(fixed_effects)))\n\n  Fixed effects: 2\n\n\n\n# =========================================================\n# Build 4 Progressive Models\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Building 4 Progressive Models ===\\n\\n\")\n\n=== Building 4 Progressive Models ===\n\nmake_formula &lt;- function(y, main_vars, interact_vars = NULL, fe_vars = NULL) {\n  rhs &lt;- main_vars\n  if (!is.null(interact_vars) && length(interact_vars) == 2) {\n    rhs &lt;- c(rhs, paste(interact_vars, collapse = \":\"))\n  }\n  if (!is.null(fe_vars)) {\n    rhs &lt;- c(rhs, fe_vars)\n  }\n  as.formula(paste(y, \"~\", paste(rhs, collapse = \" + \")))\n}\n\nf1 &lt;- make_formula(y_var, structural_vars)\nf2 &lt;- make_formula(y_var, c(structural_vars, census_vars))\nf3 &lt;- make_formula(y_var, c(structural_vars, census_vars, spatial_vars))\n\ninteract_pairs &lt;- NULL\nif (\"total_livable_area\" %in% structural_vars && \"median_incomeE\" %in% census_vars) {\n  interact_pairs &lt;- c(\"total_livable_area\", \"median_incomeE\")\n}\n\nf4 &lt;- make_formula(y_var, c(structural_vars, census_vars, spatial_vars),\n                   interact_vars = interact_pairs, fe_vars = fixed_effects)\n\nmodels &lt;- list()\ncat(\"  Fitting Model 1...\"); models[[\"M1: Structural\"]] &lt;- lm(f1, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 1...\n\n\n ‚úì\n\ncat(\"  Fitting Model 2...\"); models[[\"M2: +Census\"]] &lt;- lm(f2, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 2...\n\n\n ‚úì\n\ncat(\"  Fitting Model 3...\"); models[[\"M3: +Spatial\"]] &lt;- lm(f3, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 3...\n\n\n ‚úì\n\ncat(\"  Fitting Model 4...\"); models[[\"M4: +Interact+FE\"]] &lt;- lm(f4, data = df, na.action = na.exclude); cat(\" ‚úì\\n\")\n\n  Fitting Model 4...\n\n\n ‚úì\n\n\n\n# =========================================================\n# Model Performance Evaluation\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Model Performance Evaluation ===\\n\\n\")\n\n=== Model Performance Evaluation ===\n\ntrain_perf &lt;- lapply(models, function(m) {\n  pred &lt;- fitted(m)\n  actual &lt;- df[[y_var]][!is.na(fitted(m))]\n  \n  # Log-scale metrics\n  rmse_log &lt;- sqrt(mean((pred - actual)^2, na.rm = TRUE))\n  mae_log &lt;- mean(abs(pred - actual), na.rm = TRUE)\n  \n  # Original-scale metrics ‚Äî use expm1 to invert log1p\n  pred_original &lt;- expm1(pred)\n  actual_original &lt;- expm1(actual)\n  \n  # Method 3: remove outliers based on residuals (|residual| &gt; 3 standard deviations)\n  residuals_original &lt;- pred_original - actual_original\n  res_sd &lt;- sd(residuals_original, na.rm = TRUE)\n  valid_idx &lt;- abs(residuals_original) &lt;= 3 * res_sd\n  \n  # Compute RMSE and MAE after removing outliers\n  rmse_original &lt;- sqrt(mean((pred_original[valid_idx] - actual_original[valid_idx])^2, na.rm = TRUE))\n  mae_original &lt;- mean(abs(pred_original[valid_idx] - actual_original[valid_idx]), na.rm = TRUE)\n  \n  data.frame(\n    RMSE_log = rmse_log,\n    RMSE_original = rmse_original,\n    MAE_log = mae_log,\n    MAE_original = mae_original,\n    R2 = summary(m)$r.squared,\n    Adj_R2 = summary(m)$adj.r.squared,\n    N_vars = length(coef(m)) - 1,\n    N = length(pred),\n    N_valid = sum(valid_idx, na.rm = TRUE),      # retained sample count\n    Pct_valid = mean(valid_idx, na.rm = TRUE) * 100  # percentage of retained samples\n  )\n}) %&gt;% bind_rows(.id = \"Model\")\n\n# Print removal stats\ncat(\"\\nOutlier removal statistics (|residual| &gt; 3œÉ):\\n\")\n\n\nOutlier removal statistics (|residual| &gt; 3œÉ):\n\nprint(train_perf %&gt;% select(Model, N, N_valid, Pct_valid, RMSE_original, MAE_original))\n\n             Model     N N_valid Pct_valid RMSE_original MAE_original\n1   M1: Structural 24023   23693  98.62632      172980.9    113702.55\n2      M2: +Census 24023   23713  98.70957      137239.1     84251.14\n3     M3: +Spatial 24023   23714  98.71373      135990.7     82773.39\n4 M4: +Interact+FE 24023   23726  98.76368      121478.7     73179.58\n\ncat(\"Performing 10-fold cross-validation...\\n\")\n\nPerforming 10-fold cross-validation...\n\ncv_ctrl &lt;- trainControl(method = \"cv\", number = 10, verboseIter = FALSE)\nformulas &lt;- list(f1, f2, f3, f4)\nnames(formulas) &lt;- names(models)\n\ncv_results &lt;- lapply(formulas, function(fm) {\n  tryCatch(train(fm, data = df, method = \"lm\", trControl = cv_ctrl, na.action = na.omit),\n           error = function(e) NULL)\n})\ncv_results &lt;- Filter(Negate(is.null), cv_results)\n\ncv_perf &lt;- lapply(cv_results, function(m) {\n  data.frame(CV_RMSE_log = m$results$RMSE[1], CV_R2 = m$results$Rsquared[1])\n}) %&gt;% bind_rows(.id = \"Model\")\n\nperf_table &lt;- train_perf %&gt;%\n  left_join(cv_perf, by = \"Model\") %&gt;%\n  mutate(Overfit_RMSE_log = CV_RMSE_log - RMSE_log, Overfit_R2 = R2 - CV_R2)\n\ncat(\"\\n\")\nprint(perf_table, digits = 4, row.names = FALSE)\n\n            Model RMSE_log RMSE_original MAE_log MAE_original     R2 Adj_R2\n   M1: Structural   0.6863        172981  0.4860       113703 0.3442 0.3441\n      M2: +Census   0.5917        137239  0.3776        84251 0.5126 0.5124\n     M3: +Spatial   0.5870        135991  0.3714        82773 0.5202 0.5200\n M4: +Interact+FE   0.5561        121479  0.3380        73180 0.5695 0.5669\n N_vars     N N_valid Pct_valid CV_RMSE_log  CV_R2 Overfit_RMSE_log Overfit_R2\n      5 24023   23693     98.63      0.6862 0.3445       -7.634e-05 -0.0003115\n      8 24023   23713     98.71      0.5916 0.5128       -8.878e-05 -0.0001838\n     11 24023   23714     98.71      0.5869 0.5204       -1.000e-04 -0.0001902\n    142 24023   23726     98.76      0.5593 0.5644        3.269e-03  0.0050628\n\n\n\n# =========================================================\n# Heteroskedasticity Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Heteroskedasticity Test (Model 4) ===\\n\\n\")\n\n=== Heteroskedasticity Test (Model 4) ===\n\nbp_test &lt;- bptest(models[[\"M4: +Interact+FE\"]])\ncat(sprintf(\"Breusch‚ÄìPagan Test:\\n  Statistic = %.2f\\n  p-value = %.4f\\n\", \n            bp_test$statistic, bp_test$p.value))\n\nBreusch‚ÄìPagan Test:\n  Statistic = 2134.62\n  p-value = 0.0000\n\nif (bp_test$p.value &lt; 0.05) {\n  cat(\"\\nÔ∏è Significant heteroskedasticity detected (p &lt; 0.05)\\n   Recommendation: use robust standard errors\\n\")\n} else {\n  cat(\"\\n No significant heteroskedasticity detected\\n\")\n}\n\n\nÔ∏è Significant heteroskedasticity detected (p &lt; 0.05)\n   Recommendation: use robust standard errors\n\n\n\n# =========================================================\n# Model 4 Full Coefficient Table (Robust Standard Errors)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Model 4: Full Coefficient Table (Robust SE) ===\\n\\n\")\n\n=== Model 4: Full Coefficient Table (Robust SE) ===\n\nmodel4 &lt;- models[[\"M4: +Interact+FE\"]]\nrobust_coef &lt;- coeftest(model4, vcov = vcovHC(model4, type = \"HC1\"))\n\ncoef_table &lt;- tidy(robust_coef) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    sig = case_when(\n      p.value &lt; 0.001 ~ \"***\",\n      p.value &lt; 0.01 ~ \"**\",\n      p.value &lt; 0.05 ~ \"*\",\n      p.value &lt; 0.10 ~ \".\",\n      TRUE ~ \"\"\n    )\n  ) %&gt;%\n  arrange(p.value) %&gt;%\n  dplyr::select(term, estimate, std.error, statistic, p.value, sig)\n\nif (!dir.exists(\"file\")) dir.create(\"file\")\nif (!dir.exists(\"table\")) dir.create(\"table\")\n\nwrite_csv(coef_table, \"table/model4_full_coefficients.csv\")\ncat(\"  ‚úì table/model4_full_coefficients.csv (All coefficients)\\n\")\n\n  ‚úì table/model4_full_coefficients.csv (All coefficients)\n\ncoef_sig &lt;- coef_table %&gt;% filter(p.value &lt; 0.05)\nwrite_csv(coef_sig, \"table/model4_significant_coefficients.csv\")\ncat(\"  ‚úì table/model4_significant_coefficients.csv (Significant coefficients)\\n\")\n\n  ‚úì table/model4_significant_coefficients.csv (Significant coefficients)\n\ncat(sprintf(\"\\nNumber of significant variables (p &lt; 0.05): %d\\n\", nrow(coef_sig)))\n\n\nNumber of significant variables (p &lt; 0.05): 74\n\ncat(\"\\nTop 20 significant variables:\\n\")\n\n\nTop 20 significant variables:\n\nprint(as.data.frame(head(coef_sig, 20)), row.names = FALSE)\n\n                term    estimate   std.error  statistic       p.value sig\n number_of_bathrooms  0.57683321 0.015420414  37.407116 1.142354e-297 ***\n     per_cap_incomeE  0.36773913 0.025683742  14.317973  2.628332e-46 ***\n  total_livable_area  0.48199968 0.038119301  12.644505  1.571285e-36 ***\n       zip_code19103  0.63540760 0.054794635  11.596165  5.218699e-31 ***\n                age2 -0.04775122 0.004555385 -10.482367  1.183564e-25 ***\n       census_tract7  0.87985167 0.088002818   9.997994  1.729730e-23 ***\n      census_tract13  0.54303063 0.068221853   7.959775  1.799726e-15 ***\n      census_tract14  0.54498041 0.068646953   7.938887  2.129287e-15 ***\n      census_tract12  0.54252167 0.068824818   7.882646  3.341484e-15 ***\n       zip_code19106  0.63471600 0.085682176   7.407795  1.326740e-13 ***\n       zip_code19147  0.49363371 0.066777930   7.392169  1.492021e-13 ***\n     census_tract180  0.72410180 0.099219613   7.297970  3.012660e-13 ***\n       zip_code19130  0.49781714 0.072690260   6.848471  7.645435e-12 ***\n       zip_code19107  0.49815251 0.073630825   6.765543  1.358805e-11 ***\n       zip_code19148  0.44251467 0.073274428   6.039142  1.572250e-09 ***\n      census_tract11  0.39171657 0.066613539   5.880435  4.146363e-09 ***\n     census_tract379  0.54364745 0.102377266   5.310236  1.104621e-07 ***\n     census_tract179  0.61289816 0.115555239   5.303941  1.143375e-07 ***\n       zip_code19146  0.34323604 0.065585163   5.233440  1.677892e-07 ***\n       zip_code19123  0.37828484 0.072407204   5.224409  1.761784e-07 ***\n\n\n\n# =========================================================\n# Stargazer Regression Table\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generating Stargazer Regression Table ===\\n\\n\")\n\n=== Generating Stargazer Regression Table ===\n\nif (requireNamespace(\"stargazer\", quietly = TRUE)) {\n  sink(\"table/regression_table.txt\")\n  stargazer::stargazer(\n    models[[1]], models[[2]], models[[3]], models[[4]],\n    type = \"text\",\n    title = \"Progressive OLS Regression Results\",\n    dep.var.labels = \"Log Sale Price\",\n    column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"),\n    omit.stat = c(\"ser\", \"f\"),\n    digits = 3,\n    star.cutoffs = c(0.05, 0.01, 0.001),\n    notes = \"Robust standard errors in parentheses\"\n  )\n  sink()\n  cat(\"   table/regression_table.txt\\n\")\n}\n\n   table/regression_table.txt\n\n\n\n# =========================================================\n# Cross-Model Feature Importance\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Cross-Model Feature Importance Comparison ===\\n\\n\")\n\n=== Cross-Model Feature Importance Comparison ===\n\nimportance_list &lt;- list()\nfor (i in 1:4) {\n  model_name &lt;- names(models)[i]\n  model &lt;- models[[i]]\n  \n  coefs &lt;- tidy(model) %&gt;%\n    filter(term != \"(Intercept)\") %&gt;%\n    mutate(abs_estimate = abs(estimate)) %&gt;%\n    arrange(desc(abs_estimate)) %&gt;%\n    head(10) %&gt;%\n    mutate(Model = model_name) %&gt;%\n    dplyr::select(Model, term, estimate, abs_estimate)\n  \n  importance_list[[i]] &lt;- coefs\n}\n\nimportance_all &lt;- bind_rows(importance_list)\nwrite_csv(importance_all, \"table/feature_importance_all_models.csv\")\ncat(\"  ‚úì table/feature_importance_all_models.csv\\n\")\n\n  ‚úì table/feature_importance_all_models.csv\n\n# Identify variables that are important across multiple models\ntop_vars &lt;- importance_all %&gt;%\n  group_by(term) %&gt;%\n  summarise(appearances = n(), .groups = \"drop\") %&gt;%\n  filter(appearances &gt;= 3) %&gt;%\n  pull(term)\n\nif (length(top_vars) &gt; 0) {\n  importance_key &lt;- importance_all %&gt;% filter(term %in% top_vars)\n  \n  if (!dir.exists(\"plot\")) dir.create(\"plot\")\n  \n  p_importance &lt;- ggplot(importance_key, aes(Model, abs_estimate, fill = Model)) +\n    geom_col(alpha = 0.8) +\n    facet_wrap(~term, scales = \"free_y\", ncol = 4) +\n    scale_fill_manual(\n      values = c(\"#053061\", \"#2166AC\", \"#4393C3\", \"#D6604D\"),\n      guide = \"none\"\n    ) +\n    theme_minimal(base_size = 9) +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1, size = 7),\n      strip.text = element_text(face = \"bold\", size = 9),\n      panel.grid.major.x = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      plot.subtitle = element_text(hjust = 0.5, size = 10),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Feature Importance Across Models\",\n      subtitle = \"Variables appearing in top 10 of at least 3 models\",\n      x = NULL, y = \"Absolute Coefficient\"\n    )\n  \n  ggsave(\"plot/feature_importance_across_models.png\", p_importance,\n         width = 14, height = 10, dpi = 300, bg = \"white\")\n  cat(\"   plot/feature_importance_across_models.png\\n\")\n}\n\n   plot/feature_importance_across_models.png\n\n\n\n# =========================================================\n# Visualization (Original 3 Figures)\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Generate Performance Visualizations ===\\n\\n\")\n\n=== Generate Performance Visualizations ===\n\nperf_plot_data &lt;- perf_table %&gt;% mutate(Model_Num = 1:n())\n\np_r2 &lt;- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = R2, color = \"Training R¬≤\"), linewidth = 1.2) +\n  geom_line(aes(y = CV_R2, color = \"CV R¬≤\"), linewidth = 1.2) +\n  geom_point(aes(y = R2, color = \"Training R¬≤\"), size = 3) +\n  geom_point(aes(y = CV_R2, color = \"CV R¬≤\"), size = 3) +\n  scale_color_manual(values = c(\"Training R¬≤\" = \"#67001F\", \"CV R¬≤\" = \"#053061\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"R¬≤ Improvement: Progressive Model Building\", x = NULL, y = \"R¬≤ Value\")\n\np_rmse &lt;- ggplot(perf_plot_data, aes(Model_Num)) +\n  geom_line(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), linewidth = 1.2) +\n  geom_point(aes(y = RMSE_original/1000, color = \"RMSE ($1000s)\"), size = 3) +\n  scale_color_manual(values = c(\"RMSE ($1000s)\" = \"#67001F\"), name = NULL) +\n  scale_x_continuous(breaks = 1:4, labels = perf_table$Model) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 20, hjust = 1, size = 9),\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"RMSE Reduction (Original Scale)\", x = NULL, y = \"RMSE ($1000s)\")\n\np_combined &lt;- (p_r2 | p_rmse) +\n  plot_annotation(\n    title = \"4-Model Progressive OLS: Performance Evolution\",\n    subtitle = \"From structural features to full specification with interactions and fixed effects\",\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 11, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/ols_4model_performance.png\", p_combined, width = 14, height = 6, dpi = 300, bg = \"white\")\ncat(\"  ‚úì plot/ols_4model_performance.png\\n\")\n\n  ‚úì plot/ols_4model_performance.png\n\n\n\n# Prediction Scatter Plot\npred_m4 &lt;- fitted(models[[\"M4: +Interact+FE\"]])\nactual_m4 &lt;- df[[y_var]][!is.na(pred_m4)]\npred_data &lt;- data.frame(\n  actual = actual_m4,\n  predicted = pred_m4[!is.na(pred_m4)],\n  residual = pred_m4[!is.na(pred_m4)] - actual_m4\n)\n\np_pred &lt;- ggplot(pred_data, aes(actual, predicted)) +\n  geom_point(aes(color = residual), alpha = 0.4, size = 0.8) +\n  scale_color_gradient2(low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\", midpoint = 0, name = \"Residual\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = min(pred_data$actual) + 0.5, y = max(pred_data$predicted) - 0.5,\n           label = sprintf(\"R¬≤ = %.4f\\nRMSE = %.4f\", perf_table$R2[4], perf_table$RMSE[4]),\n           hjust = 0, size = 4.5, fontface = \"bold\") +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(title = \"Model 4: Prediction vs. Actual\",\n       subtitle = \"Full specification with interactions and fixed effects\",\n       x = \"Actual Sale Price (log)\", y = \"Predicted Sale Price (log)\")\n\nggsave(\"plot/ols_4model_prediction.png\", p_pred, width = 10, height = 8, dpi = 300, bg = \"white\")\ncat(\"   plot/ols_4model_prediction.png\\n\")\n\n   plot/ols_4model_prediction.png\n\n\n\n# Residual Diagnostics\nresid_data &lt;- data.frame(\n  fitted = pred_m4[!is.na(pred_m4)],\n  residual = pred_data$residual,\n  std_resid = rstandard(models[[\"M4: +Interact+FE\"]])[!is.na(pred_m4)]\n)\n\np_r1 &lt;- ggplot(resid_data, aes(fitted, residual)) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residuals vs. Fitted\", x = \"Fitted\", y = \"Residuals\")\n\np_r2 &lt;- ggplot(resid_data, aes(sample = std_resid)) +\n  stat_qq(alpha = 0.4, size = 0.6, color = \"#053061\") +\n  stat_qq_line(color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Normal Q-Q\", x = \"Theoretical\", y = \"Std. Residuals\")\n\np_r3 &lt;- ggplot(resid_data, aes(std_resid)) +\n  geom_histogram(bins = 50, fill = \"#053061\", color = \"white\", alpha = 0.8) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Residual Distribution\", x = \"Std. Residuals\", y = \"Count\")\n\np_r4 &lt;- ggplot(resid_data, aes(fitted, sqrt(abs(std_resid)))) +\n  geom_point(alpha = 0.3, size = 0.6, color = \"#053061\") +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"#67001F\", linewidth = 1) +\n  theme_minimal(base_size = 10) +\n  theme(panel.grid.minor = element_blank(), plot.background = element_rect(fill = \"white\", color = NA)) +\n  labs(title = \"Scale-Location\", x = \"Fitted\", y = \"‚àö|Std. Residuals|\")\n\np_diag &lt;- (p_r1 | p_r2) / (p_r3 | p_r4) +\n  plot_annotation(\n    title = sprintf(\"Model 4 Diagnostics (BP test p = %.4f)\", bp_test$p.value),\n    theme = theme(plot.title = element_text(size = 13, face = \"bold\", hjust = 0.5),\n                  plot.background = element_rect(fill = \"white\", color = NA))\n  )\n\nggsave(\"plot/ols_4model_diagnostics.png\", p_diag, width = 12, height = 10, dpi = 300, bg = \"white\")\ncat(\"  ‚úì plot/ols_4model_diagnostics.png\\n\")\n\n  ‚úì plot/ols_4model_diagnostics.png\n\n\n\n# =========================================================\n# Spatial Prediction Comparison Plot\n# =========================================================\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n  cat(\"=== Generating Spatial Prediction Comparison Plot ===\\n\\n\")\n  \n  # Prepare data\n  spatial_pred_data &lt;- df %&gt;%\n    filter(!is.na(x_coord) & !is.na(y_coord)) %&gt;%\n    mutate(\n      predicted = fitted(models[[\"M4: +Interact+FE\"]]),\n      residual = predicted - .data[[y_var]]\n    ) %&gt;%\n    filter(!is.na(predicted))\n  \n  # If the dataset is too large, randomly sample to speed up plotting\n  if (nrow(spatial_pred_data) &gt; 10000) {\n    set.seed(2025)\n    spatial_pred_data &lt;- spatial_pred_data %&gt;% \n      slice_sample(n = 10000)\n    cat(\"  Large dataset detected; randomly sampled 10,000 points for plotting\\n\")\n  }\n\n# 1. Spatial Distribution of Actual Sale Prices\np_actual &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = .data[[y_var]])) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Actual Sale Price\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 2. Spatial Distribution of Predicted Sale Prices\np_predicted &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = predicted)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n    name = \"Log Price\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Predicted Sale Price (Model 4)\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# 3. Spatial Distribution of Residuals\np_residual_spatial &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, color = residual)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = 0,\n    name = \"Residual\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Prediction Residuals\",\n    subtitle = \"Blue = Underpredicted | Red = Overpredicted\",\n    x = \"X Coordinate\",\n    y = \"Y Coordinate\"\n  )\n\n# Combine the 3 plots\np_spatial_comparison &lt;- (p_actual | p_predicted | p_residual_spatial) +\n  plot_annotation(\n    title = \"Spatial Distribution: Actual vs. Predicted Sale Prices\",\n    subtitle = sprintf(\"Model 4 predictions (R¬≤ = %.4f, RMSE = %.4f)\", \n                      perf_table$R2[4], perf_table$RMSE[4]),\n    theme = theme(\n      plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = 12, hjust = 0.5),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/spatial_prediction_comparison.png\", p_spatial_comparison,\n       width = 18, height = 6, dpi = 300, bg = \"white\")\ncat(\"   plot/spatial_prediction_comparison.png\\n\")\n\n  # 4. High-Resolution Hexbin Heatmap Version\n  p_actual_hex &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = .data[[y_var]])) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data[[y_var]], na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Actual (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_predicted_hex &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = predicted)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(spatial_pred_data$predicted, na.rm = TRUE),\n      name = \"Avg Log Price\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Predicted (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_residual_hex &lt;- ggplot(spatial_pred_data, aes(x_coord, y_coord, z = residual)) +\n    stat_summary_hex(bins = 40, fun = mean) +\n    scale_fill_gradient2(\n      low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = 0,\n      name = \"Avg Residual\"\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 13),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(title = \"Residuals (Hexbin Average)\", x = \"X Coordinate\", y = \"Y Coordinate\")\n  \n  p_spatial_hexbin &lt;- (p_actual_hex | p_predicted_hex | p_residual_hex) +\n    plot_annotation(\n      title = \"Spatial Distribution (Hexbin Aggregation): Actual vs. Predicted\",\n      subtitle = \"Hexagonal bins show average values in each area\",\n      theme = theme(\n        plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/spatial_prediction_hexbin.png\", p_spatial_hexbin,\n         width = 18, height = 6, dpi = 300, bg = \"white\")\n  cat(\"  ‚úì plot/spatial_prediction_hexbin.png\\n\")\n  \n  # 5. Spatial Clustering of Residuals\n  # Calculate spatial statistics of residuals\n  residual_stats &lt;- spatial_pred_data %&gt;%\n    summarise(\n      mean_residual = mean(residual, na.rm = TRUE),\n      sd_residual = sd(residual, na.rm = TRUE),\n      min_residual = min(residual, na.rm = TRUE),\n      max_residual = max(residual, na.rm = TRUE)\n    )\n  \n  cat(\"\\n  Spatial Residual Statistics:\\n\")\n  cat(sprintf(\"    Mean: %.4f\\n\", residual_stats$mean_residual))\n  cat(sprintf(\"    Standard Deviation: %.4f\\n\", residual_stats$sd_residual))\n  cat(sprintf(\"    Range: [%.4f, %.4f]\\n\", \n              residual_stats$min_residual, residual_stats$max_residual))\n  \n  # Identify high-residual areas\n  high_residual_areas &lt;- spatial_pred_data %&gt;%\n    filter(abs(residual) &gt; 2 * residual_stats$sd_residual) %&gt;%\n    mutate(residual_type = ifelse(residual &gt; 0, \"Overpredicted\", \"Underpredicted\"))\n  \n  if (nrow(high_residual_areas) &gt; 0) {\n    cat(sprintf(\"\\n  Ô∏è Detected %d high-residual points (|residual| &gt; 2œÉ):\\n\", \n                nrow(high_residual_areas)))\n    cat(sprintf(\"    Overpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Overpredicted\")))\n    cat(sprintf(\"    Underpredicted: %d\\n\", sum(high_residual_areas$residual_type == \"Underpredicted\")))\n    \n    # Save high-residual point data\n    write_csv(high_residual_areas %&gt;% \n                dplyr::select(x_coord, y_coord, !!sym(y_var), predicted, residual, residual_type),\n              \"file/high_residual_locations.csv\")\n    cat(\"     file/high_residual_locations.csv\\n\")\n  }\n}\n\n\n================================================================================\n=== Generating Spatial Prediction Comparison Plot ===\n\n  Large dataset detected; randomly sampled 10,000 points for plotting\n\n\n   plot/spatial_prediction_comparison.png\n\n\n  ‚úì plot/spatial_prediction_hexbin.png\n\n  Spatial Residual Statistics:\n    Mean: 0.0036\n    Standard Deviation: 0.5571\n    Range: [-3.8418, 3.8371]\n\n  Ô∏è Detected 483 high-residual points (|residual| &gt; 2œÉ):\n    Overpredicted: 294\n    Underpredicted: 189\n     file/high_residual_locations.csv\n\n\n\n# =========================================================\n# Save Results\n# =========================================================\n\nwrite_csv(perf_table, \"file/ols_4model_performance.csv\")\nsaveRDS(models, \"file/ols_4models.rds\")\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 4 Completed\\n\")\n\n Step 4 Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Output Files:\\n\\n\")\n\n Output Files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\ncat(\"  ‚Ä¢ table/model4_full_coefficients.csv - Full coefficient table (Model 4)\\n\")\n\n  ‚Ä¢ table/model4_full_coefficients.csv - Full coefficient table (Model 4)\n\ncat(\"  ‚Ä¢ table/model4_significant_coefficients.csv - Significant coefficients (Model 4)\\n\")\n\n  ‚Ä¢ table/model4_significant_coefficients.csv - Significant coefficients (Model 4)\n\ncat(\"  ‚Ä¢ table/regression_table.txt - Stargazer regression summary\\n\")\n\n  ‚Ä¢ table/regression_table.txt - Stargazer regression summary\n\ncat(\"  ‚Ä¢ table/feature_importance_all_models.csv - Cross-model feature importance\\n\")\n\n  ‚Ä¢ table/feature_importance_all_models.csv - Cross-model feature importance\n\ncat(\"  ‚Ä¢ file/ols_4model_performance.csv - Model performance comparison\\n\")\n\n  ‚Ä¢ file/ols_4model_performance.csv - Model performance comparison\n\ncat(\"  ‚Ä¢ file/ols_4models.rds - RDS objects of all 4 models\\n\\n\")\n\n  ‚Ä¢ file/ols_4models.rds - RDS objects of all 4 models\n\ncat(\"Figures:\\n\")\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/ols_4model_performance.png - Performance evolution\\n\")\n\n  ‚Ä¢ plot/ols_4model_performance.png - Performance evolution\n\ncat(\"  ‚Ä¢ plot/ols_4model_prediction.png - Model 4 prediction vs. actual\\n\")\n\n  ‚Ä¢ plot/ols_4model_prediction.png - Model 4 prediction vs. actual\n\ncat(\"  ‚Ä¢ plot/ols_4model_diagnostics.png - Model 4 diagnostic plots\\n\")\n\n  ‚Ä¢ plot/ols_4model_diagnostics.png - Model 4 diagnostic plots\n\ncat(\"  ‚Ä¢ plot/feature_importance_across_models.png - Cross-model feature importance\\n\")\n\n  ‚Ä¢ plot/feature_importance_across_models.png - Cross-model feature importance\n\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  cat(\"  ‚Ä¢ plot/spatial_prediction_comparison.png - Spatial prediction comparison\\n\")\n  cat(\"  ‚Ä¢ plot/spatial_prediction_hexbin.png - Spatial prediction heatmap\\n\")\n}\n\n  ‚Ä¢ plot/spatial_prediction_comparison.png - Spatial prediction comparison\n  ‚Ä¢ plot/spatial_prediction_hexbin.png - Spatial prediction heatmap\n\ncat(\"\\n\")\ncat(\" Model Summary:\\n\")\n\n Model Summary:\n\nfor (i in 1:4) {\n  rmse_fmt &lt;- formatC(perf_table$RMSE_original[i], format = \"f\", digits = 0, big.mark = \",\")\n  cat(sprintf(\"  Model %d: R¬≤ = %.4f, RMSE = $%s [%d variables]\\n\",\n              i, perf_table$R2[i], rmse_fmt, perf_table$N_vars[i]))\n}\n\n  Model 1: R¬≤ = 0.3442, RMSE = $172,981 [5 variables]\n  Model 2: R¬≤ = 0.5126, RMSE = $137,239 [8 variables]\n  Model 3: R¬≤ = 0.5202, RMSE = $135,991 [11 variables]\n  Model 4: R¬≤ = 0.5695, RMSE = $121,479 [142 variables]\n\ncat(\"\\n Paper Writing Suggestions:\\n\")\n\n\n Paper Writing Suggestions:\n\ncat(\"  1. Results (Opening): Descriptive statistics table\\n\")\n\n  1. Results (Opening): Descriptive statistics table\n\ncat(\"  2. Results (Core): Stargazer regression summary (comparison of 4 models)\\n\")\n\n  2. Results (Core): Stargazer regression summary (comparison of 4 models)\n\ncat(\"  3. Results (Details): Significant coefficients of Model 4\\n\")\n\n  3. Results (Details): Significant coefficients of Model 4\n\ncat(\"  4. Discussion: Cross-model feature importance comparison\\n\")\n\n  4. Discussion: Cross-model feature importance comparison\n\ncat(\"  5. Mention the use of robust standard errors to address heteroskedasticity\\n\\n\")\n\n  5. Mention the use of robust standard errors to address heteroskedasticity\n\n\nStep4 plus\n\n# =========================================================\n# Step 4 Supplement: Cook‚Äôs Distance + Moran‚Äôs I Spatial Autocorrelation\n# =========================================================\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(patchwork)\nlibrary(spdep)  # Moran‚Äôs I\nlibrary(sf)     # Spatial data\n\nset.seed(2025)\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 4 Supplement: Influential Point Diagnostics and Spatial Autocorrelation Test\\n\")\n\n Step 4 Supplement: Influential Point Diagnostics and Spatial Autocorrelation Test\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\n# === Load Data and Models ===\ndf &lt;- read_csv(\"file/opa_sales_step2_clean.csv\", show_col_types = FALSE)\nmodels &lt;- readRDS(\"file/ols_4models.rds\")\nmodel4 &lt;- models[[\"M4: +Interact+FE\"]]\n\ny_var &lt;- case_when(\n  \"sale_price_log\" %in% names(df) ~ \"sale_price_log\",\n  \"log_sale_price\" %in% names(df) ~ \"log_sale_price\",\n  \"sale_price\" %in% names(df) ~ \"sale_price\",\n  TRUE ~ NA_character_\n)\n\n\n# =========================================================\n# 1. Cook‚Äôs Distance ‚Äì Influential Observation Detection\n# =========================================================\n\ncat(\"=== Cook‚Äôs Distance Analysis ===\\n\\n\")\n\n=== Cook‚Äôs Distance Analysis ===\n\n# Calculate Cook‚Äôs distance\ncooks_d &lt;- cooks.distance(model4)\nn &lt;- length(cooks_d)\nthreshold &lt;- 4/n  # Classic threshold\n\n# Identify influential points\ninfluential &lt;- which(cooks_d &gt; threshold)\ncat(sprintf(\"Sample size: %d\\n\", n))\n\nSample size: 24023\n\ncat(sprintf(\"Cook‚Äôs D threshold: %.6f (4/n)\\n\", threshold))\n\nCook‚Äôs D threshold: 0.000167 (4/n)\n\ncat(sprintf(\"Number of influential points: %d (%.2f%%)\\n\", length(influential), 100*length(influential)/n))\n\nNumber of influential points: 1060 (4.41%)\n\n# Save influential observations\nif (length(influential) &gt; 0) {\n  influential_data &lt;- df[influential, ] %&gt;%\n    mutate(\n      cooks_d = cooks_d[influential],\n      fitted = fitted(model4)[influential],\n      residual = residuals(model4)[influential]\n    ) %&gt;%\n    arrange(desc(cooks_d)) %&gt;%\n    head(100)  # Save Top 100\n  \n  write_csv(influential_data, \"file/influential_observations.csv\")\n  cat(\"  ‚úì file/influential_observations.csv (Top 100 Influential Points)\\n\")\n  \n  # Display Top 10\n  cat(\"\\nTop 10 Influential Points:\\n\")\n  print(influential_data %&gt;% \n          select(parcel_number, sale_price, cooks_d, fitted, residual) %&gt;% \n          head(10), \n        n = 10)\n}\n\n  ‚úì file/influential_observations.csv (Top 100 Influential Points)\n\nTop 10 Influential Points:\n# A tibble: 10 √ó 5\n   parcel_number sale_price cooks_d fitted residual\n   &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 211051105           9.39 0.00407   13.2    -3.84\n 2 071348000          15.0  0.00383   11.0     4.04\n 3 401237000          15.0  0.00343   11.6     3.44\n 4 401237400          15.0  0.00343   11.6     3.44\n 5 043166400          15.4  0.00335   11.6     3.85\n 6 041330100          15.4  0.00333   11.6     3.84\n 7 043055800          15.4  0.00329   11.6     3.82\n 8 381091300          14.7  0.00320   11.6     3.04\n 9 401220400          15.0  0.00311   11.8     3.28\n10 611317900           9.21 0.00303   13.3    -4.04\n\n# Visualize Cook‚Äôs Distance\ncooks_df &lt;- data.frame(\n  index = 1:n,\n  cooks_d = cooks_d,\n  influential = cooks_d &gt; threshold\n)\n\np_cooks &lt;- ggplot(cooks_df, aes(index, cooks_d, color = influential)) +\n  geom_point(alpha = 0.4, size = 0.8) +\n  geom_hline(yintercept = threshold, linetype = \"dashed\", color = \"#67001F\", linewidth = 1) +\n  scale_color_manual(\n    values = c(\"FALSE\" = \"#053061\", \"TRUE\" = \"#67001F\"),\n    labels = c(\"FALSE\" = \"Normal\", \"TRUE\" = \"Influential\"),\n    name = NULL\n  ) +\n  annotate(\"text\", x = n*0.8, y = threshold*1.2,\n           label = sprintf(\"Threshold = %.6f\", threshold),\n           color = \"#67001F\", size = 4) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"white\", color = NA)\n  ) +\n  labs(\n    title = \"Cook‚Äôs Distance ‚Äì Detection of Influential Observations\",\n    subtitle = sprintf(\"%d influential points (%.2f%% of data)\", \n                      length(influential), 100*length(influential)/n),\n    x = \"Observation Index\",\n    y = \"Cook‚Äôs Distance\",\n    caption = \"Threshold = 4/n (classic rule)\"\n  )\n\nif (!dir.exists(\"plot\")) dir.create(\"plot\")\nggsave(\"plot/cooks_distance.png\", p_cooks, width = 12, height = 6, dpi = 300, bg = \"white\")\ncat(\"\\n   plot/cooks_distance.png\\n\")\n\n\n   plot/cooks_distance.png\n\n\n\n# =========================================================\n# 2. Moran‚Äôs I ‚Äì Spatial Autocorrelation Test\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\"=== Moran‚Äôs I Spatial Autocorrelation Test ===\\n\\n\")\n\n=== Moran‚Äôs I Spatial Autocorrelation Test ===\n\n# Check coordinates\nif (all(c(\"x_coord\", \"y_coord\") %in% names(df))) {\n  \n  # Prepare spatial data\n  df_spatial &lt;- df %&gt;%\n    filter(!is.na(x_coord), !is.na(y_coord)) %&gt;%\n    mutate(\n      residual = residuals(model4)[!is.na(x_coord) & !is.na(y_coord)],\n      sale_price_actual = .data[[y_var]]\n    ) %&gt;%\n    filter(!is.na(residual))\n  \n  # Convert to sf object\n  coords &lt;- df_spatial %&gt;% select(x_coord, y_coord)\n  coords_sf &lt;- st_as_sf(coords, coords = c(\"x_coord\", \"y_coord\"), crs = 2272)\n  \n  cat(sprintf(\"Valid samples: %d\\n\\n\", nrow(df_spatial)))\n  \n  # === 2.1 Moran‚Äôs I for Actual Sale Price ===\n  cat(\"--- Spatial Autocorrelation of Actual Sale Price ---\\n\")\n  \n  # Build spatial weight matrix (K-nearest neighbors, k=8)\n  nb_knn &lt;- knn2nb(knearneigh(coords_sf, k = 8))\n  listw_knn &lt;- nb2listw(nb_knn, style = \"W\")\n  \n  # Moran‚Äôs I test\n  moran_price &lt;- moran.test(df_spatial$sale_price_actual, listw_knn)\n  \n  cat(sprintf(\"  Moran‚Äôs I = %.4f\\n\", moran_price$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_price$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_price$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_price$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_price$p.value))\n  \n  if (moran_price$p.value &lt; 0.001) {\n    cat(\"  *** Highly significant positive spatial autocorrelation (p &lt; 0.001)\\n\")\n  } else if (moran_price$p.value &lt; 0.05) {\n    cat(\"  ** Significant positive spatial autocorrelation (p &lt; 0.05)\\n\")\n  } else {\n    cat(\"  No significant spatial autocorrelation detected\\n\")\n  }\n  \n  # === 2.2 Moran‚Äôs I for Residuals ===\n  cat(\"\\n--- Spatial Autocorrelation of Model 4 Residuals ---\\n\")\n  \n  moran_resid &lt;- moran.test(df_spatial$residual, listw_knn)\n  \n  cat(sprintf(\"  Moran‚Äôs I = %.4f\\n\", moran_resid$estimate[1]))\n  cat(sprintf(\"  Expected = %.4f\\n\", moran_resid$estimate[2]))\n  cat(sprintf(\"  Variance = %.6f\\n\", moran_resid$estimate[3]))\n  cat(sprintf(\"  Z-score = %.4f\\n\", moran_resid$statistic))\n  cat(sprintf(\"  p-value = %.4e\\n\", moran_resid$p.value))\n  \n  if (moran_resid$p.value &lt; 0.001) {\n    cat(\"  Ô∏è Strong residual spatial autocorrelation detected (p &lt; 0.001)\\n\")\n    cat(\"  Suggestion: Consider Spatial Lag Model (SAR) or Spatial Error Model (SEM)\\n\")\n  } else if (moran_resid$p.value &lt; 0.05) {\n    cat(\"  Ô∏è Residual spatial autocorrelation detected (p &lt; 0.05)\\n\")\n    cat(\"  Suggestion: Add more spatial variables or use spatial regression models\\n\")\n  } else {\n    cat(\"  ‚úì No significant residual spatial autocorrelation ‚Äì spatial effects well captured\\n\")\n  }\n  \n  # === 2.3 Moran Scatterplots ===\n  \n  # Calculate spatial lags\n  lag_price &lt;- lag.listw(listw_knn, df_spatial$sale_price_actual)\n  lag_resid &lt;- lag.listw(listw_knn, df_spatial$residual)\n  \n  # Standardize\n  price_std &lt;- scale(df_spatial$sale_price_actual)[,1]\n  lag_price_std &lt;- scale(lag_price)[,1]\n  resid_std &lt;- scale(df_spatial$residual)[,1]\n  lag_resid_std &lt;- scale(lag_resid)[,1]\n  \n  # Define quadrants\n  quadrant_price &lt;- case_when(\n    price_std &gt; 0 & lag_price_std &gt; 0 ~ \"HH (High-High)\",\n    price_std &lt; 0 & lag_price_std &lt; 0 ~ \"LL (Low-Low)\",\n    price_std &gt; 0 & lag_price_std &lt; 0 ~ \"HL (High-Low)\",\n    TRUE ~ \"LH (Low-High)\"\n  )\n  \n  quadrant_resid &lt;- case_when(\n    resid_std &gt; 0 & lag_resid_std &gt; 0 ~ \"HH\",\n    resid_std &lt; 0 & lag_resid_std &lt; 0 ~ \"LL\",\n    resid_std &gt; 0 & lag_resid_std &lt; 0 ~ \"HL\",\n    TRUE ~ \"LH\"\n  )\n  \n  moran_df_price &lt;- data.frame(\n    value = price_std,\n    lag_value = lag_price_std,\n    quadrant = quadrant_price\n  )\n  \n  moran_df_resid &lt;- data.frame(\n    value = resid_std,\n    lag_value = lag_resid_std,\n    quadrant = quadrant_resid\n  )\n  \n  # Plot ‚Äì Sale Price\n  p_moran_price &lt;- ggplot(moran_df_price, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH (High-High)\" = \"#67001F\", \n                 \"LL (Low-Low)\" = \"#053061\",\n                 \"HL (High-Low)\" = \"#F4A582\",\n                 \"LH (Low-High)\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran‚Äôs I = %.4f***\", moran_price$estimate[1]),\n             size = 5, fontface = \"bold\", color = \"#67001F\") +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran‚Äôs I Scatter Plot ‚Äì Sale Price\",\n      subtitle = \"Strong positive spatial autocorrelation\",\n      x = \"Standardized Sale Price\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Plot ‚Äì Residuals\n  p_moran_resid &lt;- ggplot(moran_df_resid, aes(value, lag_value, color = quadrant)) +\n    geom_point(alpha = 0.4, size = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(aes(group = 1), method = \"lm\", se = FALSE, \n                color = \"#67001F\", linewidth = 1.2) +\n    scale_color_manual(\n      values = c(\"HH\" = \"#67001F\", \"LL\" = \"#053061\",\n                 \"HL\" = \"#F4A582\", \"LH\" = \"#92C5DE\"),\n      name = \"Quadrant\"\n    ) +\n    annotate(\"text\", x = -3, y = 3, \n             label = sprintf(\"Moran‚Äôs I = %.4f%s\", \n                           moran_resid$estimate[1],\n                           ifelse(moran_resid$p.value &lt; 0.001, \"***\",\n                                  ifelse(moran_resid$p.value &lt; 0.05, \"**\", \"\"))),\n             size = 5, fontface = \"bold\", \n             color = ifelse(moran_resid$p.value &lt; 0.05, \"#67001F\", \"gray50\")) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n      plot.subtitle = element_text(hjust = 0.5, size = 11),\n      legend.position = \"right\",\n      plot.background = element_rect(fill = \"white\", color = NA)\n    ) +\n    labs(\n      title = \"Moran‚Äôs I Scatter Plot ‚Äì Model 4 Residuals\",\n      subtitle = ifelse(moran_resid$p.value &lt; 0.05, \n                       \"Residual spatial autocorrelation detected\",\n                       \"No significant residual spatial autocorrelation\"),\n      x = \"Standardized Residual\",\n      y = \"Spatial Lag (Average of Neighbors)\"\n    )\n  \n  # Combine plots\n  p_moran_combined &lt;- (p_moran_price | p_moran_resid) +\n    plot_annotation(\n      title = \"Spatial Autocorrelation Analysis (Moran‚Äôs I)\",\n      subtitle = \"Left: Original prices show strong clustering | Right: Model residuals\",\n      theme = theme(\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.background = element_rect(fill = \"white\", color = NA)\n      )\n    )\n  \n  ggsave(\"plot/morans_i_scatter.png\", p_moran_combined, \n         width = 16, height = 7, dpi = 300, bg = \"white\")\n  cat(\"\\n   plot/morans_i_scatter.png\\n\")\n  \n  # Save Moran‚Äôs I results\n  moran_summary &lt;- data.frame(\n    Variable = c(\"Sale Price\", \"Model 4 Residuals\"),\n    Morans_I = c(moran_price$estimate[1], moran_resid$estimate[1]),\n    Expected = c(moran_price$estimate[2], moran_resid$estimate[2]),\n    Variance = c(moran_price$estimate[3], moran_resid$estimate[3]),\n    Z_score = c(moran_price$statistic, moran_resid$statistic),\n    P_value = c(moran_price$p.value, moran_resid$p.value),\n    Significant = c(moran_price$p.value &lt; 0.05, moran_resid$p.value &lt; 0.05)\n  )\n  \n  write_csv(moran_summary, \"file/morans_i_results.csv\")\n  cat(\"  ‚úì file/morans_i_results.csv\\n\")\n  \n} else {\n  cat(\" Missing coordinate data ‚Äì Moran‚Äôs I test cannot be performed\\n\")\n}\n\nValid samples: 24023\n\n--- Spatial Autocorrelation of Actual Sale Price ---\n\n\n  Moran‚Äôs I = 0.5328\n  Expected = -0.0000\n  Variance = 0.000009\n  Z-score = 175.1095\n  p-value = 0.0000e+00\n  *** Highly significant positive spatial autocorrelation (p &lt; 0.001)\n\n--- Spatial Autocorrelation of Model 4 Residuals ---\n  Moran‚Äôs I = 0.0820\n  Expected = -0.0000\n  Variance = 0.000009\n  Z-score = 26.9827\n  p-value = 1.1797e-160\n  Ô∏è Strong residual spatial autocorrelation detected (p &lt; 0.001)\n  Suggestion: Consider Spatial Lag Model (SAR) or Spatial Error Model (SEM)\n\n\n\n   plot/morans_i_scatter.png\n  ‚úì file/morans_i_results.csv\n\n\n\n# =========================================================\n# Summary\n# =========================================================\n\ncat(\"\\n\", rep(\"=\", 80), \"\\n\", sep = \"\")\n\n\n================================================================================\n\ncat(\" Step 4 Supplement Completed\\n\")\n\n Step 4 Supplement Completed\n\ncat(rep(\"=\", 80), \"\\n\\n\")\n\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n\ncat(\" Output Files:\\n\\n\")\n\n Output Files:\n\ncat(\"Tables:\\n\")\n\nTables:\n\nif (exists(\"influential_data\")) {\n  cat(\"  ‚Ä¢ file/influential_observations.csv ‚Äì List of influential observations\\n\")\n}\n\n  ‚Ä¢ file/influential_observations.csv ‚Äì List of influential observations\n\nif (exists(\"moran_summary\")) {\n  cat(\"  ‚Ä¢ file/morans_i_results.csv ‚Äì Moran‚Äôs I test results\\n\")\n}\n\n  ‚Ä¢ file/morans_i_results.csv ‚Äì Moran‚Äôs I test results\n\ncat(\"\\nFigures:\\n\")\n\n\nFigures:\n\ncat(\"  ‚Ä¢ plot/cooks_distance.png ‚Äì Cook‚Äôs Distance Scatter Plot\\n\")\n\n  ‚Ä¢ plot/cooks_distance.png ‚Äì Cook‚Äôs Distance Scatter Plot\n\nif (exists(\"p_moran_combined\")) {\n  cat(\"  ‚Ä¢ plot/morans_i_scatter.png ‚Äì Moran‚Äôs I Scatter Plot\\n\")\n}\n\n  ‚Ä¢ plot/morans_i_scatter.png ‚Äì Moran‚Äôs I Scatter Plot\n\ncat(\"\\n Key Findings:\\n\")\n\n\n Key Findings:\n\nif (exists(\"moran_resid\")) {\n  if (moran_resid$p.value &lt; 0.05) {\n    cat(\"  Ô∏è Significant spatial autocorrelation detected in residuals\\n\")\n    cat(\"     ‚Üí The model did not fully capture spatial effects\\n\")\n    cat(\"     ‚Üí Suggest using spatial econometric models (SAR/SEM)\\n\")\n  } else {\n    cat(\"   No significant spatial autocorrelation in residuals\\n\")\n    cat(\"     ‚Üí Fixed effects adequately controlled for spatial dependence\\n\")\n  }\n}\n\n  Ô∏è Significant spatial autocorrelation detected in residuals\n     ‚Üí The model did not fully capture spatial effects\n     ‚Üí Suggest using spatial econometric models (SAR/SEM)\n\ncat(\"\\n\")\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(patchwork)\n\n# tidytext\nif (!require(\"tidytext\", quietly = TRUE)) {\n  reorder_within &lt;- function(x, by, within, fun = mean, sep = \"___\", ...) {\n    new_x &lt;- paste(x, within, sep = sep)\n    stats::reorder(new_x, by, FUN = fun)\n  }\n  \n  scale_x_reordered &lt;- function(..., sep = \"___\") {\n    reg &lt;- paste0(sep, \".+$\")\n    ggplot2::scale_x_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n  \n  scale_y_reordered &lt;- function(..., sep = \"___\") {\n    reg &lt;- paste0(sep, \".+$\")\n    ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n  }\n} else {\n  library(tidytext)\n}\n\n\n# Read data\ndf &lt;- read_csv(\"./table/feature_importance_all_models.csv\", show_col_types = FALSE)\n\n# Data cleaning\ndf &lt;- df %&gt;%\n  mutate(\n    estimate = as.numeric(estimate),\n    abs_estimate = as.numeric(abs_estimate)\n  )\n\n# Variable classification\ndf &lt;- df %&gt;%\n  mutate(\n    var_type = case_when(\n      grepl(\"census_tract|zip_code\", term) ~ \"Location Fixed Effects\",\n      grepl(\"income|POVERTY\", term) ~ \"Socioeconomic\",\n      grepl(\"bathroom|bedroom|livable_area\", term) ~ \"Structural\",\n      grepl(\"age\", term) ~ \"Age\",\n      grepl(\"coord|dist\", term) ~ \"Spatial\",\n      TRUE ~ \"Other\"\n    ),\n    # Clean variable names\n    term_clean = case_when(\n      term == \"number_of_bathrooms\" ~ \"Bathrooms\",\n      term == \"number_of_bedrooms\" ~ \"Bedrooms\",\n      term == \"total_livable_area\" ~ \"Livable Area\",\n      term == \"per_cap_incomeE\" ~ \"Per Capita Income\",\n      term == \"median_incomeE\" ~ \"Median Income\",\n      term == \"PCTPOVERTY\" ~ \"Poverty Rate\",\n      term == \"age2\" ~ \"Age¬≤\",\n      term == \"dist_to_center\" ~ \"Distance to Center\",\n      term == \"x_coord\" ~ \"X Coordinate\",\n      term == \"y_coord\" ~ \"Y Coordinate\",\n      grepl(\"census_tract\", term) ~ gsub(\"census_tract\", \"Census Tract \", term),\n      grepl(\"zip_code\", term) ~ gsub(\"zip_code\", \"ZIP \", term),\n      TRUE ~ term\n    )\n  )\n\n# Keep only top 10 most important variables per model\ntop_features &lt;- df %&gt;%\n  group_by(Model) %&gt;%\n  slice_max(abs_estimate, n = 10) %&gt;%\n  ungroup()\n\n# Color palette (deep blue to deep red)\ncolor_palette &lt;- c(\n  \"Structural\" = \"#67001F\",        # deep red\n  \"Socioeconomic\" = \"#4393C3\",     # medium blue  \n  \"Age\" = \"#2166AC\",               # darker blue\n  \"Spatial\" = \"#053061\",           # darkest blue\n  \"Location Fixed Effects\" = \"#D6604D\"  # reddish tone\n)\n\ncat(\"Generating visualization (using deep blue‚Äìdeep red palette)...\\n\\n\")\n\nGenerating visualization (using deep blue‚Äìdeep red palette)...\n\n\n\n# ========== Figure 1: Lollipop Chart ==========\ncat(\"  Generating Figure 1: Lollipop chart...\\n\")\n\n  Generating Figure 1: Lollipop chart...\n\np1 &lt;- ggplot(top_features, aes(x = reorder_within(term_clean, abs_estimate, Model), \n                                y = abs_estimate, color = var_type)) +\n  geom_segment(aes(xend = reorder_within(term_clean, abs_estimate, Model), yend = 0), \n               size = 1.2, alpha = 0.7) +\n  geom_point(size = 4, alpha = 0.9) +\n  coord_flip() +\n  facet_wrap(~Model, scales = \"free_y\", ncol = 2) +\n  scale_x_reordered() +\n  scale_color_manual(values = color_palette, name = \"Feature Type\") +\n  labs(\n    title = \"Feature Importance Across Progressive Models\",\n    subtitle = \"Lollipop chart showing top 10 features per model\",\n    x = NULL,\n    y = \"Absolute Coefficient\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    strip.text = element_text(face = \"bold\", size = 11),\n    strip.background = element_rect(fill = \"gray95\", color = NA),\n    legend.position = \"bottom\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_line(color = \"gray90\", size = 0.3),\n    axis.text.y = element_text(size = 9),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_lollipop_v2.png\", p1, \n       width = 14, height = 10, dpi = 300, bg = \"white\")\n\ncat(\"    ‚úì Saved to: plot/feature_importance_lollipop_v2.png\\n\")\n\n    ‚úì Saved to: plot/feature_importance_lollipop_v2.png\n\n\n\n# ========== Figure 2: Heatmap ==========\ncat(\"  Generating Figure 2: Heatmap...\\n\")\n\n  Generating Figure 2: Heatmap...\n\nheatmap_data &lt;- df %&gt;%\n  group_by(Model) %&gt;%\n  slice_max(abs_estimate, n = 8) %&gt;%\n  ungroup() %&gt;%\n  mutate(Model_short = gsub(\"M[0-9]: \", \"\", Model))\n\np2 &lt;- ggplot(heatmap_data, aes(x = Model_short, y = fct_reorder(term_clean, abs_estimate), \n                                fill = abs_estimate)) +\n  geom_tile(color = \"white\", size = 1) +\n  geom_text(aes(label = sprintf(\"%.3f\", abs_estimate)), \n            color = \"white\", size = 3.5, fontface = \"bold\") +\n  scale_fill_gradient2(\n    low = \"#053061\", mid = \"#F7F7F7\", high = \"#67001F\",\n    midpoint = median(heatmap_data$abs_estimate),\n    name = \"Coefficient\\nMagnitude\"\n  ) +\n  labs(\n    title = \"Feature Importance Heatmap Across Models\",\n    subtitle = \"Top 8 features per model (darker = stronger effect)\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray40\"),\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\", size = 11),\n    axis.text.y = element_text(size = 10),\n    legend.position = \"right\",\n    panel.grid = element_blank(),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\nggsave(\"plot/feature_importance_heatmap_v2.png\", p2, \n       width = 12, height = 8, dpi = 300, bg = \"white\")\n\ncat(\"    ‚úì Saved to: plot/feature_importance_heatmap_v2.png\\n\")\n\n    ‚úì Saved to: plot/feature_importance_heatmap_v2.png\n\n\n\n# ========== Figure 3: Nightingale Rose Chart ==========\ncat(\"  Generating Figure 3: Nightingale rose chart...\\n\")\n\n  Generating Figure 3: Nightingale rose chart...\n\nmodels &lt;- unique(top_features$Model)\n\nrose_plots &lt;- lapply(models, function(m) {\n  data_m &lt;- top_features %&gt;%\n    filter(Model == m) %&gt;%\n    arrange(desc(abs_estimate)) %&gt;%\n    head(8) %&gt;%\n    mutate(term_clean = factor(term_clean, levels = term_clean))\n  \n  ggplot(data_m, aes(x = term_clean, y = abs_estimate, fill = abs_estimate)) +\n    geom_col(width = 1, alpha = 0.9, color = \"white\", size = 0.5) +\n    coord_polar(theta = \"x\") +\n    scale_fill_gradient2(\n      low = \"#4393C3\", mid = \"#F7F7F7\", high = \"#67001F\",\n      midpoint = median(data_m$abs_estimate),\n      name = \"Importance\"\n    ) +\n    labs(title = m, x = NULL, y = NULL) +\n    theme_minimal(base_size = 10) +\n    theme(\n      plot.title = element_text(face = \"bold\", hjust = 0.5, size = 11),\n      axis.text.y = element_blank(),\n      axis.text.x = element_text(size = 12, face = \"bold\"),\n      panel.grid.major = element_line(color = \"gray90\", size = 0.3),\n      panel.grid.minor = element_blank(),\n      legend.position = \"none\",\n      plot.background = element_rect(fill = \"white\", color = NA),\n      panel.background = element_rect(fill = \"white\", color = NA)\n    )\n})\n\np3 &lt;- wrap_plots(rose_plots, ncol = 2) +\n  plot_annotation(\n    title = \"Feature Importance: Nightingale Rose Chart\",\n    subtitle = \"Top 8 features visualized as rose petals (larger petal = more important)\",\n    theme = theme(\n      plot.title = element_text(face = \"bold\", size = 18, hjust = 0.5),\n      plot.subtitle = element_text(size = 13, hjust = 0.5, color = \"gray40\"),\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n  )\n\nggsave(\"plot/feature_importance_rose_v2.png\", p3, \n       width = 14, height = 12, dpi = 300, bg = \"white\")\n\ncat(\"     Saved to: plot/feature_importance_rose_v2.png\\n\")\n\n     Saved to: plot/feature_importance_rose_v2.png\n\n\n\n# Print Summary\ncat(\"\\n==============================================\\n\")\n\n\n==============================================\n\ncat(\"üé® All visualizations have been successfully generated! (Deep Blue‚ÄìRed Palette)\\n\")\n\nüé® All visualizations have been successfully generated! (Deep Blue‚ÄìRed Palette)\n\ncat(\"==============================================\\n\\n\")\n\n==============================================\n\ncat(\"üìä Generated Figures:\\n\")\n\nüìä Generated Figures:\n\ncat(\"  1. feature_importance_lollipop_v2.png  - Lollipop Chart\\n\")\n\n  1. feature_importance_lollipop_v2.png  - Lollipop Chart\n\ncat(\"  2. feature_importance_heatmap_v2.png   - Heatmap\\n\")\n\n  2. feature_importance_heatmap_v2.png   - Heatmap\n\ncat(\"  3. feature_importance_rose_v2.png      - Nightingale Rose Chart\\n\\n\")\n\n  3. feature_importance_rose_v2.png      - Nightingale Rose Chart\n\ncat(\"üé® Color Palette (following Moran's I visualization):\\n\")\n\nüé® Color Palette (following Moran's I visualization):\n\ncat(\"  ‚Ä¢ Structural: Deep Red (#67001F)\\n\")\n\n  ‚Ä¢ Structural: Deep Red (#67001F)\n\ncat(\"  ‚Ä¢ Socioeconomic: Deep Blue (#4393C3)\\n\")\n\n  ‚Ä¢ Socioeconomic: Deep Blue (#4393C3)\n\ncat(\"  ‚Ä¢ Age: Darker Blue (#2166AC)\\n\")\n\n  ‚Ä¢ Age: Darker Blue (#2166AC)\n\ncat(\"  ‚Ä¢ Spatial: Deepest Blue (#053061)\\n\")\n\n  ‚Ä¢ Spatial: Deepest Blue (#053061)\n\ncat(\"  ‚Ä¢ Location FE: Reddish Tone (#D6604D)\\n\\n\")\n\n  ‚Ä¢ Location FE: Reddish Tone (#D6604D)"
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Property taxes depend on OPA‚Äôs assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia‚Äôs 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#are-current-property-tax-assessments-in-philadelphia-fair-and-accurate",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Are current property tax assessments in Philadelphia fair and accurate?",
    "text": "Are current property tax assessments in Philadelphia fair and accurate?\n\n\n\nProperty taxes depend on OPA‚Äôs assessed values.\n\nAssessments often deviate from actual sale prices.\n\nSome areas are consistently over- or under-assessed.\n\nUnequal assessments create unfair tax burdens.\n\n\n\n\nSource: Reinvestment Fund, Examining the Accuracy, Uniformity & Equity of Philadelphia‚Äôs 2023 Real Estate Assessments (Apr 2024)."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#motivations",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#motivations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Motivations",
    "text": "Motivations\nFairness: Ensure residents pay taxes aligned with true property values.\nTransparency: Build trust through objective, data-based methods.\nEfficiency: Improve city revenue stability and policy planning.\n\nTraditional assessments rely on outdated, manual approaches.\n\n\nMachine learning models can better capture local market dynamics."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#data-sources",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nPhiladelphia Property Sales (n= 24023,2023-2024)\nCensus ACS (Income, Education, Poverty)\nOpenDataPhilly (Number and Distance: Crime, Park&Recreation, Transportation, Hospital,School )"
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#spatial-distribution-of-housing-and-prices",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#spatial-distribution-of-housing-and-prices",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Spatial Distribution of Housing and Prices",
    "text": "Spatial Distribution of Housing and Prices\n\n\nWhere Are the Houses?\n\n\nWhere Are the Expensive Houses?"
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#larger-homes-higher-neighborhood-income-and-more-bathrooms-increase-prices-while-older-properties-tend-to-sell-for-less.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Larger homes, higher neighborhood income, and more bathrooms increase prices ‚Äî while older properties tend to sell for less.",
    "text": "Larger homes, higher neighborhood income, and more bathrooms increase prices ‚Äî while older properties tend to sell for less."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#adding-more-real-world-data-to-build-a-more-fair-and-accurate-housing-price-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Adding more real-world data to build a more fair and accurate housing price model",
    "text": "Adding more real-world data to build a more fair and accurate housing price model\nM1: Basic home features (size, age) ‚Üí simple but limited\nM2: + Census data ‚Üí adds community context\nM3: + Spatial data ‚Üí captures location effects\nM4: + Interactions ‚Üí reflects real neighborhood differences"
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#home-size-and-bathrooms-remain-important-across-all-models-while-neighborhood-and-location-features-gain-influence-after-improving.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.",
    "text": "Home size and bathrooms remain important across all models, while neighborhood and location features gain influence after improving.\n\n\n\nBathrooms and Livable area stay top-ranked across all models\n\nIncome and Census tract rise in importance as they‚Äôre added\n\nFinal model shows Location Effects becoming dominant predictors of price"
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#model-performance-the-predicted-prices-from-our-final-model-align-strongly-with-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance: The predicted prices from our final model align strongly with actual sales.",
    "text": "Model Performance: The predicted prices from our final model align strongly with actual sales.\n   &gt; ‚ÄúAverage Error($)‚Äù shows the average gap between predicted and actual prices (lower = more accurate)."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#the-map-highlights-neighborhoods-where-predicted-prices-differ-most-from-actual-sales.",
    "title": "Philadelphia Housing Price Prediction",
    "section": "The map highlights neighborhoods where predicted prices differ most from actual sales.",
    "text": "The map highlights neighborhoods where predicted prices differ most from actual sales.\n\n\nInterpretation\nBlue areas: Homes undervalued by the model ‚Üí may face under-assessment\nRed areas: Homes overvalued ‚Üí may face over-assessment\nCentral & southern zones: show the largest mismatches ‚Äî indicating uneven market patterns\n &gt; The areas, shown in deep red or blue, are likely where property assessments are least accurate, and where tax fairness may be at greatest risk.\n &gt; These ‚Äúhard-to-predict‚Äù areas should be prioritized for review in future assessment updates."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#three-evidence-based-recommendations",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#three-evidence-based-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Three Evidence-Based Recommendations",
    "text": "Three Evidence-Based Recommendations\nReview Where the Model Shows the Largest Gaps ‚Üí Our residual analysis pinpoints neighborhoods with the highest prediction errors ‚Äî the same areas where assessments are likely least fair. Prioritize these zones for reassessment and data verification.\nUse the Model as a Fairness Benchmark ‚Üí Instead of replacing official assessments, use the model as a cross-check tool to flag properties with unusually high or low assessed-to-predicted ratios.\nInstitutionalize Annual Model Updates ‚Üí Retrain the model each year using new sales and census data so assessments stay current with real market trends, preventing future inequities."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#limitations-improvements",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#limitations-improvements",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Improvements",
    "text": "Limitations & Improvements\nData gaps: Some neighborhoods have limited or inconsistent sales data, and important local factors like school quality or public amenities are not fully represented.\nSpatial variation: The model‚Äôs accuracy differs across regions ‚Äî it performs very well in some areas but less so in others, suggesting that geography strongly influences results.\nSpatial modeling: Next, we will apply models that explicitly account for spatial relationships and neighborhood effects to improve prediction accuracy.\nData enhancement: We plan to expand data coverage in underrepresented areas to reduce bias and strengthen fairness across communities."
  },
  {
    "objectID": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#thank-you",
    "href": "Yang_Chen_Fang_Gao_Xiang_Zhao_Presentation.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Thank You",
    "text": "Thank You\nTurning Data into Fairer Assessments\n\n\nProject Team\nXiaoqing Chen\nZicheng Xiang\nLingxuan Gao\nZhiyuan Zhao\nFan Yang\nZhe Fang\n\nContact\nSixers Consulting 6ers@upenn.edu|www.6ers.com\n\n\n\nPrepared for the City of Philadelphia ‚Äî Office of Property Assessment (OPA)"
  }
]